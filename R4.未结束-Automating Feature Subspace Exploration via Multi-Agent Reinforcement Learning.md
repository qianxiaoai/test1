# Automating Feature Subspace Exploration via Multi-Agent Reinforcement Learning



Kunpeng Liu University of Central Florida Florida, US kunpengliu@knights.ucf.edu

Le Wu
 Hefei University of Technology Hefei, China lewu@hfut.edu.cn



Yanjie Fu∗ University of Central Florida Florida, US Yanjie.Fu@ucf.edu

Rui Bo
 Missouri Univ. of Sci. and Tech. Missouri, US rbo@mst.edu



Pengfei Wang
 CNIC, Chinese Academy of Sciences Beijing, China wpf@cnic.cn

Xiaolin Li Nanjing University Nanjing, China lixl@nju.edu.cn



## ABSTRACT

Feature selection is the preprocessing step in machine learning which tries to select the most relevant features for the subsequent prediction task. Effective feature selection could help reduce dimen- sionality, improve prediction accuracy and increase result compre- hensibility. It is very challenging to find the optimal feature subset from the subset space as the space could be very large. 



While much effort has been made by existing studies, reinforcement learning can provide a new perspective for the searching strategy in a more global way. In this paper, we propose a multi-agent reinforcement learning framework for the feature selection problem. Specifically, we first reformulate feature selection with a reinforcement learning framework by regarding each feature as an agent. 



Then, we obtain the state of environment in three ways, i.e., statistic description, autoencoder and graph convolutional network (GCN), in order to make the algorithm better understand the learning progress. We show how to learn the state representation in a graph-based way, which could tackle the case when not only the edges, but also the nodes are changing step by step.



 In addition, we study how the co- ordination between different features would be improved by more reasonable reward scheme. The proposed method could search the feature subset space globally and could be easily adapted to the real-time case (real-time feature selection) due to the nature of reinforcement learning. Also, we provide an efficient strategy to accelerate the convergence of multi-agent reinforcement learning. Finally, extensive experimental results show the significant im- provement of the proposed method over conventional approaches.



CCS CONCEPTS
 • Computing methodologies → Multi-agent reinforcement learning; Feature selection.



Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

© 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00 https://doi.org/10.1145/3292500.3330868





KEYWORDS

feature selection; automated exploration; multi-agent reinforce- ment learning

ACM Reference Format:

Kunpeng Liu, Yanjie Fu, Pengfei Wang, Le Wu, Rui Bo, and Xiaolin Li. 2019. Automating Feature Subspace Exploration via Multi-Agent Reinforcement Learning. In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’19), August 4–8, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3292500.3330868

## 1 INTRODUCTION

Feature selection aims to select an optimal subset of relevant fea- tures for a downstream predictive task [2, 34]. Effective feature selection can help to reduce dimensionality, shorten training times, enhance generalization, avoid overfitting, improve predictive ac- curacy, and provide better interpretation and explanation. In this paper, we study the problem of automated feature subspace explo- ration for improving downstream predictive tasks.



Prior studies in feature selection can be grouped into three cate- gories: (i) filter methods (e.g., univariate feature selection [5, 33], correlation based feature selection [10, 34]), in which features are ranked by a specific score; (ii) wrapper methods (e.g., evolution- ary algorithms [11, 31], branch and bound algorithms [13, 20]), in which optimal feature subset is identified by a search strategy that collaborates with predictive tasks; (iii) embedded methods (e.g., LASSO [29], decision tree [26]), in which feature selection is part of the optimization objective of predictive tasks. However, these studies have shown not just strengths but also some limitations. 



For example, filter methods ignore the feature dependencies and inter- actions between feature selection and predictors. Wrapper methods have to search a very large feature space of 2N feature subspace candidates, where N is the feature number. Embedded methods are subject to the strong structured assumptions of predictive mod- els. As can be seen, feature selection is a complicated process that requires (i) strategic design of feature significance measurement, (ii) accelerated search of near-optimized feature subset, and (iii) meaningful integration of predictive models.

Reinforcement learning can interact with environments, learn from action rewards, balance exploitation and exploration, and search for long-term optimal decisions [17, 35]. These traits pro- vide great potential to automate feature subspace exploration. Ex- isting studies [4, 14] create a single agent to make decisions. In these models, the single agent has to determine the selection or deselection of all N features. In other words, the action space of this agent is 2N . Such formulation is similar to the evolutionary algorithms [6, 11, 31], which are NP-hard and can merely obtain local optima. In this paper, we intend to propose a better solution using reinforcement learning for feature selection. However, several challenges arise toward this goal.

First, how can we reformulate the problem so that the action space could be limited? We reformulate the problem with multi- agent reinforcement learning. We assign an agent to each feature, the actions of these feature agents are to select or deselect their cor- responding features, and the state of environment is characteristics of the selected feature subspace. One key challenge in multi-agent learning is to coordinate the interactions between agents. The inter- actions can be from two aspects: (i) cooperation and (ii) competition between agents, which can be quantified by feature-feature mutual information in our case. We propose to integrate feature-feature mutual information with predictive accuracy as the reward scheme. In this way, we guide the cooperation and competition between agents for effective feature exploration.

Second, how can we accurately describe the state representa- tion in multi-agent reinforcement learning? We regard the selected feature subspace as the state of environment. To construct state rep- resentations, traditional methods are to extract descriptive statistics (e.g., mean, variance) of the state distribution. However, in feature subspace exploration, the number of selected features changes over time during the exploratory process. If we also extract the mean and variance of each selected feature to describe the state, length of the state representation vector will change over time. But, the policy networks in multi-agent reinforcement learning require a fixed-length state representation. To tackle this challenge, we de- velop three different methods: (i) meta descriptive statistics, (ii) auto-encoder based deep representation, (iii) dynamic graph based graph convolutional network (GCN). In Method i, after extracting the first round descriptive statistics of each selected feature, we extract the second-round descriptive statistics of these first-round descriptive statistics as the state representation vector, so that the state length will not change along with the varying number of selected features. In Method ii, since the number of rows are static in the selected feature subspace, we construct a row-row similar- ity graph, namely static subspace graph, to describe the state. An autoencoder method is applied to learn the state representation. In Method iii, we construct a feature-feature similarity graph to describe the state. Since nodes are features, the number of nodes changes over time. We exploit GCN to learn state representations from dynamic graphs.

Third, how can we improve the robustness of our framework against a vastly different state distribution, while accelerating the exploration of optimal features? Traditionally, we can use expe- rience replay [18, 19] to train our multi-agent framework. In the experience replay, an agent takes samples from the agent’s memory that stores different types of training samples to train the model. In automatic control area, reinforcement learning usually considers all of the samples in the memory, because all possible states need to be evaluated. However, in feature selection, noise, outliers, or low-rewarded data samples can lead to inaccurate understanding of

a feature and feature-feature correlations, and, thus, jeopardize the accuracy of feature selection. Can we create a sampling strategy to select sufficient high-quality samples and avoid low-quality sam- ples? An intuitive method is to oversample high-quality samples by increasing their sampling probabilities. But, this method can not guarantee the independences of samples between different training steps, as it is equivalent to reduce the memory size. To address this issue, we develop a gaussian mixture model (GMM) based genera- tive rectified sampling strategy. Specifically, we first train a GMM with high-quality samples. The trained GMM is then used to gen- erate a sufficient number of independent samples from different mixture distribution components for reinforcement learning.

In summary, in this paper, we develop an enhanced multi-agent reinforcement learning framework for feature subspace exploration. Specifically, our contributions are as follows: (1) We reformulate feature subspace exploration with a multi-agent reinforcement learning framework and integrate the interactions between features into a new reward scheme. (2) We develop three different methods: meta descriptive statistics, autoencoder based deep representation, and dynamic graph based graph convolutional network (GCN), to derive accurate state representation. (3) We develop a GMM-based generative rectified sampling method to improve the training and exploration. (4) We conduct extensive experiments to demonstrate the enhanced performances of our method.



## 2 PROBLEM FORMULATION



We study the problem of feature subspace exploration, which is formulated as a multi-agent reinforcement learning task. Figure 1 shows an overview of our proposed multi-agent reinforcement learning based feature exploration framework. Given a set of fea- tures to be explored, we first create a feature agent for each feature. This feature agent is to decide whether its associated feature is selected or not. The selected feature subset is regarded as the en- vironment, in which feature agents interact with each other. The correlations between features are schemed by reward assignment. Specifically, the components in our multi-agent reinforcement learn- ing framework includes agents, state, environment, reward, reward assignment strategy, and agent actions.

Multi-Agent. Assuming there are N features, we define N agents for the N features. For one agent, it is designed to make the selection decision for the corresponding feature.
 Actions. For the i-th feature agent, the feature action ai = 1 in- dicates the i-th feature is selected, and ai = 0 indicates the i-th feature is deselected.

Environment. In our design, the environment is the feature sub- space, representing a selected feature subset. Whenever a feature agent issue an action to select or deselect a feature, the state of feature subspace (environment) changes.

State. The state s is to describe the selected feature subset. To ex- tract the representation of s, we explore three different strategies, i.e., meta descriptive statistics, autoencoder based deep representa- tion and dynamic graph based graph convolutional network (GCN). We will elaborate these three state representation techniques in Section 3.3.

Reward. We design a measurement to quantify the overall reward R generated by the selected feature subset, which is defined the weighted sum of (i) predictive accuracy of the selected feature subset Acc, (ii) redundancy of the selected feature subset Rv, and

(iii) relevance of the selected feature subset Rd.
 Reward Assignment Strategy. We develop a strategy to allocate the overall reward to each feature agent. The assignment of the overall reward to each agent, indeed, shows the coordination and competition relationship among agents. In principle, we should recognize and reward all of the participated feature agents. Figure 2 shows an example of reward assignment. There are four features with four corresponding feature agents. In the previous iteration, the feature 1, 2, 3 are selected, and the feature 4 is unselected. In the current iteration, feature agent 1 and feature agent 2 issue actions to select feature 1 and feature 2; feature agent 3 issues an action to deselect feature 3; feature agent 4 does not participate and issue any action to change the status of feature 4. In summary, there are only three feature agents (FA1, FA2, FA3) that participate and issue actions. Therefore, the current reward R is equally shared by these three agents.



--- 这里有图 ----



## 3 PROPOSED METHOD

We first present the multi-agent reinforcement learning framework for automated feature subspace exploration. Later, we discuss how to measure reward, how to improve state representation, and how to accelerate feature subspace exploration.

### 3.1 Framework Overview

Figure 3 shows our proposed framework consists of many feature subspace exploration steps. Each exploration step includes two stages, i.e., control stage and training stage.



In the control stage, each feature agent takes actions based on their policy networks, which take current state as input and output recommended actions and next state. The select/deselect actions of each feature agent will change the size and contents of the selected feature subset, and thus, lead to a new selected feature subspace. We regard the selected feature subset as environment. The state repre- sents the statistical characteristics of the selected feature subspace. We derive a comprehensive representations of the state through three different methods, i.e., descriptive statistics, autoencoder and GCN (refer to Section 3.3). Meanwhile, the actions taken by fea- ture agents generate an overall reward. This reward will then be assigned to each of the participating agents.

In the training stage, agents train their policy via experience

replay independently. For agent i, at time t, a newly-created tuple

{st , at , rt , st+1}, including the state (st ), the action (at ), the reward iiiiii

(rt ) and the next state (st+1), is stored into each agent’s memory. ii

We then propose a GMM-based generative rectified sampling (refer to Section 3.4) to derive mini-batches from memories. The agent i uses its corresponding mini-batch samples to train its Deep Q- Network (DQN), in order to obtain the maximum long-term reward based on the Bellman Equation [27]:

Q(st,at|θt)=rt +γ maxQ(st+1,at+1|θt+1) (1) iiiii

where θ is the parameter set of Q network, and γ is the discount. The exploration of feature subspace continues until convergence

or meeting several predefined criteria.

### 3.2 Measuring Reward

We propose to combine the predictive accuracy Acc, the feature subspace relevance Rv, and the feature subspace redundancy Rd as the reward R of actions.



---这里有图--

 Predictive Accuracy. Our goal is to explore and identify a satisfac- tory feature subset, which will be used to train a predictive model in a downstream task, such as classification and outlier detection. We propose to use the accuracy Acc of the predictive model to quantify the reward. Specifically, if the predictive accuracy is high, the actions that produce the selected feature subset should receive a high reward; if the predictive accuracy is low, the actions that produce the selected feature subset should receive low rewards. Feature Subspace Characteristics. Aside from exploiting the pre- dictive accuracy as reward, we propose to take into account the characteristics of the selected feature subset. Specifically, a qualified feature subset is usually of low information redundancy and of high information relevance to the predictive labels (responses). Both the information relevance and redundancy can be quantified by the mutual information, denoted by I . Formally, I by:



-------Rd  Rv 等公式-------



### 3.3 Improving State Representation

Assuming there is a M ∗N dataset D, which includes M data samples and N features. Let nj be the number of selected features at the j-th exploration step. Then, M ∗ n j is the dimension of the selected data matrix S, which varies over exploration steps. However, the policy network and target network in DQN require the state representation vector s to be a fixed-length vector at each exploration step. We thus, need to derive a fixed-length state vector s from the selected data matrix S, whose dimensions change over time.

To derive accurate state representation with fixed length, we develop three different methods, including (i) meta descriptive sta- tistics of feature subspace; (ii) static subspace graphs based au- toencoder; (iii) dynamic feature-feature similarity graphs based graph convolutional network (GCN). The commonness between these three methods is that they all first learn representations for each feature, and then aggregate them to get a state representation. The differences between them lie on the representation learning algorithms and aggregation strategies.



--------图4--------





Method 1: Meta Descriptive Statistics of Feature Subspace.

Figure 4 shows how we extract the meta data of descriptive statistics from the selected data matrix through a two step procedure.

Step 1: We extract descriptive statistics of the selected data ma- trix S, including the standard deviation, minimum, maximum and Q1 (the first quartile), Q2 (the second quartile), and Q3 (the third quartile). Specifically, we extract the seven descriptive statistics of each feature (column) in S, and thus, obtain a descriptive statistics matrix D with size of 7 ∗ nj .



Step 2: We extract the seven descriptive statistics of each row in the descriptive statistics matrix D, and obtain a meta descriptive statisticsmatrixD withasizeof7∗7. ′

Finally,welinkeachcolumnD togetherintothestatevectors with a fixed length of 49.



Method 2: Autoencoder Based Deep Representation of Fea- ture Subspace. Autoencoder has been widely used for represen- tation learning by minimizing the reconstruction loss between an original input and a reconstructed output [1]. An autoencoder con- tains an encoder that maps the input into a latent representation,



























