MAB-Malware: A Reinforcement Learning Framework for Attacking Static Malware Classifiers



Wei Song
 University of California, Riverside Riverside, USA wsong008@ucr.edu



Deepali Garg Avast
 Santa Clara, USA deepali.garg@avast.com



Xuezixiang Li University of California, Riverside Riverside, USA xli287@ucr.edu

Dmitry Kuznetsov Avast
 Prague, Czech kuznetsov@avast.com

Sadia Afroz Avast, ICSI
 San Francisco, USA sadia.afroz@avast.com sadia@icsi.berkeley.edu

Heng Yin
 University of California, Riverside Riverside, USA heng@cs.ucr.edu



ABSTRACT

Modern commercial antivirus systems increasingly rely on machine learning to keep up with the rampant inflation of new malware. However, it is well-known that machine learning models are vul- nerable to adversarial examples (AEs). Previous works have shown that ML malware classifiers are fragile to the white-box adver- sarial attacks. However, ML models used in commercial antivirus products are usually not available to attackers and only return hard classification labels. Therefore, it is more practical to evaluate the robustness of ML models and real-world AVs in a pure black- box manner. We propose a black-box Reinforcement Learning (RL) based framework to generate AEs for PE malware classifiers and AV engines. It regards the adversarial attack problem as a multi-armed bandit problem, which finds an optimal balance between exploiting the successful patterns and exploring more varieties. Compared to other frameworks, our improvements lie in three points. 1) Limiting the exploration space by modeling the generation process as a state- less process to avoid combination explosions. 2) Due to the critical role of payload in AE generation, we design to reuse the successful payload in modeling. 3) Minimizing the changes on AE samples to correctly assign the rewards in RL learning. It also helps identify the root cause of evasions. As a result, our framework has much higher black-box evasion rates than other off-the-shelf frameworks. Re- sults show it has over 74%â€“97% evasion rate for two state-of-the-art ML detectors and over 32%â€“48% evasion rate for commercial AVs in a pure black-box setting. We also demonstrate that the transferabil- ity of adversarial attacks among ML-based classifiers is higher than the attack transferability between purely ML-based and commercial AVs.

KEYWORDS

adversarial attack, reinforcement learning, neural networks, mal- ware classification

1 INTRODUCTION

Malware attacks continue to be one of the most pressing security issues users face today. Recent research showed that during the first nine months of 2019, at least 7.2 billion malware attacks and 151.9 million ransomware attacks have been reported. The attack rate hit a new high with the COVID-19 pandemic. The traditional signature-based methods cannot keep up with this rampant in- flation of novel malware. Hence commercial antivirus companies started using machine learning [7, 66]. Machine-learning-based de- tectors are scalable and efficient at protecting against the huge influx of malware, which is why since the first paper in 2001 on detecting malware using machine learning [61], there has been an explosion of academic research papers on predicting malicious content using machine learning, many of them flaunting high accuracy and being able to detect new malware unseen during training [5, 15, 55, 56, 60]. On the other hand, research has also demonstrated that machine- learning-based detectors can be easily evaded by making even trivial changes to malware [3, 4, 10, 13, 14, 17, 19â€“22, 24, 26, 27, 30â€“ 33, 35â€“37, 39, 40, 42, 44, 45, 49, 52, 53, 57, 58, 63â€“65, 68, 69]. Even commercial antivirus systems, such as Cylance, have been shown to be susceptible to trivial adversarial attacks [6].



https://www.msspalert.com/cybersecurity-research/sonicwall-research-malware- attacks- 2019/
 https://labs.bitdefender.com/2020/04/coronavirus- themed- threat- reports- havent- flattened- the- curve/

https://nicholas.carlini.com/writing/2019/all- adversarial- example- papers.html



Since 2014, there have been more than 1400 papers on adversarial attacks and defense. However, these works mainly focus on the image domain. The adversarial attacks on malware samples are different from attacks in the image domain. For images, adversaries can alter the value of any pixel, as long as the changes are bounded with a ğ¿ğ‘ -norm. But for malware samples, a one-byte change can break the format of a valid PE, or break the original malicious functionality. This is why adversaries usually do not directly modify the raw bytes of the PE file. Instead, they construct a set of actions. Each action can transform the malware sample without breaking the original functionality. For example, the action could be adding a new redundant section (adding a new entry in the section table and appending the section content at the end). Then, the adversarial example generation problem is transformed into finding correct actions and corresponding contents that lead to misclassification.

Adversarial attacks against static malware classifiers are not new. Researchers have proposed a variety of techniques to gener- ate evasive samples (the terms â€œevasive samplesâ€ and â€œadversarial examplesâ€ are used exchangeably in this paper), including genetic programming [18, 68], Monte Carlo tree search [54], and deep Q- learning [4]. Although some of these attempts [54, 68] are dealing with PDF malware and source code authorship respectively, the general algorithms can be applied to PE malware.

Although these techniques have been demonstrated to be ef- fective, we have identified several limitations. First, the existing techniques model the AE generation in a stateful manner, meaning that the actions depend on one another. While this modeling is general, it is usually hard to train a stateful model given that the search space is large. We observe that many actions for transform- ing PE malware are independent. Therefore we choose a stateless modeling approach, which can significantly reduce the learning difficulty and result in more productive AE generation. Second, most of the existing techniques only learn a decision-making policy that decides what action to take in the next step and randomly picks content if needed. For instance, when adding a new section, it will fill the new section with random content. However, contents are as important as actions. If content associated with the action has proved to be useful in one AE, the same action-content pair will likely be useful for some other samples as well. So we should model action and its content as an integral unit. Third, when an AE is successfully generated, these techniques will assign rewards to all the actions involved. In our evaluation, we observe that only a small number of (mostly one or two) actions are essential and the rest are redundant. Assigning rewards to these redundant actions will confuse the learning process.

Based on these insights, we propose an open-source reinforce- ment learning framework, called MAB-Malware, to generate AEs for PE malware. We model this problem as a classic multi-armed bandit (MAB) problem, by treating each action-content pair as an independent slot machine. We model each machineâ€™s reward as a Beta distribution and use Thompson sampling to select the next action and content, striking a balance between exploitation and exploration. We devise an action minimization process, which min- imizes an AE by removing redundant actions and further reducing essential actions into even smaller actions (called micro-actions). We then assign rewards only to these essential micro-actions. This minimization process also helps interpret the root cause of evasions.

In summary, the contributions of this paper are as follows:

â€¢ We examine the existing algorithms in blackbox AE gen- eration and provide key insights for stateful vs. stateless modeling, content-aware vs. content-agnostic modeling, and redundant vs. essential actions.

â€¢ We argue that a stateless and content-aware modeling is more suitable for generating adversarial PE malware, and an action minimization process is essential.

â€¢ To meet these design choices, we propose and implement a novel MAB-based reinforcement learning framework for generating adversarial PE malware.

â€¢ We conduct an extensive evaluation using 5000 PE malware samples on two popular machine learning models and three commercial AV engines. MAB-Malware can achieve a very high evasion rate (over 75%) for machine learning models and outperform the existing blackbox AE generation algorithms by large margins. It also shows a noticeable improvement in commercial AV engines.
 â€¢ Based on our action minimization, we further look into the root cause of these evasions. Our experiment results sug- gest the static classifiers in the commercial AV engines are vulnerable to trivial changes to a malware sample. We also demonstrate that the transferability of adversarial attacks among ML-based classifiers is high (over 80%) but low (less than 7%) between purely ML-based and commercial AVs.

To facilitate the follow-up research on this topic, we plan to share our framework as well as the dataset of adversarial malware samples with researchers upon request.

2 PROBLEM

2.1 Threat Model

We follow the study by Carlini et al. [8] to describe our threat model, from three aspects: adversarial goal, adversarial capabilities, and adversarial knowledge.

Adversarial Goal. The adversaryâ€™s goal is to manipulate malware samples to evade the detection of static PE malware classifiers. Other types of malware like PDF malware or Android malware are not within the scope of this study. This is an untargeted attack be-cause we only consider a binary classification (benign or malicious) not specific malware families in this classification task and we are only interested in causing the malicious samples to be classified as benign.

Adversarial Capabilities. In this work, we assume that the ad- versary does not have access to the training phase of the malware classifiers. For instance, the adversary cannot inject poisonous data into the training dataset.

Also, the adversary cannot arbitrarily change the input data. In most scenarios of adversarial attacks, such as image recognition, the adversary is required to make only â€œsmallâ€ changes to the original sample to keep the manipulation visually imperceptible. However, when attacking malware classification, the restriction is not on the number or size of changes, but on the preservation of malicious functionality. If â€œsmallâ€ changes on a malware sample indeed confuse a malware classifier but prevent the malware from acting maliciously, this manipulation is not considered successful.

Adversarial Knowledge. Based on the knowledge an adversary can obtain, an attack can be divided into two types: 1) whitebox attacks where the adversary has unlimited access to the model; and 2) blackbox attacks where the adversary has no knowledge about the model and can obtain the classification results only through a limited number of attempts. A classification result can be a score or simply a label.

In this work, we consider an adversary with only blackbox access. The adversary does not know anything about the internals of the deployed classifiers, can perform a limited number of attempts to the classifiers, and can observe the classifiersâ€™ actions when the samples are considered malicious.





2.2 Problem Definition

In this paper, we focus on three state-of-the-art machine learning classifiers and the static classifiers of 3 top commercial antivirus products.

We aim to automatically generate adversarial examples for mal- ware classifiers and explain the root cause of the evasions. The problem can be split into two sub-problems: adversarial example generation and feature interpretation.

We aim to manipulate a malware sample such that malware clas- sifiers misclassify it as a benignware, and do not break its malicious functionalities. For whitebox attacks in the image domain, changes to original images are bounded with ğ¿2 and ğ¿âˆ norms. It ensures that the pixel changes are imperceptible to humans. However, in the malware classification domain, as long as the malware behaviors remain the same, it is unlikely for normal users to notice the differ- ences. Thatâ€™s why previous blackbox attacks [4, 11, 23] on malware do not try to minimize changes when generating AEs. However, we find that the minimal change requirement is still crucial for three main reasons: 1) it reveals which actions and the corresponding payloads are essential to generate evasive samples that can be ap- plied to other samples to create successfully evasive samples; 2) it unveils which feature changes caused the evasion to ensure that the classifier does not rely on superficial features; 3) it reduces the chance of creating broken binaries. In the blackbox setting, instead of minimizing added noises in feature space, we minimize action sequences applied to generate AEs. It includes removing redun- dant actions and replacing actions that cause large changes to the features used for detection.

Let X be a malware dataset, ğ‘“ be a malware classifier that maps a sample ğ‘¥ âˆˆ X to a classification label ğ‘¦ âˆˆ {0, 1} (0 represents benign, 1 represents malicious). We implement an action set A = {ğ‘1, ğ‘2, . . . ğ‘ğ‘› } that can be used to perturb malware samples. We define an objective function for adversarial example generation in (2). An adversarial example ğ‘¥ â€² = ğ‘¡ (ğ‘¥ ) is generated by applying a transformation function ğ‘¡ , which is a sequence of actions sampled from set A. L(ğ‘“ (ğ‘¡(ğ‘¥)),ğ‘¦ Ì„) measures the difference between the predicted label of ğ‘“ (ğ‘¡(ğ‘¥)) and benign labelğ‘¦ Ì„. The transformation function ğ‘¡ subjects to the constraint that ğ‘¡ (ğ‘¥ ) does not change the functionality of ğ‘¥, i.e. the functionality difference ğ›¿(ğ‘¥,ğ‘¡(ğ‘¥)) before and after transformation equals to 0.



3 MOTIVATION

In this section, we first discuss the existing techniques on AE gen- eration and their limitations, and then we present our insights that motivate our MAB-based approach.

3.1 Existing Techniques on AE Generation

Deep Q-learning. Anderson et al. [4] propose to apply deep rein- forcement learning (RL) to generate AE for PE malware to bypass machine learning models. They first define a set of actions (file mu- tations), including changing PE headers, appending overlay bytes, packing, and unpacking. Then the agent selects the next action based on a policy and an environmental state. When an evasive sample is generated, all applied actions (including early actions that produce no immediate reward) get promoted for a given state.

Genetic Programming. Demetrio et al. [18] propose a genetic programming-based approach to generate AEs of PE malware in a black-box attack manner. It formalizes the problem as a constrained minimization problem, to trade-off between the probability of eva- sion and injected payload size. It first extracts some benign content from benign binaries, and then appends a random fraction of them at the end of target malware samples. The fitness function is defined as the sum of confidences score and injected payload size. In each iteration, it selects variants with the lowest fitness score. It iterates over three steps: selection, cross-over, and mutation. Cross-over and mutation guarantee that the new population is sufficiently different from the previous variants. Another paper from Xu et al. [68] also uses a genetic programming-based approach to gen- erate adversarial PDF malware. They first define a set of actions, called mutation operators, which include deleting, inserting, and replacing an object in a PDF tree structure. It uses the confidence score as the fitness score and also conducts selection, mutation, but without cross-over. Different from Demetrio et al.â€™s work [18], their approach saves and reuses the successful traces for new samples. The trace contains the sequence of actions and the content used by each action.

Monte Carlo Tree Search. Quiring et al. [54] propose a Monte Carlo Tree Search (MCTS) based approach to mislead the classi- fication of source code authorship. Although this work is not in the malware classification domain, the high-level idea is applicable. They define a set of actions (code transformation) for changing stylistic patterns. Then they create a Monte Carlo search tree, in which each node represents a variant of the code and each edge represents an action. In each iteration, the node with the highest average classifier score is selected. From that node, the framework generates a set of transformation sequences and propagates the observed classifier scores from the leaf node back to the root node. The process repeats until it generates an evasive sample. Then the task of AE generation is converted to a path search problem. The goal is to find a path on the tree that leads to misclassification.

3.2 Our Insights

While these existing techniques have demonstrated their effec- tiveness more or less, we observe several key insights, which can motivate us to develop a better technique for AE generation.

Stateful vs. Stateless Modeling. Existing techniques model the AE generation in a stateful manner. They aim to find the best state in each iteration, select the best action in the current state according to the policy, and transform the sample to enter the next state. In other words, they try to find an optimal path of states that leads to evasion. Stateful modeling is necessary for hard tasks, such as AlphaGo and video-game-playing problems. The optimal path to reach success is often very deep. It also means that it is usually difficult and time-consuming to train a stateful model.

Our key insight is that it is not necessary to model the problem of AE generation for PE malware in a stateful manner. Since it is generally hard to manipulate PE files without breaking their functionalities, the existing actions are rather coarse-grained and mostly independent. For instance, actions like removing debug information, section rename, section add, etc. can be applied in any order. The resulting binary file is the same regardless of the order in which these actions are applied. Of course, there are cases where actions may depend on one another. For instance, when adding multiple sections to a PE file, the order matters. Even in these cases, we argue that the dependency between these actions may not be very strong. According to our evaluation results for machine learning models and AV engines in Section 5.4, oftentimes only one or two actions are needed to generate an AE, after removing the unnecessary actions. It means that the dependency between actions (if it exists) is rather weak, at least for the machine learning models and AV engines we evaluated.

Based on this insight, we believe that stateless modeling of AE generation for PE malware is reasonable. Compared to stateful mod- eling, stateless modeling would treat each action independently, allowing a faster learning process and more productive AE gen- eration. To this end, we propose to utilize a classic reinforcement learning model, multi-armed bandit [41], to model each action as an independent slot machine. It estimates the probability of each action being evasive over limited trials and leverages the estimated probabilities to select the best actions to maximize the overall re- ward: generating as many AEs as possible in a limited number of trials.

Content Modeling. Many actions used for manipulating PE need to be associate with some contents. For instance, when adding a new section, we need to specify what content to be filled in that section, and when renaming a section, we need to provide a new section name. Our second insight is that these contents are as important as the actions. If content associated with one action has proved to be useful in one AE, the same action-content pair will likely be useful in the future too.

Unfortunately, the existing techniques mentioned above (except the one by Xu et al [68]) do not take contents into account. They only learn a decision-making policy to decide what action to take in the next step and take random content if required. For example, if the next action to take is â€œSection Addâ€ according to the policy, they will fill the new section with random content (from a pool of data). Xu et al. [68] indeed take contents into account, but they do not make the best use of it (only reuse traces for the first generation). Our MAB-based framework treats an action-content pair as an integral unit (a slot machine) for modeling. If a new action-content pair is discovered to be useful to generate an adversarial example, it will be saved as a new slot machine, and put into the machine pool. When other samples select that machine again, the same content will be reuse again.



Precise Reward Assignment. Reward assignment is essential to all the existing AE generation techniques described above. When an AE is successfully generated, a positive reward is assigned to the corresponding state or the corresponding sequence of actions. As discussed earlier, not all actions are essential to the generation of this AE. According to our evaluation in Section 5.4, in most cases, only one or two actions are essential and all the rest are unnecessary. Therefore, assigning rewards to all the actions involved in an AE generation will lead to a less accurate reinforcement learning model.

Instead, it is better to remove the unnecessary actions, and only assign rewards to the essential actions. Hence, our third insight is that we should precisely assign rewards only to the essential actions.



æ­¤å¤„æœ‰å›¾

Figure 1: An example of action minimization.



To identify the essential actions for an AE, we propose a minimization process. As shown in Figure 1, the original malware sample ğ‘¥ resides in the malicious region of the feature space. We perform a sequence of single actions ğ‘1, ğ‘2 and ğ‘3 until the generated sample ğ‘¥123 successfully reaches the benign region. ğ‘¥123 is an adversarial example. In the minimization phase, first, we remove useless actions. The action ğ‘2 is essential, because by removing action ğ‘2, the generated sample ğ‘¥13 is no longer evasive. The action ğ‘1 is useless because by removing action ğ‘1, the generated sample ğ‘¥23 has no effect in the classifierâ€™s decision. Then we disentangle these actions into micro ones (i.e., actions that cause smaller changes). ğ‘2 can be replaced with micro-actions ğ‘2â€² . Action ğ‘3 can be replaced with micro-actions ğ‘â€² or ğ‘â€²â€². We generate three samples 33 ğ‘¥2â€²3,ğ‘¥2â€²3â€² andğ‘¥2â€²3â€². Finally,wehaveanadversarialsampleğ‘¥2â€²3â€²â€² with a minimized action sequence (ğ‘â€²,ğ‘â€²â€²). So a positive reward 23 â€² â€²â€² can be precisely assigned to these essential actions ğ‘2 and ğ‘3 .



# 4 MAB-MALWARE



## 4.1 The Framework



The workflow of our framework MAB-Malware is shown in Figure 2.

It consists of two main modules: the Binary Rewriter and the Action

Minimizer. The Binary Rewriter utilizes Thompson sampling to

select action sequence ğ‘¡ from the action set A and rewrites original

malwaresampleğ‘¥togeneratevariantsğ‘¥â€² =ğ‘¡(ğ‘¥).Ifğ‘¥â€²canevadethe

detection of the target classifier, the Action Minimizer continues to

remove redundant actions from ğ‘¡ to generate minimized evasive

samples ğ‘¥ â€² and infer the root cause of the evasion. At last, the ğ‘šğ‘–ğ‘›

framework verifies the functionality of ğ‘¥ â€² . If the behavior of it is ğ‘šğ‘–ğ‘›

changed, the original sample ğ‘¥ is put back into the working queue. Otherwise, ğ‘¥ â€² is returned.

## 4.2 Binary Rewriter

æ­¤å¤„æœ‰è¡¨æ ¼



Action Set and Features. In Table 1, we define the actions that can be applied to malware to create adversarial samples. Each action manipulates a set of features that a classifier may use to detect malware. Examination of open-source malware detectors such as EMBER is used to hypothesize about the features that might be used in commercial AVs. These features are categorized into three categories: hash-based signatures (file hash and section hash), rule-based signatures (section count, section name, section padding, debug info, checksum, certificate, and code sequences), and data distribution based features (byte histogram, and byte entropy histogram).

Macro-actions. We implement most actions proposed by Ander- son et al. [4] using the pefile library and fix many corner cases that may break the functionality. We also adopt a code randomization action (CR) from Pappas et al. [48]. It is a defense method originally proposed to prevent Return Oriented Programming (ROP) attacks.



Micro-actions.Ifanactionğ‘changesfeaturesetğ¹ = {ğ‘“1,ğ‘“2,... ğ‘“ğ‘˜} of a malware sample, then another action that changes only a sub- set of ğ¹ is a micro-action of ğ‘. We implement 5 micro-actions for macro-action OA, SP, SA, SR, and CR. Table 2 shows all the actions used in our framework and the corresponding affected features for each action. Consider SP action (append content at the end of a section) as an example: by looking up Table 2, we find that SP action affects features File Hash (ğ¹1), Section Hash (ğ¹2) and Rule-Based Signatures in the section padding data(ğ¹5). ğ¹1, ğ¹2 are affected if any modification is made to the file and section content. ğ¹5 is affected only if SP action modifies the padding content that may contain body-based signatures. Thus, one micro action of SP is SP1 (append 1 byte at the end of a section), as shown in Table 2, which does not affect ğ¹5. OA1 action (Append 1 byte at the end of a binary) is also a micro action of SP since it only affects ğ¹1.

4.2.2 Multi-armed Bandit (MAB) Problem. In the AE generation problem, each action with certain concrete content may or may not be useful to generate an evasive sample. We consider each ma- chine ğ‘š provides a random reward from a probability distribution specific to that tuple. Our objective is to maximize the sum of re- wards earned through a sequence of action-content pairs. Since the trials are limited, at each trial, we need to tradeoff between "exploitation" of the tuple that has the highest expected reward and "exploration" to obtain more knowledge about the rewards of other tuples. Thatâ€™s why we model this problem as a multi-armed bandit (MAB) problem [41]. The MAB problem is a classic reinforce- ment learning problem. It maximizes gains by allocating limited resources to multiple competing choices, and the property of each choice is gradually learned in the process of resource allocation. Each action-content pair is considered as a machine ğ‘š.

4.2.3 Thompson Sampling. In our task, we face the delayed feed- back Problem. When evaluating the static modules of commercial antivirus systems, we need to copy the generated sample to the virtual machine with antivirus and wait for the scanning result. This process takes seconds, even minutes for certain AVs. If we adopt a deterministic algorithm, such as UCB1 or Bayesian UCB, it will always select the one with the highest values before the result returns. It causes inefficient trials because of outdated information. To address this issue, we use Thompson sampling algorithm [67], which is more robust than deterministic algorithms in the delayed feedback environment [12].

We assume the reward returned by each machine ğ‘š follows a beta distribution [1] specific to that tuple. The beta distribution is a continuous probability distribution parameterized by two positive parameters, denoted by ğ›¼ and ğ›½, i.e. ğ‘š âˆ¼ Beta(ğ›¼, ğ›½).

We consider each machine ğ‘š returns a random reward from a beta distribution specific to that tuple. Formally, we assume the rewardsformachineğ‘šfollowsthedistributionğ‘Ÿ(ğ‘) âˆ¼ ğµğ‘’ğ‘Ÿğ‘›ğ‘œğ‘¢ğ‘™ğ‘™ğ‘–(ğœƒ). The expected reward ğœƒ is a fixed value and unknown to the player. When pulling machine ğ‘š, it gets 1 with probability ğœƒ , and 0 with probability 1 âˆ’ ğœƒ . In the beginning, for each machine, ğ›¼ =1, ğ›½ =1. When an evasive sample is generated, for every action used, if it cannot be minimized by the minimizer (we will elaborate on the minimization process in the next section), we increase its ğ›¼ by 1. Otherwise, we increase its ğ›½ by 1. In other words, ğ›¼ and ğ›½ correspond to the counts of success or fail respectively.

At each action selection iteration, for each machine, we sample a value from its ğµğ‘’ğ‘¡ğ‘(ğœƒ;ğ›¼,ğ›½) distribution and select the machine with the highest value as the next machine. When the ğ›¼ and ğ›½ values of a machine are small, the uncertainty of ğ‘š is high. Even if this average reward is lower than other machines, it still has a relatively high possibility to get a large value. In this way, new machines are selected for exploration. After several trials, the ğ›¼ and ğ›½ value of becoming large, and the uncertainty decrease. In this way, machines with high average rewards are selected for exploitation. Thompson sampling can automatically find an optimal balance between exploration and exploitation. Because every machine has a possibility to be selected, Thompson sampling is more robust than deterministic algorithms, such as UCB1.



When an evasive sample is generated and minimized, besides update the regards for existing machines, we also need to add new machines to our machine pool. If an essential machine includes content, we can create a new machine, with ğ›¼=1, ğ›½=1. In the be- ginning, the new machineâ€™s uncertainty is high, and it is relatively easy to be selected. When selected, the new machine reuses the successful content.



4.2.4 Workflow of Binary Rewriter. Algorithm 1 summarizes the workflow of the Binary Rewriter. For a seed malware sample ğ‘ , we aim to generate a minimized evasive sample as ğ‘¥ â€² , such that it ğ‘šğ‘–ğ‘› evades the target classifier (ğ‘“ (ğ‘¥â€² ) = 0) and only change minimal ğ‘šğ‘–ğ‘›

features. First, we add the original machines to the machine list M, the ğ›¼ value and ğ›½ value of each machine are set to 1. When these machines need content, they will select random content. In each iteration, for each machine, we sample a value based on its ğ›½ distribution, and select the machine with the highest value, apply the corresponding machine to the current sample. We apply various actions iteratively until we get an evasive sample or exceed the total number of attempts ğ‘šğ‘ğ‘¥_ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘šğ‘ğ‘¡. If the sample becomes evasive, we further use Acton Minimizer to remove redundant machines. If the remaining machines are new (with new content), we add a new machine ğ‘šâ€². If itâ€™s an existing machine, we increase the ğ›¼ value of it and its parent machine, which has the same action but with random content. If the current machine cannot create an evasive sample, we increase its ğ›½ value.



æ­¤å¤„æœ‰ä»£ç  ã€‚



##  4.3 Action Minimizer



The Action Minimizer removes unnecessary actions and uses micro- actions to replace macro-actions, to produce a â€œminimizedâ€ evasive sample that only changes minimal features to evade.



æ­¤å¤„æœ‰ä»£ç  ã€‚



As shown in Algorithm 2, for the action sequence ğ‘ = (ğ‘1, ğ‘2, ...) of an ğ‘ trying to remove the action. We apply the new sequence on the original sample ğ‘  to generate a new sample ğ‘¥ â€². If ğ‘¥ â€² is evasive, we consider the action ğ‘ğ‘– redundant, and remove ğ‘ğ‘– from the action sequence. If not, we further use micro-actions to replace the original action with minimal feature changes. For example, the SA action (add a new section) changes many features of the original binary.

It creates a new entry in the section table, adds a chunk of content at the end of the file, it also changes the file hash. If we replace this action with the OA1 action (append 1-byte overlay data), and it still can evade detection, then we can use OA1 to replace SA. In this way, we generate the minimized evasive sample ğ‘¥ â€² . ğ‘šğ‘–ğ‘› From a defenderâ€™s point of view, we also would like to understand how evasion happens, where the weakest point of the classifiers is. The action minimization of evasive samples provides a good opportunity to infer that information. Figure 3 shows how we break macro-actions into micro-actions. Take the action Section Append (SP) as an example. First, by looking up Table 2, SP changes feature ğ¹ = {ğ¹1, ğ¹2, ğ¹5 } (File Hash, Section Hash and Section padding). The actions that only change a subset of ğ¹ are OA1 that changes {ğ¹1} and SP1 that changes {ğ¹1, ğ¹2 }. Starting from the minimum change, we try to replace SP with OA1 and check if the file is still evasive. If so, we can say the evasion is caused by breaking the file hash (ğ¹1). If not, we continue to replace SP with SP1. If successful, the evasion is caused by breaking section hash (ğ¹2). Otherwise, the evasion is caused by breaking signatures in section padding content (ğ¹5).



Figure 3: Decision rules are used to map actions to feature space



# 5 EVALUATION



 ## 5.1 Experiment Setup



Dataset: In this paper, we generate adversarial examples for Win- dows PE binaries. To ensure the executability and functionality of the generated samples, the format and constraints of PE files must remain intact.

To guarantee the quality of malware samples, we randomly select 5000 samples from VirusTotal that meet the following requirements: 1) more than 80% antivirus engines of VirusTotal label them mali- cious; and 2) the execution of those samples in a Cuckoo sandbox shows malicious behavior. Visual Basic (VB) programs are also ex- cluded from the dataset because the IDA Pro in the implementation of code randomization [48] cannot generate CFG for them. These implementation issues are left as future work.



Figure 4: Evasion Results on Different Frameworks

Figure 5: Evasion Results on Different RL Algorithms



Setup: The experiments are performed on 20 virtual machines of the Microsoft Azure cloud platform. The configuration of each virtual machine is Standard D2s v3 (2 vcpus, 8 GiB memory). All the scripts such as Binary Rewriter, Action Minimizer, and result analysis are implemented in Python 3.6.9. The Binary Rewriter also requires the pefile library and IDA Pro 6.8. For all the antivirus software under testing, free versions and default settings are used. We choose three top commercial antivirus products for blackbox testing, which are anonymized as AV1, AV2, and AV3. Each antivirus is installed on an Azure virtual machine with Windows 7 Version 6.1 Build 7601 (Service Pack 1).

To ensure the malware will not infect other machines in the network and the stability and reproducibility of our experiments, all network traffic is routed to an InetSim instance on the host machine to provide simulated network services.

We choose the following models as our target models:



â€¢ EMBER [5] is an open-source machine-learning-based clas- sifier that uses a tree-based classifier model LightGBM to detect malware. It generates a 2350-dimensional feature vec- tor for each sample consisting of two main types of features: raw features (e.g. ByteHistogram, ByteEntropyHistogram, Strings) and parsed features (e.g. GeneralFileInfo, Header- FileInfo, SectionInfo, ImportsInfo, ExportsInfo). We use the implementations provided by Machine Learning Security Evasion Competition (MLSEC) 2019[46].

â€¢ MalConv [55] is a malware detection model that uses a neu- ral network to learn knowledge directly on the raw bytes of malware samples. We use the model implemented in MLSEC 2019 [46].

â€¢ Commercial AVs. We also test the static classifiers of 3 top commercial antivirus systems, according to PC Maga- zine [51].



## 5.2 Adversarial Example Generation



Comparison with Other Off-the-Shelf Frameworks. We com- pare our MAB-Malware with other two off-the-shelf attack frame- works: SecML-Malware and Gym-Malware. SecML-Malware is a plugin for the SecML Python library. It contains many kinds of attacks, including black-box attacks and white-box attacks. We uti- lize its genetic programming-based black-box attack (GAMMA) in this experiment. In the selection step, it supports both confidence score-based selection and hard label-based selection (classifiers only return benign or malicious). We assume attackers cannot get the confidence score in our threat model. So we only evaluate the hard label-based attack. Gym-Malware is a reinforcement learning-based malware manipulation environment using OpenAIâ€™s gym. Its agents learn how to manipulate PE files to bypass AV based on a reward provided by taking specific manipulation actions. To evaluate the effect of reinforcement learning, it supports both reinforcement learning-based agent action section and random action selection.

We measure the evasion rate for two machine learning-based models, MalConv and EMBER. Evasion Rate is defined as:

ğ‘…ğ‘’ =ğ‘ğ‘’/ğ‘ğ‘‘ (2)

where ğ‘ğ‘’ is the total number of successful evasive samples, and ğ‘ğ‘‘ is the total number of original samples that can be detected by the target model. For a fair comparison, we use the same dataset (5000 samples from VirusTotal) and the same trained MalConv and EMBER models (from the Machine Learning Static Evasion Competition 2019 [46].) in all three frameworks. Each experiment runs five times to calculate the average.



https://github.com/zangobot/secml_malware.git https://github.com/endgameinc/gym- malware.git





From Figure 4, we can see that our MAB-Malware framework works much better than other approaches. It can generate AEs for 97.72% samples to evade MalConv, 74.4% samples to evade EMBER. The evasion rate of SecML-Malware (GAMMA-hard label) only 63.6% and 50.0% respectively. We can also see that even with the knowledge of the confidence score, the evasion rates are similar and sometimes even worse. Gym-Malware has the lowest evasion rate. The evasion rate is almost identical with or without reinforcement learning. This indicates that this deep Q-learning model does not learn meaningful knowledge to guide the evasion. The reason is that the problem modeling creates an exponentially large search space. And without action minimization, the reward assignment is chaotic. Within 60 trials, it cannot explore enough in such a large space and learn meaningful policy to select the correct action and corresponding content.

Comparison with Other Algorithms. In the previous experi- ment, we observe that MAB-Malware generates much more ad- versarial examples than the other off-the-shelf frameworks. How- ever, the action set of these three frameworks is different. SecML- Malware only utilizes benign content injection and appending. Gym-Malwareâ€™s action set is much larger, including header manipu- lation and packing. Our action set is re-implemented using pefile and contains more micro-actions. As a result, it is still unclear which part of our framework causes the differences: the action set, or our unique problem modeling and reinforcement learning algorithm.

So in this experiment, we only use our own action set and change the action selection algorithms. The baseline is random selection. Then we need to compare our method with the other reinforcement learning algorithms. In the experiment above, we have already shown that the Q-learning models cannot directly improve the evasion rate over random selection. In this experiment, we further implement another MCTS-based reinforcement learning algorithm. Quiring et al. [54] proposed an MCTS-based approach to mislead the classification of source code authorship. Because their code cannot be directly applied to malware classifiers, we borrowed their ideas and reimplement the selection, simulation & expansion, and backpropagation operation for our problem.

From Figure 5, we find that in the same action set, our MAB algorithm greatly improves the evasion rate over random action selection, while the MCTS algorithm barely provides any improve- ment. Recall that unlike the existing frameworks, we model the problem differently. Existing frameworks model the AE genera- tion in a stateful manner, and try to find an optimal path of states that leads to an evasion. When they generate an AE, they do not minimize the feature changes and assign rewards to potentially re- dundant actions. It makes the training difficult in a large searching space. Moreover, existing frameworks do not have a mechanism to efficiently reuse successful payloads. Whenever an action is se- lected, they need to start searching for the correct payload from scratch repeatedly.

Attacking Commercial Antivirus. We also test our framework on three commercial antivirus engines. Since the throughputs of commercial AV engines are much lower than machine learning

classifiers, we only use 1000 samples for this experiment and only compare MAB-Malware with random action selection. For AV2 and AV3, our approach improves the evasion rate by 8% to 9%. For AV1, although the evasion rates are almost the same in the end, the evasion rate of our approach is faster in the beginning.

From the evasion rates for EMBER and MalConv, we can find out that MAB-Malware can easily mislead pure machine learning mod- els. The reason is that besides finding good actions, MAB-Malware is also good at preserving good content, which largely affects the data distribution of the generated variants. It may indicate that AV2 and AV3 make more use of machine learning models than AV1 in their static detection. We will further infer the root cause of the evasions for different AVs in Section 5.4.

Number of bytes changed. The Action Minimizer makes sure that the minimized evasive samples only change minimal content to flip the classification label. So by checking how many bytes we need to change, we can infer the robustness of different malware classifiers. To measure the difference between the minimized evasive example and the original malware, we compute the total number of bytes appended or modified.

By positioning the samples in a line sorted by byte changes (Figure 8), we notice that:



â€¢ By only changing one byte of the original malware, we can generate 33 for AV1, 32 for AV2, 3 for AV3.

â€¢ Machinelearningmodelsarenotvulnerabletosmallchanges. However, it does not mean that ML models are more robust than commercial AVs. From previous evasion rate results, we can see ML models are easier to evade using our framework than commercial AVs.



## 5.3 Testing Functionality Preservation



To conduct a blackbox attack on malware classifiers, we need an ac- tion set that provides several different transformations that change different features, but do not change the functionality. We found that the action set in Gym-Malware, which is implemented using LIEF [38] library, is not safe. According to our experiment result in Table 3, more than 60% of the generated binaries after a single action cannot be executed, or behave differently. To solve this problem, we carefully reimplement most actions using the pefile [9] library to avoid many corner cases that may lead to a broken binary. For example, before adding a new section, we check whether there is sufficient space between the last section header entry and the first section. Similarly, we also detect overlay data at the end of the file before appending new section content right after the last section.



Figure 8: Number of Changed Bytes of Adversarial Examples. By changing only one byte, adversarial examples are created for all tested AVs, with the exception of EMBER, which is not vulnerable to a few bytes of change in the binary.



Only less than 4% of rewritten samples after one action behave differently from the original samples. The detailed functional rate comparison is shown in Table 3 in the evaluation section.

We implement our own action set A using the pefile library whereas the Gym-Malware rewrites binaries using the LIEF library. We noticed that rewriting a binary with the LIEF library can cause unnecessary changes to the binary that can sometimes result in broken files, thus destroying the functionality of the original mal- ware samples. To compare our actions with the actions from Gym- Malware, we randomly select 50 malware samples from our dataset, create adversarial samples by applying different actions, analyze all variants in the Cuckoo sandbox, and compare the behavior sig- natures with the original samples.

From Table 3 we can see that except for the Overlay Append action, most actions in the Gym-Malware framework cause 63.24% of the rewritten samples to lose functionality. In contrast, only less than 8% of the rewritten samples using our actions create broken binaries. One reason that the innocuous actions break the binaries is that these actions can override other sections causing corrupted files (see Appendix figure 12 for an example).





## 5.4 Explanation



Understanding why evasion happens can help improve the robust- ness of a classifier against adversarial attacks. For each evasive sample, the Action Minimizer first removes all redundant actions and uses micro-actions to replace the macro-actions. We summa- rize the most frequent action sequence combination is Figure 7. According to the rules in Figure 3, we can infer the root cause of each evasion, shown in Figure 9. We found that:

- For two machine learning-based classifiers, the most impor- tant action is Overlay Append (OA). Other actions that only change a few bytes have almost no effect on them. It shows that the change in data distribution is the root cause of the evasions.

- The Section Add 1 Byte (SA1) action plays a significant role in evading all AVs. It indicates that all AVs utilize section count as an important feature for detecting malware.

- Comparing to AV2 and AV3, AV1 is also vulnerable to the Code Section Append 1 Byte (CP1) action. CP1 alters the hash of the code section. It indicates AV1 uses code section hash as an important feature for detecting malware.

- The Section Rename 1 Byte (SR1) action itself can generate many adversarial examples for AV2. SR1 changes one byte of one section name. It indicates that AV2 relies heavily on the section name for detecting malware.

- Comparing with AV2 and AV3, the Section Add (SA) action and the Overlay Append (OA) action have almost no effect on AV1. SA and OA greatly change the data distribution of the original malware samples. It indicates that AV2 and AV3 integrate some machine learning models in static detection. And AV1 mainly uses the signature-based approach to detect malware.

  

  ## 5.5 Transferability

  

  Transferability refers to the property that allows an adversar- ial sample that can evade one model can also evade other similar models. If the adversarial malware samples are transferable, then evading one malware detector would be enough to evade all mal- ware detectors.

Figure 10: Transferability of Adversarial Samples

Figure 10 shows the percentage of evasive samples generated for one classifier that can also evade other classifiers. The number in the cell (model A, model B) shows the percentage of evasive samples generated for model A can also evade model B. We noticed:

â€¢ Thetransferabilitybetweenmachinelearningmodelsisquite high, although EMBER and MalConv are trained on different features (2350-dimensional extracted features vs. raw bytes), and the architecture is different (decision tree vs. neural network).

â€¢ Both AV2 and AV3 utilize machine learning models and con- sider Section count as an important feature. So the trans- ferability between AV2 and AV3 is relatively higher than others.

# 6 DISCUSSIONS



Triviality of Defense. The triviality of the defense depends on the type of attack. To defend the overlay append attack, the defender can ignore the overlay data when training models. To defend against the SA attack, the defender can lower the importance of benign fea- tures in models, and only consider malware features. To defend the RD, SR, BC attack, the defender should avoid using such fragile pat- terns as malware features. However, completely ignoring the trivial features can reduce the accuracy of a malware detector. The code randomization (CR) attack is hard to defend because the defender cannot locate the small snippet of binary that is randomized.

Why do commercial AVs rely on simple features? Looking at the result, it seems surprising that trivial changes to malware can evade professionally developed commercial anti-virus systems used by millions of users. Why have not commercial AVs fixed these problems already? One hypothesis could be that adversarial examples are a result of the trade-off between true positive and false-positive rates. The commercial systems need to provide a fast decision while maintaining a low false-positive rate. The relatively simple features, such as file hash or white-listing benign strings can help gain a high accuracy with a low false-positive rate. For example, in their attack against Cylance [6], researchers noticed that to reduce their false positive rate the Cylance team whitelisted some families of executables, one of them was an online game. So, the whitelisting used to reduce the false-positive rate was enough to create an adversarial attack. Another hypothesis is that anti-virus systems need to protect against the malware of today, instead of focusing on new attacks that are not currently happening. Real adversaries probably use techniques different from the ones used to create ML-based adversarial attacks. A third hypothesis is that anti-virus systems do not rely only on static detection, but also on dynamic and behavioral detection. If all the adversarial malware samples get detected when they are executed, the AVs are not concerned with the static-only adversarial attacks as these attacks cannot infect real users.

Are adversarial attacks harmful to users? We perform a pre- liminary test to see the extent to which static-only adversarial examples evade the full AV pipeline and infect users. We create adversarial samples by modifying 30 ransomware samples and test whether the samples that evade static classifiers can infect usersâ€™ machines. We hypothesize that the dynamic and behavioral classi- fiers of the AVs will detect and stop the static adversarial examples when they are executed, thus, posing no real harm to the users. Except for AV2, all the other AVs blocked the execution of adver- sarial ransomware samples. All of the 30 adversarial ransomware samples evade the behavior detector of AV2; files are encrypted and blackmail messages are shown on the screen. However, the online version of AV2 can detect all the samples as AV2 relies heav- ily on cloud techniques. This represents a potentially new attack surface to investigate in the future, where static-only evasion can sometimes evade the entire AV pipeline and infect users due to the design decision of an AV.

Recommendation for Antivirus Systems. Our attacks demon- strate that static classifiers are easy to evade. So for full protection, commercial AV systems need to rely more on dynamic and behavior- based detection. Even though some papers already demonstrated that dynamic classifiers can be evaded by splitting a malware sample into many different pieces [29, 50], currently no one demonstrated a scalable and generalized attack against dynamic classifiers.

Recommendation for Researchers. We demonstrate how adver- sarial examples can be used to explain a complex blackbox system. When training malware classifiers, researchers should use explana- tion techniques to understand the behavior of the classifiers and check if the learned features are fragile features that can be easily evaded or if they conflict with expert knowledge. We also argue that for security applications, demonstrating harm to real users is crucial to understanding the real ramification of an attack.

The generality of our evasive techniques. First, our proposed framework conducts a blackbox attack against classifiers. Unlike whitebox attacks, it does not require knowledge of the architecture and parameters of the target classifier. Theoretically, our approach can be used on any malware classifier, as long as the classifier returns a label for testing samples. Second, we attacked 7 represen- tative malware detectors of diverse techniques, including a decision tree-based model (EMBER), a deep learning model (MalConv), and 3 commercial AV engines from top-level AV companies. The sig- nificant evasion rate improvement of these detectors proves the generality of our method.

Mitigation using Dynamic Detection Our solution cannot by- pass dynamic detectors, but we argue that dynamic evasion is an- other research topic. Static evasion itself is an important research direction because it provides a defense before users execute poten- tially dangerous programs. This is why ML-based static classifiers, such as EMBER and MalConv, increasingly attract attention in the security community.

# 7 RELATED WORK



Adversarial attacks on machine learning is a rapidly growing field. Since 2014, there has been more than 1400 papers on adversarial attacks and defense. However, only about 42 papers focused on the malware domain, the rest focuses on the image domain. These works performed attacks on Android malware [19, 20, 22, 30, 33, 36, 39, 45, 47, 52, 53, 65, 69], PDF malware [14, 21, 44, 68], Windows malware (PE files) [3, 4, 13, 17, 24, 26â€“28, 31, 32, 35, 37, 40, 49, 57, 58, 63, 64], IoT malware [2] and Flash-based malware [43]. We compare ourselves with the papers on attacking Windows malware detectors (See Table 4). In this domain, only three papers performed blackbox attacks [4, 11, 23] where an adversary has only external access to the malware detector and also tested their approach against commercial AVs. Ceschin et al. [11] developed a white-box attack using open-source models and then submitted the examples to VirusTotal. We refrained from submitting adversarial samples to VirusTotal because many AVs use these samples to retrain their model and the adversarial samples can cause model poisoning. Fleshman et al. [23] performed blackbox random attacks against four commercial AVs. Anderson et al. [4] propose a reinforcement learning framework gym-malware to performed blackbox attacks against malware classifier EMBER. However, their method only shows about 15% improvement over random selection. Our attack demonstrates a much higher evasion rate against commercial AVs. We also perform an in-depth analysis of the AVs to understand why evasion was successful. Ashkenazy et al. [6] performed a targeted attack against Cylance, a machine learning-based malware detector, which cannot be generalized to other AVs.

Only two papers verified the functionalities of the adversarial malware samples using the Cuckoo sandbox [11, 59]. However, none of them used adversarial examples to interpret how anti- virus systems work. Our work is the first to generate minimized adversarial examples that can be used for blackbox interpretation of anti-virus systems.



https://nicholas.carlini.com/writing/2019/all- adversarial- example- papers.html



# 8 CONCLUSION



We design a reinforcement learning guided framework MAB-Malware to perform adversarial attacks on state-of-the-art machine learning models for malware classification and top commercial antivirus static classifiers. We model the action selection problem as a multi- armed bandit problem. During the attack, MAB-Malware infers the property of actions and dynamically adding new machines with unseen successful content. It finds an optimal balance between exploitation and exploration to maximize the evasion rate within limited trials. The Action Minimization module of MAB-Malware filters out the actions that are ineffective for adversarial sample generation and only change minimal features, so our framework can also be used to explain why evasion occurs. For each commer- cial antivirus system, we compute the effectiveness of each action and the key features that cause evasions. Our results show that MAB-Malware largely improves the evasion rate over other rein- forcement learning frameworks and that some of the adversarial attacks are transferable between different antivirus systems that are similar to one another.















