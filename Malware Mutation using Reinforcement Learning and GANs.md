![horizontal line](https://lh5.googleusercontent.com/GPQkahV4bLiBBEZX11kder2Y-tQIZvq_2mcwDA9uf08Oli_o0WwWnqXLfhd2lOBe8NPLCk3j_r8AQhYelGM94sP0vA9M5ORlNnT7lwnTUrzUotI5Ft0d_aAxJI_-la53PiJBtBQP)**PE****SIDIOIUS**Malware Mutation using Reinforcement Learning and GANs
**INTRODUCTION**Over the past two decades, numerous research has been conducted on using AI to detect malware by extracting features and then classifying them using machine learning algorithms. As Hu and Tan point out [5], this has led to several malware authors to focus their time and effort to attacking such malware detection techniques. Our team aims to accomplish the same. The purpose of our project is to use artificial intelligence to mutate a malware sample to bypass anti-virus agents while keeping its functionality intact. In the past, notable work has been done in this domain with researchers either looking at reinforcement learning or generative adversarial networks as their weapons of choice to modify the states of a malware executable in order to deceive anti-virus agents. After extensive research on the use of RL and GAN in this domain, we noted some of the limitations that arise when attempting to implement these solutions independently. The solution we propose in this document aims to overcome these limitations by implementing an innovative and novel approach to the generation of evasive malware by using a combination of reinforcement learning and generative adversarial networks. **LIMITATIONS OF PREVIOUS WORKS**A prominent research conducted in malware manipulation using reinforcement learning was proposed by the Endgame team. Endgame’s gym-malware research focuses on a malware manipulation environment for OpenAI gym which can be used for developing and comparing different reinforcement learning agents. Gym-malware is an environment that can take a malware sample and output the feature state, perform a selected manipulation and provide rewards based on a black-box classifier. The research mainly focuses on developing the environment and not the RL agent itself. The code used for the manipulation of the malware samples is outdated and also results in breaking the functionality of the malware and hence different actions had to be implemented. Research conducted by Fang et. al. [3] makes use of the above environment to design a deep reinforcement learning agent. The implementation of the research paper has not been made available and hence the research was just referred to give us an idea on how to implement the deep reinforcement learning agent. Several contributions have also been made in the domain of malware mutation using GANs, with one of the most prominent solutions being put forward by Hu and Tan in [5]. They propose a solution, dubbed “MalGAN”, that outlines how a feature vector comprising of a malicious binary’s imports features can be fed to a GAN to generate adversarial features that are capable of tricking a black-box detector into misclassifying the adversarial malicious features as benign. The paper boasted a TPR of 0% from 97% using machine learning algorithms like *Logistical Regression, SVM, Multilayer Perceptron* and *VOTE* and a TPR of 20% from 97% using *Random Forest* and *Decision Tree* as seen in table 1. The research, however, did not provide any working repository for us to test their results making it impossible for us to validate. Fortunately, there were implementations of the solution most notably by Yanming Lai [[link](https://github.com/yanminglai/Malware-GAN)]. ![img](https://lh3.googleusercontent.com/d7yqW55CiXwm3_tvUQBQwQYym24wehU4Kw9S8K3JEwmzkp5y_29wKaebguM7rXP-xDFuBEH0Uh9j4y4lQ8DWvKLsBO5mcs1ayzeVhtGn2h0-asYbPOaKdsNHaUKHvBZt0IhA8uNI)A major drawback of this solution, however, was in its limited scope. The solution proposed was only targeting a feature vector comprised of a small part of the binary that is it’s import function features (refer to section 4.2.1. for more details). It also did not cover an end-to-end approach with a malware binary, resulting in unrealistic results.  **SOLUTION** Our solution makes use of a combination of deep reinforcement learning and Generative Adversarial Networks in order to overcome some of the limitations faced while using these approaches independently, as discussed below and seen in figure 1.The use of deep reinforcement learning alone would work with actions like appending bytes to a section, appending bytes to the binary itself, removing the signature and packing/unpacking the file as it does not require further decisions to be made. However, new import functions and unused sections to be added can either be randomly selected, as done in previous research or can be selected using artificial intelligence. As the total number of imports and sections to choose from is thousands in number, training an RL agent for this would be inefficient in terms of time and physical resources. ![img](https://lh6.googleusercontent.com/3SbS4utGuDa_RD3X1YqGOSIQ_-RiFhcLbIPPBwiadAWxFZ-lERjEuGeCBhSNWl77WxMWqD_Y67o3w-kECpVU-wQ2bk7PGN7NHBFIXn719eGFLyzgYkPnpQ0QC6vKvArWfLQWjkCt)*Figure 1. Total actions space with the union of DRL and GAN*The alternative approach would be with the use of GANs where a GAN could be used to adversarially generate feature vectors for import functions that reduce a malware’s true positive rate. There are, however, limitations of using the GAN approach alone. For starters, GANs can not be used to generate examples that are not inherently present in benign files, such as additional bytes at the end of a section or the file itself. Other singular tasks such as removing the signature or modifying the header checksum also fall out of scope for GANs.The proposed solution successfully generates a mutated malware sample by using reinforcement learning to decide on the sequence of modifications to make. In case the modifications chosen by the RL agent is either adding import functions or adding/renaming section names, GANs are used to generate an adversarial feature vector of imports and sections that perturb a malware to appear benign in contrast to randomly selecting the imports and sections. While designing our solution, the first step was to figure out what manipulations can be made to mutate a malware file without breaking it. Based on our research and experimentations (as explained below), we narrowed down the actions required to achieve our objective to 4 mutations which include, *appending unused import functions*, *appending unused section names*, *renaming sections* and *appending bytes to the end of the file.***Solution Architecture**The architecture of the proposed solution is shown in figure 1. ![img](https://lh5.googleusercontent.com/n1QKa8hQE3qVkYtr6qR9LRyhOY5uwpt43f7OMCaiNJafzn6vg_hNQQEvtXUgi80FfkPI9jHMQW-35H06Dmt4eMwAXKN82kW5ZyQLEdAgrFjJEmgQw-n2Uj3SDosz8J6AYM0czmRE)
*Figure 1. Total actions space with the union of DRL and GAN*The solution is divided into two main components, i.e. the Generative Adversarial Network and the Deep Reinforcement Learning modules. The malware samples are required to go through a certain amount of feature extraction prior to sending it to the GAN. A script has been developed that takes in both the malware and benign samples and extracts their PE binary features like the import functions and section names. This is stored in a feature vector mapping dictionary for ease of retrieval and future manipulation. Once the feature vector mapping has been generated, we generate the individual feature mappings for each malware and benign files using the feature vector mapping dictionary that will be sent to the GAN as input. The GAN takes in the feature vectors for the malware samples and concatenates it with a noise vector which is randomly populated with 1s and 0s. (For a more detailed explanation of the GAN refer to section 4.2.3.) After running for a few epochs, the GAN is able to generate a number of adversarially generated feature vectors with import functions and section names which have a TPR of 0%. Once the GAN completes the generation of the adversarial feature vector for the imports and sections, the deep reinforcement learning uses it as an input when the action selected by the agent is adding imports, adding sections or renaming sections. The environment and the agent work together to mutate the malware sample and calculate reward based on a sequence of actions. After every interval, a batch of experiences sampled using priorities is used to calculate the loss which is then and backpropagated to the Q network to update the weights. To avoid the problem known as moving target(For a more detailed explanation refer to section 4.2.2), a Q target network is used separately which is only updated at a certain interval. After the training is complete, the Q network is used as the optimal policy network to generate the most optimal sequence of mutations to be performed on a malware sample to bypass AV scanner and maintain its functionality. The model is tested after a certain number of episodes to check the success rate and if it exceeds a threshold, the training is stopped and the model is saved to be used for malware mutation.  **Solution Components**Portable Executable FilesFor the scope of this project, only PE binaries were experimented on, however, we speculate that the architecture solution we have designed would be perfectly capable of producing similar results on other executable formats such as ELF, MachO and Android formats like OAT, DEX, VDEX and ART. We will restrict this section exclusively to the different components of the PE binary that have been exploited in the solution. However, we highly encourage readers to read through the structure of the PE file [2] in order to better understand how the different sections of the binary can be exploited, and to what extent. It should also be kept in mind that modifications must be attempted while keeping the true functionality of the malware intact. A simple breakdown of a PE file is shown in figure 2, highlighting the different components that have been used by our solution.
![img](https://lh3.googleusercontent.com/mQTEJn7f5RrnlmBMW_UIiwK0TInb5DFAi8clXhAu1d1-mKJNj4OY5Kc8gqjvRl-9QEYqJoc_ql2DpC6v2XpOmRj4v-_U4jJC8egejJ9JHKsoNUh35uzUf47hM8qRGlgPpgDg80G-)Figure 2. Breakdown of a Windows Portable Executable file. After extensive research on the structure and functions of the different components in the PE file, we concluded to narrow our modifications to the section table (Section Headers) present in the headers and the import functions present in the sections compartment of the binary file. The section table is comprised of multiple rows of section headers and contains information related to the various sections available in the image of an executable file such as section name, virtual size, virtual address, size of the raw data, pointers to the raw data and characteristics amongst others. During experimentation, a script was run to collect all the unique section names and import functions occurring in our malware and benign dataset. It was observed that there are certain sections that frequently appear in PE binaries such as the .text, .data, .rdata, .idata, .reloc, .rsrc and .debug. Another observation was that renaming existing sections, adding random bits to existing sections and adding unused sections in the PE file not only decreased the true positive rate of the malware but also kept its functionality intact. These observations were also supported by the observations of Anderson et. al [1] leading us to select the sections table as one of the components of the PE file to modify. Another popular approach stated by Anderson et. al [1]  to reduce the TPR of the malware was adding unused functions to the import table contained in the .idata section. A major purpose of the import table is to make modularity easier. Although the modification of the imports tables greatly contributed to reducing the TPR of the malicious file, it didn’t perform as well in keeping its functionality intact. Based on our experimentation we came to the conclusion that malwares that perform LEA action on the Import Address Table (IAT) do not function well when additional imports are added. Additionally, there are other actions such as appending random bytes to the binary, packing and unpacking the PE file, removing the signature and modifying the header checksum that can be performed to reduce the TPR of the malware. These actions have also been explored in our solution. During our experimentation phase, when training the RL agent, we realised that out of all the actions the agent would choose one of the following actions no matter what the input was i.e. appending to section, renaming section, adding imports and adding random bytes to the binary. Therefore, we decided to move forward with training the agent again with only these 4 actions. Deep Reinforcement LearningReinforcement Learning is an area of machine learning that trains an agent to take the right actions in an environment. It does this by giving reward for every right action taken by the agent and occasionally negative rewards for wrong actions. The agent’s goal is to maximize cumulative reward. After a certain number of episodes, the agent learns to navigate the environment on its own. In our scenario, the problem can be solved using a similar approach. The malware’s features can be an environment; the agent’s goal would be to mutate the malware by performing some actions and the reward will be based on the detection rate by the AV scanners. The learning (Q-learning) happens by building an optimal policy which aims to maximize the expected value of the reward for next and successive steps, starting from the current state. The input to the Q-learning algorithm will be the environment’s state and the output will be the q-values for all the possible actions. It updates the policy based on the current state, action taken during exploration, reward received and the next state. Deep learning is a part of machine learning methods based on artificial neural network. The network is generally built using one input layer, one or more hidden layers and an output layer. Each layer of the network learns to transform its input data. Our solution includes implementing an algorithm based on a research paper [3] where a deep Q network is used that can take the features of the malware as an input and give the q values for the different actions. The algorithm uses two networks i.e. Q-value and Q-target to compute the loss function for backpropagation. The way it works is that one network (Q-value) is responsible for selection action for the current state and the target network (Q-target) is responsible for the evaluation of the action. To avoid the problem of moving target caused by updating the weights for both current and targeted q values, two networks are used where only one network’s (Q-Value) weights are updated and the target network is kept fixed for an interval. Additionally, experience replay is used to let the agent reuse past experiences for the training. At each training step, we store a tuple of state, action, next_state, reward. We further implemented prioritized experience replay to make sure no experience is left out because of sampling.Based on previous research and our own experimentation the features extracted from the malware sample as input are the byte histogram normalized to sum to unity and two-dimensional entropy histogram. The action space includes adding sections, adding import libraries and functions, renaming sections and appending bytes to the binary. 
Generative Adversarial NetworksA Generative Adversarial Network or GAN is a class of neural network that allows the network to generate data with an internal structure resembling that of the input dataset. This is accomplished by putting two complementary neural networks, a generator and a discriminator against each other. As the name suggests the purpose of the generator is to generate new adversarial examples based on the input vectors while the discriminator tries to distinguish whether the adversarial example resembles the initial input vector. Commonly, GANs have been used in the domain of image or video generation, whereupon feeding in enough images as the input, the GAN is able to generate a set of images which resembles that of input but unique from the input itself, in other words, it learns from the internal features of the input, making it a flexible and powerful tool. This led us to use GANs in adversarially generating malware samples with a lower malicious true positive rate. Following our initial research, we were highly impressed by the works of Hu and Tan [5] in generating adversarial malware examples for attacks based on GAN and their solution, which they dubbed “MalGAN”. Our solution involves the implementation of MalGAN and builds upon the caveats of their solution. Figure 3. Illustrates a modified architecture of the MalGAN model. ![img](https://lh4.googleusercontent.com/NlSgK4KerpWd99yCrnO8UVmtV0RoUBc4uO6hHRwTesjHE2-kqFD4t3ahLHzWwdr-wxmT6b8q3k-FuNEO43mwryGcErQSLLqncZfsUlHM1iFwrhdKM3dtrOmGq_02U-Yfow_R_E49)*Figure 3: The Malware Generative Adversarial Network*The MalGAN architecture primarily consists of three components, the black box detector and the generator and discriminator from the generative adversarial network described earlier. **4.2.3.1. Blackbox Detector**The purpose of the Blackbox detector, derived from the black-box nature of the AV that detects a malware, is to predict whether the adversarially generated feature vector is malicious or benign. This unit is independent of the rest of the generative network and does not get retrained at any point. As the detector is a black box, no information of its underlying machine learning models is used by the generator. The black-box detector was trained with popular machine learning algorithms used in malware analysis like Random Forest, Decision Trees, Logistical Regression, SVMs, VOTE Ensemble and multilayer Perceptron. **4.2.3.2. Generator**The generator takes in the feature vector (such as sections and imports as discussed in section ) of the malicious binary and a randomly generated noise vector with the values 1 and 0 as input. The two vectors are concatenated and are passed as input to the generator which alters it to generate an adversarial feature vector of the same size as that of the input vector, with the objective of being labelled as benign by the black-box detector. The generator’s feedforward network with two hidden layers produces a vector comprising of fractional values, which is not of much use to the binary nature of the solution. In order to overcome this limitation, the fractional values are converted to 1s and 0s based on a threshold usually set at 0.5. The originally generated feature is still retained for backwards propagation. The original paper puts forward another requirement of XORing the modified generated feature vector with that of its original counterpart. However, this was ignored in our approach as the MalGAN proposed by Hu and Tan were only aimed at generating adversarial imports to be added, while the proposed solution aims to expand beyond that and generate alternative features as well like section names. In order to overcome the limitation of not eliminating non-unique features (imports and sections alike), we take this step into consideration while reconstructing the malware binary. **4.2.3.3. Discriminator** The major limitation of the detectors black-box nature lies in the amount of information it is able to output. The black-box detectors labels, i.e. benign or malicious, is insufficient information for the generator to efficiently learn how to construct better evasive adversarial examples. The discriminator aims to overcome this limitation by learning the approximation of the blackbox detectors decision function. This differential function is then given to the generator to construct a better gradient for learning. **EXPERIMENTS****Experiment Setup**SamplesMalicious samples used for training and testing the solution were acquired from VirusTotal. There were a total of 30,000 samples and after filtering out these based on packed/unpacked and the families of malware we had 5000 backdoors and unpacked samples. For the scope of the project, we only focused on one type of the family i.e. Backdoors. The reason to train the AI agent with only one family is that the feature set used as input is more consistent and that the agent will need less time to train as the number of different inputs reduces in size. The benign sample set used by the GAN was acquired by collecting all the benign executables running on the team’s host computer and totalled to an amount of approximately 4000 files. Training**Action space for RL agent** : After training the agent on all the 8 actions it was observed that no matter what the input, the agent would always choose from only 4 actions i.e. adding sections/imports, renaming sections and appending overlay. Hence the agent was trained again with only these 4 actions**Feature space for RL agent** : This was based on previous research done which showed that byte histogram normalized to sum to unity and two-dimensional entropy histogram would be able to give the state of malware for the input. The feature vector had 518 elements which were then normalised to a value between -0.5 and 0.5.**Deep Neural Network for RL agent :** This was based on previous research which uses a neural network as the following: hidden layer with 256 nodesrectified linear unit functionhidden layer with 64 nodesrectified linear unit function**Exploration vs Exploitation for RL agent :** To make sure that the agent derives the optimal policy, it’s important to find the right balance between exploration (taking random actions) and exploitation (taking a greedy action). After training the agent with different minimum epsilons, it was observed that after a certain number of episodes the probability of an agent choosing a random action should be 0.2.**Reward for RL agent :** The reward for each action is given based on the score of the classifier used. For our project due to the limited time we have used a pre-trained classifier based on gradient boosting by gym-malware. The reward was calculated as the difference in the score of the original sample and the mutated sample after every action. The episode ends once the score goes below a certain threshold. Results of different threshold experimented with can be seen below.**Feature Vector Size for GAN:** The GAN was initially given an input feature vector of 128 features from a test implementation by [yanminglai](https://github.com/yanminglai/Malware-GAN) and a noise vector of 10. Although the research paper claimed a TPR of 0%-10% for the adversarially generated feature this value was, in reality, closer to 40%-60%. Following these preliminary findings, we took the liberty of generating our own feature vector by mapping all the unique imports and sections from our malware and benign dataset. The result of this was a 7028 feature vector mapping comprising of 6725 imports and 304 sections resulting in a TPR of 0%. **Detector Algorithm for GAN:** The black-box detector was trained through a collection of machine learning algorithms (refer to Section 4.2.3.1.) and it was observed that out of all the algorithms, *Random Forest* always had the highest TPR often reaching values between 60%-80% with a feature vector size of 128 features, while other algorithms like the *Logistical Regression* and *Decision Trees* ranged between 35%-45% and Multilayer Perceptron producing in the most promising results randing between 0%-20%. With these preliminary results, we decided to focus on training the model to reduce the TPR when trained with the Random Forest algorithm. **Activation Function for GAN:** The GAN was also trained with multiple activation functions like ReLE, LeakyReLU, ELU, TanH and Sigmoid. However, the activation function didn’t play a significant role in the TPR and was hence ignore for later training phases. 
**Experiment Results**One of the challenges faced working on the project is that using VirusTotal API as a blackbox detector during the training and testing phases for the RL agent and GAN was difficult as the analysis of a file takes time which slowed down the process. Hence throughout the process, a local blackbox detector was used to calculate rewards for the RL agent and generate an adversarial feature vector for GAN. To check if the mutated malware was functional, it was uploaded to cuckoobox. Only the samples that executed and generated some dynamic features were considered functional. Some statistics regarding our experimentation can be seen in Table 1.**Results comparing the number of functional mutations generated when trained with different thresholds for detection and maximum mutations allowed (Testing Data : 250 samples)**Threshold for detectionMaximum mutations allowedMutations GeneratedFunctional MutationAverage VirusTotal Score 90 %8014011440/6985 %1201156229/6980 %160942411/69*Table 1: Results comparing the number of functional mutations generated with different thresholds for detection* **Results comparing the impact blackbox detector algorithm and activation function have on the TPR of the adversarially generated feature vector**While testing the accuracy of the MalGAN model, various machine learning algorithms were considered to test the performance of the blackbox detector. Different activation functions were also explored for the feed-forward neural networks in the Generator and the Discriminator to observe the impact it has on the TPR of the adversarially generated feature vector. The below table, table 2 looks at *the size of the feature vector (M),*  *the size of the noise vector (Z)*, *The number of epochs, the test size, detector type, activation function, average validation, average number of bits changed, original malware’s feature vector TPR, Modified malware’s feature vector TPR* and *the TPR for a benign sample* to test the accuracy of the model in different configuration settings. **![img](https://lh3.googleusercontent.com/IE9Wl0OhGinLahjkWXcWZeXJAOPhCkjvEaJMxke4OBcgT_b6GROvNxl11kLTR-l8IFDBHeXDjHs3-oGOBrDGnZYn3aLHXVd6V-v8Gk_gvz50Mx4G5lBfXcAm2vdUUvzMhreNIr61)***Table 2: Results comparing the impact blackbox detector algorithm and activation function have on the TPR of the adversarially generated feature vector*Testing was conducted on the blackbox detector, to observe how different blackbox machine learning algorithms affect the training process in GAN. It was observed that *Random Forest* provides the most stringent TPR for an adversarially generated malware, resulting in a TPR of 63% from 98% when trained on a data set of 128 feature vectors and an activation function of LeakyReLU. For this reason, the Random Forest algorithm was considered for the next series of experimentations with the activation functions of the neural networks. When experimented with the different activation functions, we observed the high TPR for adversarially generated feature vectors when Sigmoid was used in the generator and the discriminator, leading us to narrow down the activation function for later stages of testing and experimentation.**Results comparing the impact the size of the feature vector have on the TPR of the adversarially generated feature vector****![img](https://lh5.googleusercontent.com/WgL9s3YosRY5mZfSTn4I3nO_8jBT6oYTAgWfNMP_GYGvYAaRcxMcnokflr6UGPJhGA5iJongZv9hThG9wL1QyG6I2SyvJyeax8H6ux3wp5XZyKiyJVkXyq90MBH3QqnUi4ek9OhG)***Table 3: Results comparing the impact the size of the feature vector have on the TPR of the adversarially generated feature vector*
Once we were done testing the different detector types and the activation functions, we explored the impact of changing the number of features in our feature vector. In order to do so, we extracted the features (section names and import functions) from approximately 3000 malicious and benign binary samples, resulting in a feature vector of 7029 as seen in Table 3. The results were exceptional, with a drop from a TPR of ~90% to ~0% for the adversarially generated feature vector. This was tested on different detector algorithms and the same result was observed, including for the *RandomForest*. **FUTURE WORK AND CONCLUSION**
The RL agent and GAN were both trained using our own blackbox detector. Hence, even though the agent was able to learn to evade the local detector well enough when uploading the mutated sample to VirusTotal, the detection rates did not match up. In order to overcome this limitation, we plan to substitute the blackbox detector with VirusTotal API for the training and testing of the solution. As observed in the experiment results, increasing the number of mutations resulted in the decrease of functional mutated samples. Additional research needs to be conducted in the future to narrow down the specific mutations causing the malfunction.

**REFERENCES**
[1] 	Anderson, H., Kharkar, A., Filar, B., Evans, D. and Roth, P. (2018). Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning. [online] arXiv.org. Available at: https://arxiv.org/abs/1801.08917.[2]	Docs.microsoft.com. (n.d.). PE Format - Windows applications. [online] Available at: https://docs.microsoft.com/en-us/windows/win32/debug/pe-format#general-concepts.[3]	Fang, Z., Wang, J., Li, B., Wu, S., Zhou, Y. and Huang, H. (2019). Evading Anti-Malware Engines With Deep Reinforcement Learning. [online] Ieeexplore.ieee.org. Available at: https://ieeexplore.ieee.org/abstract/document/8676031 [Accessed 25 Aug. 2019].[4]	https://resources.infosecinstitute.com. (2019). Malware Researcher’s Handbook (Demystifying PE File). [online] Available at: https://resources.infosecinstitute.com/2-malware-researchers-handbook-demystifying-pe-file/#gref.[5]	Hu, W. and Tan, Y. (2018). Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN. [online] arXiv.org. Available at: https://arxiv.org/abs/1702.05983.