**Adversarial-Example Attacks Toward Android Malware Detection System**

 

**Abstract:**

Recently, it was shown that the generative adversarial network (GAN) based adversarial-example attacks could thoroughly defeat the existing Android malware detection systems. However, they can be easily defended through deploying a firewall (i.e., adversarial example detector) to filter adversarial examples. To evade both malware detection and adversarial example detection, we develop a new adversarial-example attack method based on our proposed bi-objective GAN. Experiments show that over 95% of adversarial examples generated by our method break through the firewall-equipped Android malware detection system, outperforming the state-of-the-art method by 247.68%.

**Published in:** [IEEE Systems Journal](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4267003) ( Volume: 14, [Issue: 1](https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9020228), March 2020)

**Page(s):** 653 - 656

**Date of Publication:** 11 April 2019 

**ISSN Information:**

**INSPEC Accession Number:** 19428513

**DOI:** [10.1109/JSYST.2019.2906120](https://doi.org/10.1109/JSYST.2019.2906120)

**Publisher:** IEEE

 **Funding Agency:**

**SECTION I.**

## Introduction

As the most popular mobile operating system, Android captures roughly 85% of the worldwide smartphone volume [1]. It was reported that 97% of all mobile malware target Android devices [2], hence raising an urgent demand for Android malware detection. In essence, Android malware detection is a binary classification problem. It uses a well-trained classifier (i.e., malware detector) to determine whether an unseen app is malicious or not.

In practice, Android malware detection can be conducted remotely in clouds. Since clouds are vulnerable to attacks [3], in-cloud Android malware detection has attracted increasing attention from attackers. Recently, adversarial examples [4]–[7] were employed to mislead in-cloud Android malware detection systems. An adversarial example is a malicious input that can fool the classifiers. With an adversarial example, an attacker can build a malicious app that is able to evade Android malware detection.

MalGAN [6] is the pioneer work on black-box adversarial-example attacks toward Android malware detection. It generates adversarial examples using a generative adversarial network (GAN) without requiring the knowledge about the target. This excellent method is superior to the other related methods, as it can decrease the detection accuracy of almost all classifiers, including support vector machine (SVM) [8], Adaboost, and convolutional neural network (CNN) [9] to nearly zero. However, MalGAN cannot work once the targeted detection system is equipped with a firewall. Here the firewall is an adversarial example detector, which corresponds to a binary classifier in practice. Our experiments demonstrated that over 90% of the adversarial examples generated by MalGAN were captured by the firewall-equipped detection system in some cases. Some of these failed examples evaded malware detection, but failed to break through the firewall. The others defeated the firewall, but could not evade malware detection.

In this paper, we develop an evasive adversarial-example attack method E-MalGAN to mislead the in-cloud firewall-equipped Android malware detection system. As a black-box attack method, E-MalGAN does not require any information about the target. It employs our proposed variant of GANs, called bi-objective GAN, to generate adversarial examples. In general, a GAN consists of two deep neural networks, i.e., Generator and Discriminator. However, the bi-objective GAN has two Discriminators, which lead its Generator to contend with both firewall and malware detector. Experimental results show that over 95% of the adversarial examples generated by E-MalGAN successfully circumvent both malware and adversarial example detection. To our knowledge, this is the first work that studies evasive adversarial-example attacks targeting the firewall-equipped Android malware detection systems.

**SECTION II.**

## System

Consider an in-cloud Android malware detection system that consists of an adversarial example detector and a malware detector, as shown in Fig. 1. Either adversarial example detector or malware detector is a binary classifier (e.g., SVM, AdaBoost, and CNN), which outputs a detection result when fed with an example. Here, the example is the combination of features extracted from the Android app that needs to be examined. In general, the features for malware detection can be dynamic, static, or hybrid. Extracting dynamic features requires monitoring the behavior of apps at run time, hence resulting in significant overhead. In this paper, we focus on the static features, including required permissions, actions, and application programming interface (API) calls.

[![Fig. 1. - System overview.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image001.gif)](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9020228/8688632/yuan1-2906120-large.gif)

 

**Fig. 1.**

System overview.

[Show All](https://ieeexplore.ieee.org/document/8688632/all-figures)

In our system, an example can be normal or adversarial. A normal example is composed of the features extracted from a benign or malicious Android app. To produce an adversarial example, an attacker first extracts features from a chosen *malicious* app, and then combine them with some irrelevant features, in order to mislead the detection system to give a BENIGN result [6]. If an example is detected to be adversarial by the adversarial example detector, the example will be blocked and the detection system will send back a detection result of MALICIOUS. Otherwise, the example will be forwarded to the malware detector for further detection, followed by sending back a detection result of BENIGN or MALICIOUS. Here we point out that a detection result of ADVERSARIAL should never be sent to the sender, as this result is helpful for an attacker to improve his/her attack model.

In this paper, we study how to sophisticatedly generate adversarial examples to evade both adversarial example detection and malware detection. For practical consideration, we assume that the attacker has no information (e.g., the type and the parameters of the classifiers) about the detection system, and the designer of detection system has no information about the parameters of the attacker's model.

**SECTION III.**

## Methodology

In this section, we first introduce a defense method to resist the state-of-the-art attack method MalGAN, and then describe our attack method E-MalGAN in detail.

### A. Defense

In our system, the adversarial example detector acts as the firewall. It is built according to the following steps.

#### 1) Data Collection

The administrator of the detection system obtains normal examples through extracting features from benign and malicious apps, and then uses a certain method (e.g., MalGAN or E-MalGAN) to generate adversarial examples.

#### 2) Model Training

The administrator uses both normal and adversarial examples together with their labels to train a binary classifier. Here, the labels are set to 1 for adversarial examples, and 0 for normal examples.

#### 3) Adversarial Example Detection

The administrator uses the well-trained classifier as a firewall to block the adversarial examples invading the detection system.

Due to space limit, we omit the discussion on how to build and deploy a malware detector in this paper. The details can be found in existing literature on malware detection (e.g., [1] and [10]). In addition, to confuse potential attackers and hide internal details, the in-cloud detection system never reveals which detector gives the detection result. As will be discussed in Section IV, the above-mentioned defense method is effective to MalGAN.

### B. Attack

Now we develop a black-box attack method that generates adversarial examples hard to detect. To this end, we design a GAN model with two optimization objectives.

#### 1) Preliminaries

GANs work by playing a noncooperative game between Generator and Discriminator. Generator attempts to create examples that resemble a real distribution. Discriminator examines examples to determine whether they are real or forged by Generator. Either Generator or Discriminator evolves to best the other, until Generator generates the examples indistinguishable to Discriminator.

#### 2) Motivation

Generating adversarial example is challenging when the targeted classifier is unknown to the attacker. To overcome this challenge, MalGAN uses Discriminator to fit the targeted classifier. The intuition behind this method is that the adversarial examples fooling Discriminator is likely to mislead the targeted classifier. However, this method does not take effect in our system, since a negative detection result does not indicate whether the corresponding example is adversarial or malicious. With *incomplete information*, MalGAN often gets confused and fails to evade detection.

#### 3) Model and Algorithm

Here we propose a new variant of GANs, i.e., bi-objective GAN, to generate adversarial examples with incomplete information. The bi-objective GAN extends the traditional GANs by adopting two Discriminators with different objectives. As shown in Fig. 2., Generator generates candidate adversarial examples; Discriminator 1 attempts to distinguish malicious examples from benign examples, and Discriminator 2 tries to distinguish adversarial examples from normal examples. In this GAN model, both Discriminators lead Generator to simultaneously fulfill two goals, i.e., breaking through the firewall and evading the malware detection.

[![Fig. 2. - Bi-objective GAN.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image002.gif)](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9020228/8688632/yuan2-2906120-large.gif)

 

**Fig. 2.**

Bi-objective GAN.

[Show All](https://ieeexplore.ieee.org/document/8688632/all-figures)

We design a training procedure to optimally determine the parameters of this bi-objective GAN. In each round of training, we use gradient descents to update the parameters of Discriminator 1, Discriminator 2, and Generator, denoted by θd1, θd2, and θg, respectively. Let G, D1, and D2 represent the functions implemented by Generator, Discriminator 1, and Discriminator 2, respectively. The details of training are given as follows.

For Discriminator 1, we use normal examples and their labels (MALICIOUS or BENIGN), given by the in-cloud detection system, to update θd1 by minimizing its loss

Ld1=Ex∈NBlog(1−D1(x))+Ex∈NMlog(D1(x))(1)

View Source![Right-click on figure for MathML and additional features.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image003.gif)where NB and NM denote the distributions of benign and malicious examples, respectively.

For Discriminator 2, we use two kinds of examples, i.e., normal examples and the generated examples declared to be malicious by the in-cloud detection system, to update θd2 by minimizing its loss

Ld2=Ex∈Nlog(1−D2(x))+Ex∈M~log(D2(x))(2)

View Source![Right-click on figure for MathML and additional features.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image003.gif)where N denotes the distribution of normal examples, and M~ represents the distribution of the generated examples detected to be malicious.

As for Generator, we update it twice in each round. After θd1 is updated, we update θg by minimizing

L1g=Em∈M,z∈Plog(1−D1(G(m,z)))(3)

View Source![Right-click on figure for MathML and additional features.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image003.gif)where M is the distribution of the malicious examples fed to Generator, and P corresponds to the distribution of noises fed to Generator. After θd2 is updated, we update θg by minimizing

L2g=Em∈M,z∈Plog(1−D2(G(m,z))).(4)

View Source![Right-click on figure for MathML and additional features.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image003.gif)

Let ηg, ηd1, and ηd2 represent the learning rates (i.e., step sizes) chosen for Generator, Discriminator 1, and Discriminator 2, respectively. The main training procedure is given in Algorithm 1, where the stop criterion is the maximum number of rounds allowed in training. When the algorithm terminates, the bi-objective GAN is expected to generate adversarial examples that are able to break through the Android malware detection system.

#### Algorithm 1: Adversarial Example Generation Algorithm.

\1. Initialize θg, θd1, θd2, ηg, ηd1, and ηd2;

\2. In each round:

(1) Sample normal examples N1 and label them with the in-cloud detection system;

(2) Update Discriminator 1 using N1 together with the corresponding labels by θd1←θd1+ηd1∇Ld1(θd1);

(3) Sample malicious examples M and update Generator with M by θg←θg+ηg∇ L1g(θg);

(4) Sample noise samples P={z1,z2,…,zn} and malicious examples M={m1,m2,…,mn};

(5) Generate candidate adversarial examples M~={m~1,m~2,…,m~n}, where m~i=G(mi,zi), and label M~ with the in-cloud detection system;

(6) Sample normal examples N2, and update Discriminator 2 with N2 and the labeled M~ by θd2←θd2+ηd2∇ Ld2(θd2);

(7) Use malicious examples M to update Generator with M by θg←θg+ηg∇ L2g(θg);

**SECTION IV.**

## Experiments

In this section, we conduct experiments to evaluate the effectiveness and efficiency of E-MalGAN.

### A. Features and Metric

In our experiments, the benign apps come from *Tencent Myapp* and *AndroZoo* [11], [12], and the malicious apps are from *VirusShare* and *Contagio*.1 We choose 130 permissions, 110 actions, and 91 APIs as the features. They are characterized as a 331-dimensional binary vector, in which every “1” means the corresponding permission/action/API is requested/registered/called, and every “0” indicates the opposite case. We extract these 331 features from apps directly on Android smartphones.

To quantify the efficiency of adversarial example generation, we propose a new performance metric, called *effective generation rate* (EGR). Let n represent the number of adversarial examples generated by an attacker. Let n~ denote the number of adversarial examples that successfully evades both malware and adversarial example detection. ERG is then defined as the ratio between n~ and n, i.e., n~/n.

Our experiments are conducted under nine scenarios, where SVM, AdBoost, and CNN act as the classifiers in the in-cloud detection system. In every scenario, we employ over 55 000 examples to train the malware detector. To train the adversarial example detector, we use 5000 normal examples, 2500 adversarial examples generated by MalGAN, and 2500 adversarial examples generated by E-MalGAN.

### B. Performance Evaluation

We first evaluate the convergence of our algorithm. It is observed that our algorithm reaches a relatively stable state after sufficient iterations in our experiments. For illustration, we consider the scenario where AdaBoost acts as the classifier for both malware detector and adversarial example detector. In this scenario, our algorithm runs for five times with different initial parameter values, and the iterations of Generator's loss are shown in Fig. 3. It can be seen that the plot of losses exhibits a decreasing trend and the loss gradually converges to nearly zero in all the cases.

[![Fig. 3. - Loss of generator in the scenario of AdaBoost-AdaBoost.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image004.gif)](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9020228/8688632/yuan3-2906120-large.gif)

 

**Fig. 3.**

Loss of generator in the scenario of AdaBoost-AdaBoost.

[Show All](https://ieeexplore.ieee.org/document/8688632/all-figures)

Now we evaluate E-MalGAN in terms of EGR. The experimental results are summarized in Table I. For comparison purpose, we also provide the results of MalGAN in this table. The first column in Table I indicates the setting of every scenario. For instance, the second element in this column, i.e., “AdaBoost-CNN,” means that AdaBoost acts as the classifier for adversarial example detection and CNN acts as the classifier for malware detection. In every scenario, we run MalGAN and E-MalGAN five times with different initial parameter values, and the average EGRs of MalGAN and E-MalGAN under every scenario are presented in the second and the third columns of Table I, respectively. From Table I, we can conclude that E-MalGAN performs better MalGAN under every scenario. Moreover, the average EGR of E-MalGAN outperforms that of MalGAN by 247.68%.

**TABLE I** EGRs of MalGAN and E-MalGAN

[![Table I](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image005.gif)](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9020228/8688632/yuan.t1-2906120-large.gif)

 

To thoroughly understand why E-MALGAN surpasses MalGAN, we survey the training procedures of both methods. Recall that only when the objectives of breaking through firewall and fooling malware detector are both fulfilled can an attack method defeat the detection system. However, it is observed that MalGAN attempts to fulfill one objective, but ignores the other. For illustration, we consider the EGR iterations of MalGAN in the scenario of SVM-SVM, shown in Fig. 4. It is clear that MalGAN achieves a low or zero EGR at the end of training. We check the examples generated by MalGAN in these cases, and realize that most of them succeed in fooling the adversarial example detector, but are captured by the malware detector. Hence, MalGAN cannot handle two objectives, and it is suitable for single-objective task.

[![Fig. 4. - EGR iterations of MalGAN in the scenario of SVM-SVM.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image006.gif)](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9020228/8688632/yuan4-2906120-large.gif)

 

**Fig. 4.**

EGR iterations of MalGAN in the scenario of SVM-SVM.

[Show All](https://ieeexplore.ieee.org/document/8688632/all-figures)

Contrarily, E-MalGAN is able to handle the two objectives and achieves a tradeoff between them. For demonstration purpose, we present the EGR iterations of E-MalGAN in the scenario of SVM-SVM in Fig. 5. In this figure, EGR reaches or gets close to one at the end of training, which means that almost all the generated examples evade both adversarial example detection and malware detection. We own the advantage of E-MalGAN over MalGAN to the introduction of two Discriminators, which stimulates Generator to simultaneously fulfill the two objectives during training.

[![Fig. 5. - EGR iterations of E-MalGAN in the scenario of SVM-SVM.](file:////Users/babytree/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image007.gif)](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/4267003/9020228/8688632/yuan5-2906120-large.gif)

 

**Fig. 5.**

EGR iterations of E-MalGAN in the scenario of SVM-SVM.

[Show All](https://ieeexplore.ieee.org/document/8688632/all-figures)

**SECTION V.**

## Conclusion

In this paper, we point out that aided by a firewall, an Android malware detection system can successfully capture most of adversarial examples generated by the state-of-the-art black-box attack method. We develop a new black-box adversarial-example attack method. Experiments show that over 95% of adversarial examples generated by our method are falsely detected to be benign by the firewall-equipped Android malware detection system, hence confirming the effectiveness of our method.

 

 

 

 

 