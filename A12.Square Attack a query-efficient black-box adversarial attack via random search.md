Square Attack: a query-efficient black-box adversarial attack via random search 



Maksym Andriushchenko‚àó1 , Francesco Croce‚àó2 , Nicolas Flammarion1 , and Matthias Hein2 1 EPFL 2 University of T¬®ubingen Abstract. We propose the Square Attack, a score-based black-box l2- and l‚àû-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized squareshaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1.8 and up to 3 compared to the recent state-ofthe-art l‚àû-attack of Al-Dujaili & O‚ÄôReilly (2020). Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack. 1 Introduction Adversarial examples are of particular concern when it comes to applications of machine learning which are safety-critical. Many defenses against adversarial examples have been proposed [26,62,45,5,37,1,7] but with limited success, as new more powerful attacks could break many of them [12,4,40,14,63]. In particular, gradient obfuscation or masking [4,40] is often the reason why seemingly robust models turn out to be non-robust in the end. Gradient-based attacks are most often affected by this phenomenon (white-box attacks but also black-box attacks based on finite difference approximations [40]). Thus it is important to have attacks which are based on different principles. Black-box attacks have recently become more popular [41,9,51] as their attack strategies are quite different from the ones employed for adversarial training, where often PGD-type attacks [37] are used. However, a big weakness currently is that these black-box attacks need to query the classifier too many times before they find adversarial examples, and their success rate is sometimes significantly lower than that of white-box attacks. ‚àóEqual contribution. 2 M. Andriushchenko et al. Fig. 1. Avg. number of queries of successful untargeted l‚àû-attacks on three ImageNet models for three score-based black-box attacks. Square Attack outperforms all other attacks by large margin In this paper we propose Square Attack, a score-based adversarial attack, i.e. it can query the probability distribution over the classes predicted by a classifier but has no access to the underlying model. The Square Attack exploits random search3 [46,48] which is one of the simplest approaches for blackbox optimization. Due to a particular sampling distribution, it requires significantly fewer queries compared to the state-of-theart black-box methods (see Fig. 1) in the score-based threat model while outperforming them in terms of success rate, i.e. the percentage of successful adversarial examples. This is achieved by a combination of a particular initialization strategy and our square-shaped updates. We motivate why these updates are particularly suited to attack neural networks and provide convergence guarantees for a variant of our method. In an extensive evaluation with untargeted and targeted attacks, three datasets (MNIST, CIFAR-10, ImageNet), normal and robust models, we show that Square Attack outperforms state-ofthe-art methods in the l2- and l‚àû-threat model. 2 Related Work We discuss black-box attacks with l2- and l‚àû-perturbations since our attack focuses on this setting. Although attacks for other norms, e.g. l0, exist [41,19], they are often algorithmically different due to the geometry of the perturbations. l2- and l‚àû-score-based attacks. Score-based black-box attacks have only access to the score predicted by a classifier for each class for a given input. Most of such attacks in the literature are based on gradient estimation through finite differences. The first papers in this direction [6,30,56] propose attacks which approximate the gradient by sampling from some noise distribution around the point. While this approach can be successful, it requires many queries of the classifier, particularly in high-dimensional input spaces as in image classification. Thus, improved techniques reduce the dimension of the search space via using the principal components of the data [6], searching for perturbations in the latent space of an auto-encoder [55] or using a low-dimensional noise distribution [31]. Other attacks exploit evolutionary strategies or random search, e.g. [3] use a genetic algorithm to generate adversarial examples and alleviate gradient masking as they can reduce the robust accuracy on randomization- and discretization-based defenses. The l2-attack of [28] can be seen as a variant of random search which chooses the search directions in an orthonormal basis and tests up to two candidate updates at each step. However, their algorithm can have suboptimal query efficiency since it adds at every step only small (in l2-norm) 3 It is an iterative procedure different from random sampling inside the feasible region. Square Attack: a query-efficient black-box adversarial attack 3 modifications, and suboptimal updates cannot be undone as they are orthogonal to each other. A recent line of work has pursued black-box attacks which are based on the observation that successful adversarial perturbations are attained at corners of the l‚àû-ball intersected with the image space [0, 1]d [49,2,39]. Searching over the corners allows to apply discrete optimization techniques to generate adversarial attacks, significantly improving the query efficiency. Both [49] and [2] divide the image according to some coarse grid, perform local search in this lower dimensional space allowing componentwise changes only of ‚àí and , then refine the grid and repeat the scheme. In [2] such a procedure is motivated as an estimation of the gradient signs. Recently, [39] proposed several attacks based on evolutionary algorithms, using discrete and continuous optimization, achieving nearly state-of-the-art query efficiency for the l‚àû-norm. In order to reduce the dimensionality of the search space, they use the ‚Äútiling trick‚Äù of [31] where they divide the perturbation into a set of squares and modify the values in these squares with evolutionary algorithms. A related idea also appeared earlier in [25] where they introduced black rectangle-shaped perturbations for generating adversarial occlusions. In [39], as in [31], both size and position of the squares are fixed at the beginning and not optimized. Despite their effectiveness for the l‚àû-norm, these discrete optimization based attacks are not straightforward to adapt to the l2-norm. Finally, approaches based on Bayesian optimization exist, e.g. [50], but show competitive performance only in a low-query regime. Different threat and knowledge models. We focus on lp-norm-bounded adversarial perturbations (for other perturbations such as rotations, translations, occlusions in the black-box setting see, e.g., [25]). Perturbations with minimal lp-norm are considered in [15,55] but require significantly more queries than norm-bounded ones. Thus we do not compare to them, except for [28] which has competitive query efficiency while aiming at small perturbations. In other cases the attacker has a different knowledge of the classifier. A more restrictive scenario, considered by decision-based attacks [9,16,27,11,13], is when the attacker can query only the decision of the classifier, but not the predicted scores. Other works use more permissive threat models, e.g., when the attacker already has a substitute model similar to the target one [44,57,17,23,52] and thus can generate adversarial examples for the substitute model and then transfer them to the target model. Related to this, [57] suggest to refine this approach by running a black-box gradient estimation attack in a subspace spanned by the gradients of substitute models. However, the gain in query efficiency given by such extra knowledge does not account for the computational cost required to train the substitute models, particularly high on ImageNet-scale. Finally, [35] use extra information on the target data distribution to train a model that predicts adversarial images that are then refined by gradient estimation attacks. 3 Square Attack In the following we recall the definitions of the adversarial examples in the threat model we consider and present our black-box attacks for the l‚àû- and l2-norms. 4 M. Andriushchenko et al. Algorithm 1: The Square Attack via random search Input: classifier f, point x ‚àà R d , image size w, number of color channels c, lp-radius , label y ‚àà {1, . . . , K}, number of iterations N Output: approximate minimizer ÀÜx ‚àà R d of the problem stated in Eq. (1) 1 xÀÜ ‚Üê init(x), l‚àó ‚Üê L(f(x), y), i ‚Üê 1 2 while i < N and xÀÜ is not adversarial do 3 h (i) ‚Üê side length of the square to modify (according to some schedule) 4 Œ¥ ‚àº P(, h(i) , w, c, x, x ÀÜ ) (see Alg. 2 and 3 for the sampling distributions) 5 xÀÜnew ‚Üê Project ÀÜx + Œ¥ onto {z ‚àà R d : kz ‚àí xkp ‚â§ } ‚à© [0, 1]d 6 lnew ‚Üê L(f(ÀÜxnew), y) 7 if lnew < l‚àó then xÀÜ ‚Üê xÀÜnew, l‚àó ‚Üê lnew ; 8 i ‚Üê i + 1 9 end 3.1 Adversarial Examples in the lp-threat Model Let f : [0, 1]d ‚Üí R K be a classifier, where d is the input dimension, K the number of classes and fk(x) is the predicted score that x belongs to class k. The classifier assigns class arg max k=1,...,K fk(x) to the input x. The goal of an untargeted attack is to change the correctly predicted class y for the point x. A point ÀÜx is called an adversarial example with an lp-norm bound of  for x if arg max k=1,...,K fk(ÀÜx) 6= y, kxÀÜ ‚àí xkp ‚â§  and ÀÜx ‚àà [0, 1]d , where we have added the additional constraint that ÀÜx is an image. The task of finding ÀÜx can be rephrased as solving the constrained optimization problem min xÀÜ‚àà[0,1]d L(f(ÀÜx), y), s.t. kxÀÜ ‚àí xkp ‚â§ , (1) for a loss L. In our experiments, we use L(f(ÀÜx), y) = fy(ÀÜx) ‚àí maxk6=y fk(ÀÜx). The goal of targeted attacks is instead to change the decision of the classifier to a particular class t, i.e., to find ÀÜx so that arg maxk fk(ÀÜx) = t under the same constraints on ÀÜx. We further discuss the targeted attacks in Sup. E.1. 3.2 General Algorithmic Scheme of the Square Attack Square Attack is based on random search which is a well known iterative technique in optimization introduced by Rastrigin in 1963 [46]. The main idea of the algorithm is to sample a random update Œ¥ at each iteration, and to add this update to the current iterate ÀÜx if it improves the objective function. Despite its simplicity, random search performs well in many situations [60] and does not depend on gradient information from the objective function g. Many variants of random search have been introduced [38,48,47], which differ mainly in how the random perturbation is chosen at each iteration (the original Square Attack: a query-efficient black-box adversarial attack 5 scheme samples uniformly on a hypersphere of fixed radius). For our goal of crafting adversarial examples we come up with two sampling distributions specific to the l‚àû- and the l2-attack (Sec. 3.3 and Sec. 3.4), which we integrate in the classic random search procedure. These sampling distributions are motivated by both how images are processed by neural networks with convolutional filters and the shape of the lp-balls for different p. Additionally, since the considered objective is non-convex when using neural networks, a good initialization is particularly important. We then introduce a specific one for better query efficiency. Our proposed scheme differs from classical random search by the fact that the perturbations ÀÜx ‚àí x are constructed such that for every iteration they lie on the boundary of the l‚àû- or l2-ball before projection onto the image domain [0, 1]d . Thus we are using the perturbation budget almost maximally at each step. Moreover, the changes are localized in the image in the sense that at each step we modify just a small fraction of contiguous pixels shaped into squares. Our overall scheme is presented in Algorithm 1. First, the algorithm picks the side length h (i) of the square to be modified (step 3), which is decreasing according to an a priori fixed schedule. This is in analogy to the step-size reduction in gradient-based optimization. Then in step 4 we sample a new update Œ¥ and add it to the current iterate (step 5). If the resulting loss (obtained in step 6) is smaller than the best loss so far, the change is accepted otherwise discarded. Since we are interested in query efficiency, the algorithm stops as soon as an adversarial example is found. The time complexity of the algorithm is dominated by the evaluation of f(ÀÜxnew), which is performed at most N times, with N total number of iterations. We plot the resulting adversarial perturbations in Fig. 3 and additionally in Sup. E where we also show imperceptible perturbations. We note that previous works [31,49,39] generate perturbations containing squares. However, while those use a fixed grid on which the squares are constrained, we optimize the position of the squares as well as the color, making our attack more flexible and effective. Moreover, unlike previous works, we motivate squared perturbations with the structure of the convolutional filters (see Sec. 4). Size of the squares. Given images of size w√ów, let p ‚àà [0, 1] be the percentage of elements of x to be modified. The length h of the side of the squares used is given by the closest positive integer to p p ¬∑ w2 (and h ‚â• 3 for the l2-attack). Then, the initial p is the only free parameter of our scheme. With N = 10000 iterations available, we halve the value of p at i ‚àà {10, 50, 200, 1000, 2000, 4000, 6000, 8000} iterations. For different N we rescale the schedule accordingly. 3.3 The l‚àû-Square Attack Initialization. As initialization we use vertical stripes of width one where the color of each stripe is sampled uniformly at random from {‚àí, } c (c number of color channels). We found that convolutional networks are particularly sensitive to such perturbations, see also [58] for a detailed discussion on the sensitivity of neural networks to various types of high frequency perturbations. Sampling distribution. Similar to [49] we observe that successful l‚àû- perturbations usually have values ¬± in all the components (note that this does 6 M. Andriushchenko et al. not hold perfectly due to the image constraints ÀÜx ‚àà [0, 1]d ). In particular, it holds xÀÜi ‚àà {max{0, xi ‚àí }, min{1, xi + }}. Algorithm 2: Sampling distribution P for l‚àû-norm Input: maximal norm , window size h, image size w, color channels c Output: New update Œ¥ 1 Œ¥ ‚Üê array of zeros of size w √ó w √ó c 2 sample uniformly r, s ‚àà {0, . . . , w ‚àí h} ‚äÇ N 3 for i = 1, . . . , c do 4 œÅ ‚Üê Uniform({‚àí2, 2}) 5 Œ¥r+1:r+h, s+1:s+h, i ‚Üê œÅ ¬∑ 1h√óh 6 end Our sampling distribution P for the l‚àû-norm described in Algorithm 2 selects sparse updates of xÀÜ with kŒ¥k0 = h ¬∑ h ¬∑ c where Œ¥ ‚àà {‚àí2, 0, 2} d and the non-zero elements are grouped to form a square. In this way, after the projection onto the l‚àû-ball of radius  (Step 5 of Algorithm 1) all components i for which  ‚â§ xi ‚â§ 1 ‚àí  satisfy xÀÜi ‚àà {xi ‚àí , xi + }, i.e. differ from the original point x in each element either by  or ‚àí. Thus ÀÜx‚àíx is situated at one of the corners of the l‚àû- ball (modulo the components which are close to the boundary). Note that all projections are done by clipping. Moreover, we fix the elements of Œ¥ belonging to the same color channel to have the same sign, since we observed that neural networks are particularly sensitive to such perturbations (see Sec. 4.3). 3.4 The l2-Square Attack Fig. 2. Perturbation of the l2-attack Initialization. The l2-perturbation is initialized by generating a 5 √ó 5 grid-like tiling by squares of the image, where the perturbation on each tile has the shape described next in the sampling distribution. The resulting perturbation ÀÜx ‚àí x is rescaled to have l2-norm  and the resulting ÀÜx is projected onto [0, 1]d by clipping. Sampling distribution. First, let us notice that the adversarial perturbations typically found for the l2-norm tend to be much more localized than those for the l‚àû-norm [54], in the sense that large changes are applied on some pixels of the original image, while many others are minimally modified. To mimic this feature we introduce a new update Œ∑ which has two ‚Äúcenters‚Äù with large absolute value and opposite signs, while the other components have lower absolute values as one gets farther away from the centers, but never reaching zero (see Fig. 2 for one example with h = 8 of the resulting update Œ∑). In this way the modifications are localized and with high contrast between the different halves, which we found to improve the query efficiency. Concretely, we define Œ∑ (h1,h2) ‚àà R h1√óh2 Square Attack: a query-efficient black-box adversarial attack 7 Algorithm 3: Sampling distribution P for l2-norm Input: maximal norm , window size h, image size w, number of color channels c, current image ÀÜx, original image x Output: New update Œ¥ 1 ŒΩ ‚Üê xÀÜ ‚àí x 2 sample uniformly r1, s1, r2, s2 ‚àà {0, . . . , w ‚àí h} 3 W1 := r1 + 1 : r1 + h, s1 + 1 : s1 + h, W2 := r2 + 1 : r2 + h, s2 + 1 : s2 + h 4  2 unused ‚Üê  2 ‚àí kŒΩk 2 2 , Œ∑ ‚àó ‚Üê Œ∑/kŒ∑k2 with Œ∑ as in (2) 5 for i = 1, . . . , c do 6 œÅ ‚Üê Uniform({‚àí1, 1}) 7 ŒΩtemp ‚Üê œÅŒ∑‚àó + ŒΩW1,i/kŒΩW1,ik2 8  i avail ‚Üê q kŒΩW1‚à™W2,ik 2 2 +  2 unused/c 9 ŒΩW2,i ‚Üê 0, ŒΩW1,i ‚Üê ( ŒΩtemp/kŒΩtempk2 ) i avail 10 end 11 Œ¥ ‚Üê x + ŒΩ ‚àí xÀÜ original l‚àû-attack - ‚àû = 0.05 l2-attack - 2 = 5 Fig. 3. Visualization of the adversarial perturbations and examples found by the l‚àû- and l2-versions of the Square Attack on ResNet-50 (for some h1, h2 ‚àà N+ such that h1 ‚â• h2) for every 1 ‚â§ r ‚â§ h1, 1 ‚â§ s ‚â§ h2 as Œ∑ (h1,h2) r,s = M X (r,s) k=0 1 (n + 1 ‚àí k) 2 , with n =  h1 2  , and M(r, s) = n ‚àí max{|r ‚àí  h1 2  ‚àí 1|, |s ‚àí  h2 2  ‚àí 1|}. The intermediate square update Œ∑ ‚àà R h√óh is then selected uniformly at random from either Œ∑ =  Œ∑ (h,k) , ‚àíŒ∑ (h,h‚àík)  , with k = bh/2c , (2) or its transpose (corresponding to a rotation of 90‚ó¶ ). Second, unlike l‚àû-constraints, l2-constraints do not allow to perturb each component independently from the others as the overall l2-norm must be kept smaller than . Therefore, to modify a perturbation ÀÜx‚àíx of norm  with localized changes while staying on the hypersphere, we have to ‚Äúmove the mass‚Äù of ÀÜx ‚àí x from one location to another. Thus, our scheme consists in randomly selecting two squared windows in the current perturbation ŒΩ = ÀÜx ‚àí x, namely ŒΩW1 and ŒΩW2 , setting ŒΩW2 = 0 and using the budget of kŒΩW2 k2 to increase the total perturbation of ŒΩW1 . Note that the perturbation of W1 is then a combination of 8 M. Andriushchenko et al. the existing perturbation plus the new generated Œ∑. We report the details of this scheme in Algorithm 3 where step 4 allows to utilize the budget of l2-norm lost after the projection onto [0, 1]d . The update Œ¥ output by the algorithm is such that the next iterate ÀÜxnew = ÀÜx + Œ¥ (before projection onto [0, 1]d by clipping) belongs to the hypersphere B2(x, ) as stated in the following proposition. Proposition 1. Let Œ¥ be the output of Algorithm 3. Then kxÀÜ + Œ¥ ‚àí xk2 = . 4 Theoretical and Empirical Justification of the Method We provide high-level theoretical justifications and empirical evidence regarding the algorithmic choices in Square Attack, with focus on the l‚àû-version (the l2- version is significantly harder to analyze). 4.1 Convergence Analysis of Random Search First, we want to study the convergence of the random search algorithm when considering an L-smooth objective function g (such as neural networks with activation functions like softplus, swish, ELU, etc) on the whole space R d (without projection4 ) under the following assumptions on the update Œ¥t drawn from the sampling distribution Pt: EkŒ¥tk 2 2 ‚â§ Œ≥ 2 t C and E|hŒ¥t, vi| ‚â• CŒ≥Àú tkvk2, ‚àÄv ‚àà R d , (3) where Œ≥t is the step size at iteration t, C, C >Àú 0 some constants and h¬∑, ¬∑i denotes the inner product. We obtain the following result, similar to existing convergence rates for zeroth-order methods [42,43,24]: Proposition 2. Suppose that E[Œ¥t] = 0 and the assumptions in Eq. (3) hold. Then for step-sizes Œ≥t = Œ≥/‚àö T, we have min t=0,...,T Ek‚àág(xt)k2 ‚â§ 2 Œ≥CÀú ‚àö T  g(x0) ‚àí Eg(xT +1) + Œ≥ 2CL 2  . This basically shows for T large enough one can make the gradient arbitrary small, meaning that the random search algorithm converges to a critical point of g (one cannot hope for much stronger results in non-convex optimization without stronger conditions). Unfortunately, the second assumption in Eq. (3) does not directly hold for our sampling distribution P for the l‚àû-norm (see Sup. A.3), but holds for a similar one, P multiple, where each component of the update Œ¥ is drawn uniformly at random from {‚àí2, 2}. In fact we show in Sup. A.4, using the Khintchine inequality [29], that EkŒ¥tk 2 2 ‚â§ 4cŒµ2h 2 and E|hŒ¥t, vi| ‚â• ‚àö 2cŒµh2 d kvk2, ‚àÄv ‚àà R d . Moreover, while P multiple performs worse than the distribution used in Algorithm 2, we show in Sec. 4.3 that it already reaches state-of-the-art results. 4 Nonconvex constrained optimization under noisy oracles is notoriously harder [22]. Square Attack: a query-efficient black-box adversarial attack 9 4.2 Why Squares? Previous works [49,39] build their l‚àû-attacks by iteratively adding square modifications. Likewise we change square-shaped regions of the image for both our l‚àû- and l2-attacks‚Äîwith the difference that we can sample any square subset of the input, while the grid of the possible squares is fixed in [49,39]. This leads naturally to wonder why squares are superior to other shapes, e.g. rectangles. Let us consider the l‚àû-threat model, with bound , input space R d√ód and a convolutional filter w ‚àà R s√ós with entries unknown to the attacker. Let Œ¥ ‚àà R d√ód be the sparse update with kŒ¥k0 = k ‚â• s 2 and kŒ¥k‚àû ‚â§ . We denote by S(a, b) the index set of the rectangular support of Œ¥ with |S(a, b)| = k and shape a √ó b. We want to provide intuition why sparse square-shaped updates are superior to rectangular ones in the sense of reaching a maximal change in the activations of the first convolutional layer. Let z = Œ¥ ‚àó w ‚àà R d√ód denote the output of the convolutional layer for the update Œ¥. The l‚àû-norm of z is the maximal componentwise change of the convolutional layer: kzk‚àû = max u,v |zu,v| = max u,v    Xs i,j=1 Œ¥u‚àíb s 2 c+i,v‚àíb s 2 c+j ¬∑ wi,j    ‚â§ max u,v  X i,j |wi,j |1(u‚àíb s 2 c+i,v‚àíb s 2 c+j)‚ààS(a,b) , where elements with indices exceeding the size of the matrix are set to zero. Note that the indicator function attains 1 only for the non-zero elements of Œ¥ involved in the convolution to get zu,v. Thus, to have the largest upper bound possible on |zu,v|, for some (u, v), we need the largest possible amount of components of Œ¥ with indices in C(u, v) = n (u ‚àí b s 2 c + i, v ‚àí b s 2 c + j) : i, j = 1, . . . , so to be non-zero (that is in S(a, b)). Therefore, it is desirable to have the shape S(a, b) of the perturbation Œ¥ selected so to maximize the number N of convolutional filters w ‚àà R s√ós which fit into the rectangle a√ób. Let F be the family of the objects that can be defined as the union of axis-aligned rectangles with vertices on N 2 , and G ‚äÇ F be the squares of F of shape s √ó s with s ‚â• 2. We have the following proposition: Proposition 3. Among the elements of F with area k ‚â• s 2 , those which contain the largest number of elements of G have N ‚àó = (a ‚àí s + 1)(b ‚àí s + 1) + (r ‚àí s + 1)+ (4) of them, with a = j‚àö k k , b =  k a  , r = k ‚àí ab and z + = max{z, 0}. This proposition states that, if we can modify only k elements of Œ¥, then shaping them to form (approximately) a square allows to maximize the number of pairs (u, v) for which |S(a, b) ‚à© C(u, v)| = s 2 . If k = l 2 then a = b = l are the optimal values for the shape of the perturbation update, i.e. the shape is exactly a square. 10 M. Andriushchenko et al. Table 1. Ablation study of the l‚àû-Square Attack which shows how the individual design decisions improve the performance. The fourth row corresponds to the method for which we have shown convergence guarantees in Sec. 4.1. The last row corresponds to our final l‚àû-attack. c indicates the number of color channels, h the length of the side of the squares, so that ‚Äú# random sign‚Äù c represents updates with constant sign for each color, while c ¬∑ h 2 updates with signs sampled independently of each other Update # random Initialization Failure Avg. Median shape signs rate queries queries random c ¬∑ h 2 vert. stripes 0.0% 401 48 random c ¬∑ h 2 uniform rand. 0.0% 393 132 random c vert. stripes 0.0% 339 53 square c ¬∑ h 2 vert. stripes 0.0% 153 15 rectangle c vert. stripes 0.0% 93 16 square c uniform rand. 0.0% 91 26 square c vert. stripes 0.0% 73 11 4.3 Ablation Study We perform an ablation study to show how the individual design decisions for the sampling distribution of the random search improve the performance of l‚àû- Square Attack, confirming the theoretical arguments above. The comparison is done for an l‚àû-threat model of radius  = 0.05 on 1, 000 test points for a ResNet50 model trained normally on ImageNet (see Sec. 5 for details) with a query limit of 10, 000 and results are shown in Table 1. Our sampling distribution is special in two aspects: i) we use localized update shapes in form of squares and ii) the update is constant in each color channel. First, one can observe that our update shape ‚Äúsquare‚Äù performs better than ‚Äúrectangle‚Äù as we discussed in the previous section, and it is significantly better than ‚Äúrandom‚Äù (the same amount of pixels is perturbed, but selected randomly in the image). This holds both for c (constant sign per color channel) and c ¬∑ h 2 (every pixel and color channel is changed independently of each other), with an improvement in terms of average queries of 339 to 73 and 401 to 153 respectively. Moreover, with updates of the same shape, the constant sign over color channels is better than selecting it uniformly at random (improvement in average queries: 401 to 339 and 153 to 73). In total the algorithm with ‚Äúsquare-c‚Äù needs more than 5√ó less average queries than ‚Äúrandom-c ¬∑ h 2‚Äù, showing that our sampling distribution is the key to the high query efficiency of Square Attack. The last innovation of our random search scheme is the initialization, crucial element of every non-convex optimization algorithm. Our method (‚Äúsquare-c‚Äù) with the vertical stripes initialization improves over a uniform initialization on average by ‚âà 25% and, especially, median number of queries (more than halved). We want to also highlight that the sampling distribution ‚Äúsquare-c ¬∑ h 2‚Äù for which we shown convergence guarantees in Sec. 4.1 performs already better in terms of the success rate and the median number of queries than the state of the art (see Sec. 5). For a more detailed ablation, also for our l2-attack, see Sup. C. Square Attack: a query-efficient black-box adversarial attack 11 Table 2. Results of untargeted attacks on ImageNet with a limit of 10,000 queries. For the l‚àû-attack we set the norm bound  = 0.05 and for the l2-attack  = 5. Models: normally trained I: Inception v3, R: ResNet-50, V: VGG-16-BN. The Square Attack outperforms for both threat models all other methods in terms of success rate and query efficiency. The missing entries correspond to the results taken from the original paper where some models were not reported Norm Attack Failure rate Avg. queries Med. queries I R V I R V I R V l‚àû Bandits [31] 3.4% 1.4% 2.0% 957 727 394 218 136 36 Parsimonious [49] 1.5% - - 722 - - 237 - - DFOc‚ÄìCMA [39] 0.8% 0.0% 0.1% 630 270 219 259 143 107 DFOd‚ÄìDiag. CMA [39] 2.3% 1.2% 0.5% 424 417 211 20 20 2 SignHunter [2] 1.0% 0.1% 0.3% 471 129 95 95 39 43 Square Attack 0.3% 0.0% 0.0% 197 73 31 24 11 1 l2 Bandits [31] 9.8% 6.8% 10.2% 1486 939 511 660 392 196 SimBA-DCT [28] 35.5% 12.7% 7.9% 651 582 452 564 467 360 Square Attack 7.1% 0.7% 0.8% 1100 616 377 385 170 109 5 Experiments In this section we show the effectiveness of the Square Attack. Here we concentrate on untargeted attacks since our primary goal is query efficient robustness evaluation, while the targeted attacks are postponed to the supplement. First, we follow the standard setup [31,39] of comparing black-box attacks on three ImageNet models in terms of success rate and query efficiency for the l‚àû- and l2-untargeted attacks (Sec. 5.1). Second, we show that our black-box attack can even outperform white-box PGD attacks on several models (Sec. 5.2). Finally, in the supplement we provide more experimental details (Sup. B), a stability study of our attack for different parameters (Sup. C) and random seeds (Sup. D), and additional results including the experiments for targeted attacks (Sup. E). 5.1 Evaluation on ImageNet We compare the Square Attack to state-of-the-art score-based black-box attacks (without any extra information such as surrogate models) on three pretrained models in PyTorch (Inception v3, ResNet-50, VGG-16-BN) using 1,000 images from the ImageNet validation set. Unless mentioned otherwise, we use the code from the other papers with their suggested parameters. As it is standard in the literature, we give a budget of 10,000 queries per point to find an adversarial perturbation of lp-norm at most . We report the average and median number of queries each attack requires to craft an adversarial example, together with the failure rate. All query statistics are computed only for successful attacks on the points which were originally correctly classified. Tables 2 and 3 show that the Square Attack, despite its simplicity, achieves in all the cases (models and norms) the lowest failure rate, (< 1% everywhere 12 M. Andriushchenko et al. Inception v3 ResNet-50 VGG-16-BN l‚àû attacks low query regime l2-attacks low query regime Fig. 4. Success rate in the low-query regime (up to 200 queries). ‚àó denotes the results obtained via personal communication with the authors and evaluated on 500 and 10,000 randomly sampled points for BayesAttack [50] and DFO [39] methods, respectively except for the l2-attack on Inception v3), and almost always requires fewer queries than the competitors to succeed. Fig. 4 shows the progression of the success rate of the attacks over the first 200 queries. Even in the low query regime the Square Attack outperforms the competitors for both norms. Finally, we highlight that the only hyperparameter of our attack, p, regulating the size of the squares, is set for all the models to 0.05 for l‚àû and 0.1 for l2-perturbations. l‚àû-attacks. We compare our attack to Bandits [32], Parsimonious [49], DFOc / DFOd [39], and SignHunter [2]. In Table 2 we report the results of the l‚àû- attacks with norm bound of  = 0.05. The Square Attack always has the lowest failure rate, notably 0.0% in 2 out of 3 cases, and the lowest query consumption. Interestingly, our attack has median equal 1 on VGG-16-BN, meaning that the proposed initialization is particularly effective for this model. Table 3. Query statistics for untargeted l2-attacks computed for the points for which all three attacks are successful for fair comparison Attack Avg. queries Med. queries I R V I R V Bandits [31] 536 635 398 368 314 177 SimBA-DCT [28] 647 563 421 552 446 332 Square Attack 352 287 217 181 116 80 The closest competitor in terms of the average number of queries is SignHunter [2], which still needs on average between 1.8 and 3 times more queries to find adversarial examples and has a higher failure rate than our attack. Moreover, the median number of queries of SignHunter is much worse than for our method (e.g. 43 vs 1 on VGG). We note that although DFOc‚ÄìCMA [39] is competitive to our attack in terms of median queries, it has a significantly higher failure rate and between 2 and 7 times worse average number of queries. Additionally, our method is also more effective in the low-query regime (Fig. 4) than other methods (including [50]) on all the models. l2-attacks. We compare our attack to Bandits [31] and SimBA [28] for  = 5, while we do not consider SignHunter [2] since it is not as competitive as for the l‚àû-norm, and in particular worse than Bandits on ImageNet (see Fig. 2 in [2]). Square Attack: a query-efficient black-box adversarial attack 13 As Table 2 and Fig. 4 show, the Square Attack outperforms by a large margin the other methods in terms of failure rate, and achieves the lowest median number of queries for all the models and the lowest average one for VGG-16-BN. However, since it has a significantly lower failure rate, the statistics of the Square Attack are biased by the ‚Äúhard‚Äù cases where the competitors fail. Then, we recompute the same statistics considering only the points where all the attacks are successful (Table 3). In this case, our method improves by at least 1.5√ó the average and by at least 2√ó the median number of queries. 5.2 Square Attack Can be More Accurate than White-box Attacks Table 4. On the robust models of [37] and [61] on MNIST l‚àû-Square Attack with  = 0.3 achieves state-of-the-art (SOTA) results outperforming white-box attacks Model Robust accuracy SOTA Square Madry et al. [37] 88.13% 88.25% TRADES [61] 93.33% 92.58% Here we test our attack on problems which are challenging for both whitebox PGD and other black-box attacks. We use for evaluation robust accuracy, defined as the worst-case accuracy of a classifier when an attack perturbs each input in some lp-ball. We show that our algorithm outperforms the competitors both on state-of-the-art robust models and defenses that induce different types of gradient masking. Thus, our attack is useful to evaluate robustness without introducing adaptive attacks designed for each model separately. Outperforming white-box attacks on robust models. The models obtained with the adversarial training of [37] and TRADES [61] are standard benchmarks to test adversarial attacks, which means that many papers have tried to reduce their robust accuracy, without limit on the computational budget and primarily via white-box attacks. We test our l‚àû-Square Attack on these robust models on MNIST at  = 0.3, using p = 0.8, 20k queries and 50 random restarts, i.e., we run our attack 50 times and consider it successful if any of the runs finds an adversarial example (Table 4). On the model of Madry et al [37] Square Attack is only 0.12% far from the white-box state-of-the-art, achieving the second best result (also outperforming the 91.47% of SignHunter [2] by a large margin). On the TRADES benchmark [63], our method obtains a new SOTA of 92.58% robust accuracy outperforming the white-box attack of [20]. Additionally, the subsequent work of [21] uses the Square Attack as part of their AutoAttack where they show that the Square Attack outperforms other whitebox attacks on 9 out of 9 MNIST models they evaluated. Thus, our black-box attack can be also useful for robustness evaluation of new defenses in the setting where gradient-based attacks require many restarts and iterations. Resistance to gradient masking. In Table 5 we report the robust accuracy at different thresholds  of the l‚àû-adversarially trained models on MNIST of [37] for the l2-threat model. It is known that the PGD is ineffective since it suffers from gradient masking [53]. Unlike PGD and other black-box attacks, our Square Attack does not suffer from gradient masking and yields robust accuracy close to zero for  = 2.5, with only a single run. Moreover, the l2-version of SignHunter [2] 14 M. Andriushchenko et al. Table 5. l2-robustness of the l‚àû-adversarially trained models of [37] at different thresholds . PGD is shown with 1, 10, 100 random restarts. The black-box attacks are given a 10k queries budget (see the supplement for details) 2 Robust accuracy White-box Black-box PGD1 PGD10 PGD100 SignHunter Bandits SimBA Square 2.0 79.6% 67.4% 59.8% 95.9% 80.1% 87.6% 16.7% 2.5 69.2% 51.3% 36.0% 94.9% 32.4% 75.8% 2.4% 3.0 57.6% 29.8% 12.7% 93.8% 12.5% 58.1% 0.6% Table 6. l‚àû-robustness of Clean Logit Pairing (CLP), Logit Squeezing (LSQ) [33]. The Square Attack is competitive to white-box PGD with many restarts (R=10,000, R=100 on MNIST, CIFAR-10 resp.) and more effective than black-box attacks [31,2] ‚àû Model Robust accuracy White-box Black-box PGD1 PGDR Bandits SignHunter Square 0.3 CLPMNIST 62.4% 4.1% 33.3% 62.1% 6.1% LSQMNIST 70.6% 5.0% 37.3% 65.7% 2.6% 16/255 CLPCIFAR 2.8% 0.0% 14.3% 0.1% 0.2% LSQCIFAR 27.0% 1.7% 27.7% 13.2% 7.2% fails to accurately assess the robustness because the method optimizes only over the extreme points of the l‚àû-ball of radius /‚àö d embedded in the target l2-ball. Attacking Clean Logit Pairing and Logit Squeezing. These two l‚àû defenses proposed in [33] were broken in [40]. However, [40] needed up to 10k restarts of PGD which is computationally prohibitive. Using the publicly available models from [40], we run the Square Attack with p = 0.3 and 20k query limit (results in Table 6). We obtain robust accuracy similar to PGDR in most cases, but with a single run, i.e. without additional restarts. At the same time, although on some models Bandits and SignHunter outperform PGD1, they on average achieve significantly worse results than the Square Attack. This again shows the utility of the Square Attack to accurately assess robustness. 6 Conclusion We have presented a simple black-box attack which outperforms by a large margin the state-of-the-art both in terms of query efficiency and success rate. Our results suggest that our attack is useful even in comparison to white-box attacks to better estimate the robustness of models that exhibit gradient masking. Acknowledgements. We thank L. Meunier and S. N. Shukla for providing the data for Figure 6. M.A. thanks A. Modas for fruitful discussions. M.H and F.C. acknowledge support by the Tue.AI Center (FKZ: 01IS18039A), DFG TRR 248, project number 389792660 and DFG EXC 2064/1, project number 390727645. Square Attack: a query-efficient black-box adversarial attack 15 References 1. Akhtar, N., Mian, A.: Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access 6, 14410‚Äì14430 (2018) 2. Al-Dujaili, A., O‚ÄôReilly, U.M.: There are no bit parts for sign bits in black-box attacks. In: ICLR (2020) 3. Alzantot, M., Sharma, Y., Chakraborty, S., Srivastava, M.: Genattack: practical black-box attacks with gradient-free optimization. In: Genetic and Evolutionary Computation Conference (GECCO) (2019) 4. Athalye, A., Carlini, N., Wagner, D.A.: Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In: ICML (2018) 5. Bastani, O., Ioannou, Y., Lampropoulos, L., Vytiniotis, D., Nori, A., Criminisi, A.: Measuring neural net robustness with constraints. In: NeurIPS (2016) 6. Bhagoji, A.N., He, W., Li, B., Song, D.: Practical black-box attacks on deep neural networks using efficient query mechanisms. In: ECCV (2018) 7. Biggio, B., Roli, F.: Wild patterns: Ten years after the rise of adversarial machine learning. Pattern Recognition 84, 317‚Äì331 (2018) 8. Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press, Cambridge (2004) 9. Brendel, W., Rauber, J., Bethge, M.: Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In: ICLR (2018) 10. Brown, T.B., Man¬¥e, D., Roy, A., Abadi, M., Gilmer, J.: Adversarial patch. In: NeurIPS 2017 Workshop on Machine Learning and Computer Security (2017) 11. Brunner, T., Diehl, F., Le, M.T., Knoll, A.: Guessing smart: biased sampling for efficient black-box adversarial attacks. In: ICCV (2019) 12. Carlini, N., Wagner, D.: Adversarial examples are not easily detected: Bypassing ten detection methods. In: ACM Workshop on Artificial Intelligence and Security (2017) 13. Chen, J., Jordan, M.I., J., W.M.: HopSkipJumpAttack: a query-efficient decisionbased attack (2019), arXiv preprint arXiv:1904.02144 14. Chen, P., Sharma, Y., Zhang, H., Yi, J., Hsieh, C.: Ead: Elastic-net attacks to deep neural networks via adversarial examples. In: AAAI (2018) 15. Chen, P.Y., Zhang, H., Sharma, Y., Yi, J., Hsieh, C.J.: Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In: 10th ACM Workshop on Artificial Intelligence and Security - AISec ‚Äô17. ACM Press (2017) 16. Cheng, M., Le, T., Chen, P.Y., Yi, J., Zhang, H., Hsieh, C.J.: Query-efficient hardlabel black-box attack: An optimization-based approach. In: ICLR (2019) 17. Cheng, S., Dong, Y., Pang, T., Su, H., Zhu, J.: Improving black-box adversarial attacks with a transfer-based prior. In: NeurIPS (2019) 18. Cohen, J.M., Rosenfeld, E., Kolter, Z.: Certified adversarial robustness via randomized smoothing. In: ICML (2019) 19. Croce, F., Hein, M.: Sparse and imperceivable adversarial attacks. In: ICCV (2019) 20. Croce, F., Hein, M.: Minimally distorted adversarial examples with a fast adaptive boundary attack. In: ICML (2020) 21. Croce, F., Hein, M.: Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In: ICML (2020) 22. Davis, D., Drusvyatskiy, D.: Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization 29(1), 207‚Äì239 (2019) 16 M. Andriushchenko et al. 23. Du, J., Zhang, H., Zhou, J.T., Yang, Y., Feng, J.: Query-efficient meta attack to deep neural networks. In: ICLR (2020) 24. Duchi, J., Jordan, M., Wainwright, M., Wibisono, A.: Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory 61(5), 2788‚Äì2806 (2015) 25. Fawzi, A., Frossard, P.: Measuring the effect of nuisance variables on classifiers. In: British Machine Vision Conference (BMVC) (2016) 26. Gu, S., Rigazio, L.: Towards deep neural network architectures robust to adversarial examples. In: ICLR Workshop (2015) 27. Guo, C., Frank, J.S., Weinberger, K.Q.: Low frequency adversarial perturbation. In: UAI (2019) 28. Guo, C., Gardner, J.R., You, Y., Wilson, A.G., Weinberger, K.Q.: Simple black-box adversarial attacks. In: ICML (2019) 29. Haagerup, U.: The best constants in the Khintchine inequality. Studia Math. 70(3), 231‚Äì283 (1981) 30. Ilyas, A., Engstrom, L., Athalye, A., Lin, J.: Black-box adversarial attacks with limited queries and information. In: ICML (2018) 31. Ilyas, A., Engstrom, L., Madry, A.: Prior convictions: Black-box adversarial attacks with bandits and priors. In: ICLR (2019) 32. Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., Madry, A.: Adversarial examples are not bugs, they are features. NeurIPS (2019) 33. Kannan, H., Kurakin, A., Goodfellow, I.: Adversarial logit pairing (2018), arXiv preprint arXiv:1803.06373 34. Karmon, D., Zoran, D., Goldberg, Y.: Lavan: Localized and visible adversarial noise. In: ICML (2018) 35. Li, Y., Li, L., Wang, L., Zhang, T., Gong, B.: Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks. In: ICML (2019) 36. Lin, Y., Jiang, H., Jiang, H.: Bandlimiting neural networks against adversarial attacks (2019), arXiv preprint arXiv:1905.12797 37. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. In: ICLR (2018) 38. Matyas, J.: Random optimization. Automation and Remote control 26(2), 246‚Äì253 (1965) 39. Meunier, L., Atif, J., Teytaud, O.: Yet another but more efficient blackbox adversarial attack: tiling and evolution strategies (2019), arXiv preprint, arXiv:1910.02244 40. Mosbach, M., Andriushchenko, M., Trost, T., Hein, M., Klakow, D.: Logit pairing methods can fool gradient-based attacks. In: NeurIPS 2018 Workshop on Security in Machine Learning (2018) 41. Narodytska, N., Kasiviswanathan, S.: Simple black-box adversarial attacks on deep neural networks. In: CVPR Workshops (2017) 42. Nemirovsky, A.S., Yudin, D.B.: Problem Complexity and Method Efficiency in Optimization. Wiley-Interscience Series in Discrete Mathematics, John Wiley & Sons (1983) 43. Nesterov, Y., Spokoiny, V.: Random gradient-free minimization of convex functions. Foundations of Computational Mathematics 17(2), 527‚Äì566 (2017) 44. Papernot, N., McDaniel, P., Goodfellow, I.: Transferability in machine learning: from phenomena to black-box attacks using adversarial samples (2016), arXiv preprint arXiv:1605.07277 Square Attack: a query-efficient black-box adversarial attack 17 45. Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami, A.: Distillation as a defense to adversarial perturbations against deep networks. In: IEEE Symposium on Security & Privacy (2016) 46. Rastrigin, L.: The convergence of the random search method in the extremal control of a many parameter system. Automaton & Remote Control 24, 1337‚Äì1342 (1963) 47. Schrack, G., Choit, M.: Optimized relative step size random searches. Mathematical Programming 10, 230‚Äì244 (1976) 48. Schumer, M., Steiglitz, K.: Adaptive step size random search. IEEE Transactions on Automatic Control 13(3), 270‚Äì276 (1968) 49. Seungyong, M., Gaon, A., Hyun, O.S.: Parsimonious black-box adversarial attacks via efficient combinatorial optimization. In: ICML (2019) 50. Shukla, S.N., Sahu, A.K., Willmott, D., Kolter, Z.: Black-box adversarial attacks with Bayesian optimization (2019), arXiv preprint arXiv:1909.13857 51. Su, J., Vargas, D., Sakurai, K.: One pixel attack for fooling deep neural networks. IEEE Transactions on Evolutionary Computation (2019) 52. Suya, F., Chi, J., Evans, D., Tian, Y.: Hybrid batch attacks: Finding black-box adversarial examples with limited queries (2019), arXiv preprint, arXiv:1908.07000 53. Tram`er, F., Boneh, D.: Adversarial training and robustness for multiple perturbations. In: NeurIPS (2019) 54. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., Madry, A.: Robustness may be at odds with accuracy. In: ICLR (2019) 55. Tu, C.C., Ting, P., Chen, P.Y., Liu, S., Zhang, H., Yi, J., Hsieh, C.J., Cheng, S.M.: Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In: AAAI Conference on Artificial Intelligence (2019) 56. Uesato, J., O‚ÄôDonoghue, B., Van den Oord, A., Kohli, P.: Adversarial risk and the dangers of evaluating against weak attacks. In: ICML (2018) 57. Yan, Z., Guo, Y., Zhang, C.: Subspace attack: Exploiting promising subspaces for query-efficient black-box attacks. In: NeurIPS (2019) 58. Yin, D., Lopes, R.G., Shlens, J., Cubuk, E.D., Gilmer, J.: A Fourier perspective on model robustness in computer vision. In: NeurIPS (2019) 59. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: CVPR (2017) 60. Zabinsky, Z.B.: Random search algorithms. Wiley encyclopedia of operations research and management science (2010) 61. Zhang, H., Yu, Y., Jiao, J., Xing, E.P., Ghaoui, L.E., Jordan, M.I.: Theoretically principled trade-off between robustness and accuracy. In: ICML (2019) 62. Zheng, S., Song, Y., Leung, T., Goodfellow, I.J.: Improving the robustness of deep neural networks via stability training. In: CVPR (2016) 63. Zheng, T., Chen, C., Ren, K.: Distributionally adversarial attack. In: AAAI (2019) 18 M. Andriushchenko et al. Supplementary Material Organization of the Supplementary Material In Section A, we present the missing proofs of Section 3 and Section 4 and slightly deepen our theoretical insights on the efficiency of the proposed l‚àû-attack. Section B covers various implementation details and the hyperparameters we used. We show a more detailed ablation study on different choices of the attack‚Äôs algorithm in Section C. Since the Square Attack is a randomized algorithm, we show the variance of the main reported metric for different random seeds in Section D. Finally, Section E presents results of the targeted attacks on ImageNet, additional results for the untargeted attacks, and an evaluation of the post-averaging defense [36] which we conclude is much less robust than claimed. A Proofs Omitted from Section 3 and Section 4 In this section, we present the proofs omitted from Section 3 and Section 4. A.1 Proof of Proposition 1 Let Œ¥ be the output of Algorithm 3. We prove here that kxÀÜ + Œ¥ ‚àí xk2 = . From Step 13 of Algorithm 3, we directly have the equality kxÀÜ + Œ¥ ‚àí xk2 = kŒΩk2 . Let ŒΩ old be the update at the previous iteration, defined in Step 1 and W1 ‚à™ W2 the indices not belonging to W1 ‚à™ W2. Then, kŒΩk 2 2 = Xc i=1 kŒΩW1‚à™W2,ik 2 2 + Xc i=1  ŒΩW1‚à™W2,i  2 2 = Xc i=1 kŒΩW1,ik 2 2 + Xc i=1  ŒΩW1‚à™W2,i  2 2 = Xc i=1 ( i avail) 2 + Xc i=1  ŒΩW1‚à™W2,i  2 2 = Xc i=1 ŒΩ old W1‚à™W2,i  2 2 +  2 unused + Xc i=1  ŒΩW1‚à™W2,i  2 2 (i) = Xc i=1 ŒΩ old W1‚à™W2,i  2 2 +  2 unused + Xc i=1  ŒΩ old W1‚à™W2,i  2 2 = ŒΩ old 2 2 +  2 unused (ii) =  2 , where (i) holds since ŒΩ old W1‚à™W2 ‚â° ŒΩW1‚à™W2 as the modifications affect only the elements in the two windows, and (ii) holds by the definition of unused in Step 4 of Algorithm 3. Square Attack: a query-efficient black-box adversarial attack 19 A.2 Proof of Proposition 2 Using the L-smoothness of the function g, that is it holds for all x, y ‚àà R d , k‚àág(x) ‚àí ‚àág(y)k2 ‚â§ Lkx ‚àí yk2 . we obtain (see e.g. [8]): g(xt + Œ¥t) ‚â§ g(xt) + h‚àág(xt), Œ¥ti + L 2 kŒ¥tk 2 2 , and by definition of xt+1 we have g(xt+1) ‚â§ min{g(xt), g(xt + Œ¥t)} ‚â§ g(xt) + min{0,h‚àág(xt), Œ¥ti + L 2 kŒ¥tk 2 2}. Using the definition of the min as a function of the absolute value (2 min{a, b} = a + b ‚àí |a ‚àí b|) yields g(xt+1) ‚â§ g(xt) + 1 2 h‚àág(xt), Œ¥ti + L 4 kŒ¥tk 2 2 ‚àí 1 2 |h‚àág(xt), Œ¥ti + L 2 kŒ¥tk 2 2 |. And using the triangular inequality (|a + b| ‚â• |a| ‚àí |b|), we have g(xt+1) ‚â§ g(xt) + 1 2 h‚àág(xt), Œ¥ti + L 2 kŒ¥tk 2 2 ‚àí 1 2 |h‚àág(xt), Œ¥ti|. Therefore taking the expectation and using that EŒ¥t = 0, we get Eg(xt+1) ‚â§ Eg(xt) ‚àí 1 2 E|h‚àág(xt), Œ¥ti| + L 2 EkŒ¥tk 2 2 . Therefore, together with the assumptions in Eq. (3) this yields to Eg(xt+1) ‚â§ Eg(xt) ‚àí CŒ≥Àú t 2 Ek‚àág(xt)k2 + LCŒ≥2 t 2 . and thus E k‚àág(xt)k2 ‚â§ 2 Œ≥tCÀú  Eg(xt) ‚àí Eg(xt+1) + LCŒ≥2 t 2  . Thus for Œ≥t = Œ≥ we have summing for t = 0 : T min 0‚â§i‚â§T Ek‚àág(xi)k2 ‚â§ 1 T X T t=0 Ek‚àág(xt)k2 ‚â§ 2 CŒ≥T Àú g(x0) ‚àí Eg(xT +1) + T LCŒ≥2 2 . We conclude setting the step-size to Œ≥ = Œò(1/ ‚àö T). 20 M. Andriushchenko et al. A.3 Assumptions in Eq. (3) Do Not Hold for the Sampling Distribution P Let us consider an update Œ¥ with a window size h = 2 and the direction v ‚àà {‚àí1, 1} w√ów√óc defined as v i k,l = (‚àí1)kl for all i, k, l. It is easy to check that any update Œ¥ drawn from the sampling distribution P is orthogonal to this direction v: hv, Œ¥i = Xc i=1 Xr+2 k=r+1 Xs+2 l=s+1 (‚àí1)kl = c(‚àí1 + 1 ‚àí 1 + 1) = 0. Thus, E|hv, Œ¥i| = 0 and the assumptions in Eq. (3) do not hold. This means that the convergence analysis does not directly hold for the sampling distribution P. A.4 Assumptions in Eq. (3) Hold for the Sampling Distribution P multiple Let us consider the sampling distribution P multiple where different Rademacher œÅk,l,i are drawn for each pixel of the update window Œ¥r+1:r+h, s+1:s+h, i. We present it in Algorithm 4 with the convention that any subscript k > w should be understood as k ‚àí w. This technical modification is greatly helpful to avoid side effect. Let v ‚àà R w√ów√óc for which we have using the Khintchine inequality [29]: Algorithm 4: Sampling distribution P multiple for l‚àû-norm Input: maximal norm , window size h, image size w, color channels c Output: New update Œ¥ 1 Œ¥ ‚Üê array of zeros of size w √ó w √ó c 2 sample uniformly r, s ‚àà {0, . . . , w} ‚äÇ N 3 for i = 1, . . . , c do 4 Œ¥r+1:r+h, s+1:s+h, i ‚Üê Uniform({‚àí2, 2} h√óh ) 5 end Square Attack: a query-efficient black-box adversarial attack 21 E|hŒ¥, vi| = E| rX +h k=r+1 sX +h l=s+1 Xc i=1 Œ¥ i k,lv i k,l| (i) = E(r,s)EœÅ| rX +h k=r+1 sX +h l=s+1 Xc i=1 Œ¥ i k,lv i k,l| (ii) ‚â• 2Œµ ‚àö 2 E(r,s)kV(r,s)k2 (iii) ‚â• ‚àö 2ŒµkE(r,s)V(r,s)k2 ‚â• ‚àö 2Œµh2 w2 kvk2, where we define by V(r,s) = {v i k,l}k‚àà{r+1,...,r+h},l‚àà{s+1,...,s+h},i‚àà{1,...,c} and (i) follows from the decomposition between the randomness of the Rademacher and the random window, (ii) follows from the Khintchine inequality and (iii) follows from Jensen inequality. In addition we have for the variance: EkŒ¥k 2 2 = E(r,s) rX +h k=r+1 sX +h l=s+1 Xc i=1 EœÅ(Œ¥ i k,l) 2 = E(r,s) rX +h k=r+1 sX +h l=s+1 Xc i=1 4Œµ 2 = 4cŒµ2h 2 . Thus the assumptions in Eq. (3) hold for the sampling distribution P multiple . A.5 Why Updates of Equal Sign? Proposition 2 underlines the importance of a large inner product E[|hŒ¥t, ‚àág(xt)i|] in the direction of the gradients. This provides some intuition explaining why the update Œ¥ single, where a single Rademacher is drawn for each window, is more efficient than the update Œ¥ multiple where different Rademacher values are drawn. Following the observation that the gradients are often approximately piecewise constant [31], we consider, as a heuristic, a piecewise constant direction v for which we will show that E[|hŒ¥ single, vi|] = Œò(kvk1) and E[|hŒ¥ multiple, vi|] = Œò(kvk2). Therefore the directions sampled by our proposal are more correlated with the gradient direction and help the algorithm to converge faster. This is also verified empirically in our experiments (see the ablation study in Sup. C). 22 M. Andriushchenko et al. Analysis. Let us consider the direction v ‚àà R w√ów composed of different blocks {V(r,s)}(r,s)‚àà{0,...,w/h} of constant sign. For this direction v we compare two different proposal P multiple and P single where we choose uniformly one random block (r, s) and we either assign a single Rademacher œÅ(r,s) to the whole block (this is P single) or we assign multiple Rademacher values {œÅ(k,l)}k‚àà{rh+1,...,(r+1)h},l‚àà{sh+1,...,(s+1)h} (this is P multiple). Using the Khintchine and Jensen inequalities similarly to Sec. A.4, we have E|hŒ¥ multiple, vi| = E| (rX +1)h k=rh+1 (sX +1)h l=sh+1 Œ¥k,lvk,l| ‚â• 2Œµ ‚àö 2 E(r,s)kV(r,s)k2 ‚â• ‚àö 2Œµh2 w2 kvk2. Moreover, we can show the following upper bound using the Khintchine inequality and the inequality between the l1- and l2-norms: E|hŒ¥ multiple, vi| = E| (rX +1)h k=rh+1 (sX +1)h l=sh+1 Œ¥k,lvk,l| ‚â§ 2ŒµE(r,s)kV(r,s)k2 = 2Œµh2 w2 w/h X r=1 w/h X s=1 kV(r,s)k2 ‚â§ 2Œµh w kvk2 Thus, E[|hŒ¥ multiple, vi|] = Œò(kvk2). For the update Œ¥ single we obtain E|hŒ¥ single, vi| = E| (rX +1)h k=rh+1 (sX +1)h l=sh+1 Œ¥r,svk,l| = E|Œ¥r,s (rX +1)h k=rh+1 (sX +1)h l=sh+1 vk,l| (i) = 2ŒµE(r,s)kV(r,s)k1 = 2Œµh2 w2 kvk1 where (i) follows from the fact the V(r,s) has a constant sign. We recover then the l1-norm of the direction v, i.e. we conclude that E[|hŒ¥ single, vi|] = Œò(kvk1). This implies that for an approximately constant block E|hŒ¥ single, vi| will be larger than E|hŒ¥ multiple, vi|. For example, in the extreme case of constant binary Square Attack: a query-efficient black-box adversarial attack 23 block |V(r,s) | = 11>, we have E|hŒ¥ single, vi| = 2Œµh2 >> E|hŒ¥ multiple, vi|  2Œµh. A.6 Proof of Proposition 3 Let x ‚àà F, and N(x) the number of elements of G that x contains. Let initialize x as a square of size s√ós, so that N(x) = 1. We then add iteratively the remaining k ‚àí s 2 unitary squares to x so to maximize N(x). In order to get N(x) = 2 it is necessary to increase x to have size s √ó (s + 1). At this point, again to get N(x) = 3 we need to add s squares to one side of x. However, if we choose to glue them so to form a rectangle s √ó (s + 2), then N(x) = 3 and once more we need other s squares to increase N, which means overall s 2 + 3s to achieve N(x) = 4. If instead we glue s squares along the longer side, with only one additional unitary square we get N(x) = 4 using s 2 + 2s + 1 < s2 + 3s unitary squares (as s ‚â• 2), with x = (s + 1) √ó (s + 1). Then, if the current shape of x is a √ó b with a ‚â• b, the optimal option is adding a unitary squares to have shape a √ó (b + 1), increasing the count N of a ‚àí s + 1. This strategy can be repeated until the budget of k unitary squares is reached. Finally, since we start from the shape s √ó s, then at each stage b ‚àí a ‚àà {0, 1}, which means that the final a will be j‚àö k k . A rectangle a √ó b in F contains (a ‚àí s + 1)(b ‚àí s + 1) elements of G. The remaining k ‚àí ab squares can be glued along the longer side, contributing to N(x) with (k ‚àí ab ‚àí s + 1)+. B Experimental Details In this section, we list the main hyperparameters and various implementation details for the experiments done in the main experiments (Sec. 5). B.1 Experiments on ImageNet For the untargeted Square Attack on the ImageNet models, we used p = 0.05 and p = 0.1 for the l‚àû- and l2- versions respectively. For Bandits, we used their code with their suggested hyperparameters (specified in the configuration files) for both l‚àû and l2. For SignHunter, we used directly their code which does not have any hyperparameters (assuming that the finite difference probe Œ¥ is set to ). For SimBA-DCT, we used the default parameters of the original code apart from the following, which are the suggested ones for each model: for ResNet-50 and VGG-16-BN ‚Äúfreq dims‚Äù = 28, ‚Äúorder‚Äù = ‚Äústrided‚Äù and ‚Äústride‚Äù = 7, for Inception v3 ‚Äúfreq dims‚Äù = 38, ‚Äúorder‚Äù = ‚Äústrided‚Äù and ‚Äústride‚Äù = 9. Notice that SimBA tries to minimize the l2-norm of the perturbations but it does not have a bound on the size of the changes. Then we consider it successful when the adversarial examples produced have norm smaller than the fixed threshold . The results for all other methods were taken directly from the corresponding papers. 24 M. Andriushchenko et al. Evaluation of Bandits. The code of Bandits [31] does not have image standardization at the stage where the set of correctly points is determined (see https://github.com/MadryLab/blackbox-bandits/issues/3). As a result, the attack is run only on the set of points correctly classified by the network without standardization, although the network was trained on standardized images. We fix this bug, and report the results in Table 2 based on the fixed version of their code. We note that the largest difference of our evaluation compared to the l‚àû results reported in Appendix E of [31] is obtained for the VGG-16-BN network: we get 2.0% failure rate while they reported 8.4% in their paper. Also, we note that the query count for Inception v3 we obtain is also better than reported in [31]: 957 instead of 1117 with a slightly better failure rate. Our l2 results also differ ‚Äì we obtain a significantly lower failure rate (9.8%, 6.8%, 10.2% instead of 15.5%, 9.7%, 17.2% for the Inception v3, ResNet-50, VGG-16-BN networks respectively) with improved average number of queries (1486, 939, 511 instead of 1858, 993, 594). B.2 Square Attack Can be More Accurate than White-box Attacks For the l‚àû-Square Attack, we used p = 0.3 for all models on MNIST and CIFAR10. For Bandits on MNIST and CIFAR-10 adversarially trained models we used ‚Äúexploration‚Äù = 0.1, ‚Äútile size‚Äù = 16, ‚Äúgradient iters‚Äù = 1 following [49]. For the comparison of l2-attacks on the l‚àû-adversarially trained model of [37] we used the Square Attack with the usual parameter p = 0.1. For Bandits we used the parameters ‚Äúexploration‚Äù = 0.01, ‚Äútile size‚Äù = 28, ‚Äúgradient iters‚Äù = 1, after running a grid search over the three of them (all the other parameters are kept as set in the original code). For SimBA we used the ‚Äúpixel attack‚Äù with parameters ‚Äúorder‚Äù = ‚Äúrand‚Äù, ‚Äúfreq dims‚Äù = 28, step size of 0.50, after a grid search on all the parameters. C Ablation Study Here we discuss in more detail the ablation study which justifies the algorithmic choices made for our l‚àû- and l2-attacks. Additionally, we discuss the robustness of the attack to the hyperparameter p, i.e. the initial fraction of pixels changed by the attack (see Fig. 5). We perform all these experiments on ImageNet with a standardly trained ResNet-50 model from the PyTorch repository. C.1 l‚àû-Square Attack Sensitivity to the hyperparameter p. First of all, we note that for all values of p we achieve 0.0% failure rate. Moreover, we achieve state-of-the-art query efficiency with all considered values of p (from 0.0125 to 0.4), i.e. we have the average number of queries below 140, and the median below 20 queries. Therefore, we conclude that the attack is robust to a wide range of p, which is an important property of a black-box attack ‚Äì since the target model is unknown, Square Attack: a query-efficient black-box adversarial attack 25 failure rate avg. queries median queries l‚àû -  = 0.05 l2 -  = 5.0 Fig. 5. Sensitivity of the Square Attack to different choices of p ‚àà {0.0125, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4}, i.e. the initial fraction of pixels changed by the attack, on ImageNet for a ResNet-50 model and one aims at minimizing the number of queries needed to fool the model, doing even an approximate grid search over p is prohibitively expensive. Algorithmic choices. In addition to the results presented in Sec. 4.3, we show in Table 7 the results of a few more variants of the Square Attack. We recall that ‚Äú# random signs‚Äù indicates how many different signs we sample to build the updates, with c being the number of color channels and h the current size of the square-shaped updates. Specifically, we test the performance of using a single random sign for all the elements for the update, ‚Äúsquare-1‚Äù, which turns out to be comparable to ‚Äúsquare-c ¬∑ h 2‚Äù, i.e. every component of the update has sign independently sampled, but worse than keeping the sign constant within each color channel (‚Äúsquare-c‚Äù). In order to implement update shape ‚Äúrectangle‚Äù, on every iteration and for every image we sample Œ±, Œ≤ ‚àº Exp(1) and take a rectangle with sides Œ± ¬∑ s and Œ≤ ¬∑ s, so that in expectation its area is equal to s 2 , i.e. to the area of the original square. This update scheme performs significantly better than changing a random subset of pixels (93 vs 339 queries on average), but worse than changing squares (73 queries on average) as discussed in Sec. 4.2. Finally, we show the results with two more initialization schemes: horizontal stripes (instead of vertical), as well as initialization with randomly placed squares. While both solutions lead to the state-of-the-art query efficiency (83 and 90 queries on average) compared to the literature, they achieve worse results than the vertical stripes we choose for our Square Attack. C.2 l2-Square Attack Sensitivity to the hyperparameter p. We observe that the l2-Square Attack is robust to different choices in the range between 0.05 and 0.4 showing approximately the same failure rate and query efficiency for all values of p in 26 M. Andriushchenko et al. Table 7. An ablation study for the performance of the l‚àû- and l2-Square Attack under various algorithmic choices of the attack. The metrics are calculated on 1,000 ImageNet images for a ResNet-50 model. The last row represents our recommended setting. For all experiments we used the best performing p (0.05 for l‚àû and 0.1 for l2) l‚àû ablation study Update # random Initialization Failure Avg. Median shape signs rate queries queries random c ¬∑ h 2 vert. stripes 0.0% 401 48 random c ¬∑ h 2 uniform rand. 0.0% 393 132 random c vert. stripes 0.0% 339 53 square c ¬∑ h 2 vert. stripes 0.0% 153 15 square 1 vert. stripes 0.0% 129 18 rectangle c vert. stripes 0.0% 93 16 square c uniform rand. 0.0% 91 26 square c rand. squares 0.0% 90 20 square c horiz. stripes 0.0% 83 18 square c vert. stripes 0.0% 73 11 l2 ablation study Update Initialization Failure Avg. Median rate queries queries Œ∑ rand Œ∑ rand-grid 3.3% 1050 324 Œ∑ single Œ∑ single-grid 0.7% 650 171 Œ∑ gaussian 0.4% 696 189 Œ∑ uniform 0.8% 660 187 Œ∑ vert. stripes 0.8% 655 186 Œ∑ Œ∑-grid 0.7% 616 170 this range, while its performance degrades slightly for very small initial squares p ‚àà {0.0125, 0.025}. Algorithmic choices. We analyze in Table 7 the sensitivity of the l2-attack to different choices of the shape of the update and initialization. In particular, we test an update with only one ‚Äúcenter‚Äù instead of two, namely Œ∑ single = Œ∑ h,h (following the notation of Eq. 2) and one, Œ∑ rand, where the step 7 in Algorithm 3 is œÅ ‚Üê Uniform({‚àí1, 1} h√óh ) instead of œÅ ‚Üê Uniform({‚àí1, 1}), which means that each element of Œ∑ is multiplied randomly by either ‚àí1 or 1 independently (instead of all elements multiplied by the same value). We can see that using different random signs in the update and initialization (Œ∑ rand) significantly (1.5√ó factor) degrades the results for the l2-attack, which is similar to the observation made for the l‚àû-attack. Alternatively to the grid described in Sec. 3.4, we consider as starting perturbation i) a random point sampled according to Uniform({‚àí/‚àö d, /‚àö d} w√ów√óc ), that is on the corners of the largest l‚àû-ball contained in the l2-ball of radius  (uniform initialization), ii) a random position on the l2-ball of radius  (Gaussian initialization) or iii) vertical stripes similarly to what done for the l‚àû-Square Attack, but with magnitude /‚àö d to fulfill the constraints on the l2-norm of the Square Attack: a query-efficient black-box adversarial attack 27 Table 8. Mean and standard deviation of the main performance metrics of the Square Attack across 10 different runs with different random seeds ImageNet, ResNet-50 Norm  Failure rate Avg. queries Median queries l‚àû 0.05 0.0% ¬± 0.0% 72 ¬± 2 11 ¬± 1 l2 5 0.6% ¬± 0.1% 638 ¬± 12 163 ¬± 8 MNIST, adversarially trained LeNet from [37] Norm  Robust accuracy Avg. queries Median queries l‚àû 0.3 87.0% ¬± 0.1% 299 ¬± 47 52 ¬± 7 l2 2 16.0% ¬± 1.4% 1454 ¬± 71 742 ¬± 78 perturbation. We note that different initialization schemes do not have a large influence on the results of our l2-attack, unlike for the l‚àû-attack. D Stability of the Attack under Different Random Seeds Here we study the stability of the Square Attack over the randomness in the algorithm, i.e. in the initialization, in the choice of the locations of square-shaped regions, and in the choice of the values in the updates Œ¥. We repeat 10 times experiments similar to the ones reported in Sec. 5.1 and Sec. 5.2 with different random seeds for our attack, and report all the metrics with standard deviations in Table 8. On ImageNet, we evaluate the failure rate (over initially correctly classified points) and query efficiency on 1,000 images using ResNet-50. On MNIST, we evaluate the robust accuracy (i.e. the failure rate over all points) and query efficiency on 1,000 images using the l‚àû-adversarially trained LeNet from [37]. Note that unlike in Sec. 5.2 in both cases we use a single restart for the attack on MNIST, and we compute the statistics on 1,000 points instead of 10,000, thus the final results will differ. On the ImageNet model, all these metrics are very concentrated for both the l‚àû- and the l2-norms. Moreover, we note that the standard deviations are much smaller than the gap between the Square Attack and the competing methods reported in Table 2. Thus we conclude that the results of the attack are stable under different random seeds. On the adversarially trained MNIST model from [37], the robust accuracy is very concentrated showing only 0.1% and 1.4% standard deviations for the l‚àû- and the l2-norms respectively. Importantly, this is much less than the gaps to the nearest competitors reported in Tables 4 and 5. We also show query efficiency for this model, although for models with non-trivial robustness it is more important to achieve lower robust accuracy, and query efficiency on successful adversarial examples is secondary. We note that the standard deviation of the mean and median number of queries is higher than for ImageNet, particularly for the l‚àû- ball of radius  = 0.3 where the robust accuracy is much higher than for the l2-ball of radius  = 2. This is possibly due to the fact that attacking more 28 M. Andriushchenko et al. robust models (within a certain threat model) is a more challenging task than, e.g., attacking standardly trained classifiers, as those used on ImageNet, which means that a favorable random initialization or perturbation updates can have more influence on the query efficiency. E Additional Experimental Results This section contains results on targeted attacks, and also additional results on untargeted attacks that complement the ImageNet results from Table 2. Moreover, we show that the Square Attack is useful for evaluating the robustness of newly proposed defenses (see Sec. E.6). E.1 Targeted Attacks While in Sec. 5 we considered only untargeted attacks, here we report the results of the different attacks in the targeted scenario. Targeted Square Attack. In order to adapt our scheme to targeted attacks, where one first choose a target class t and then tries to get the model f to classify a point x as t, we need to modify the loss function L which is minimized (see Eq. (1)). For the untargeted attacks we used the margin-based loss L(f(x), y) = fy(x) ‚àí maxk6=y fk(x), with y the correct class of x. This loss could be straightforwardly adapted to the targeted case as L(f(x), t) = ‚àíft(x) + maxk6=t fk(x). However, in practice we observed that this loss leads to suboptimal query efficiency. We hypothesize that the drawback of the margin-based loss in this setting is that the maximum over k 6= t is realized by different k at different iterations, and then the changes applied to the image tend to cancel each other. We observed this effect particularly on ImageNet which has a very high number of classes. Instead, we use here as objective function the cross-entropy loss on the target class, defined as L(f(x), t) = ‚àíft(x) + log X K i=1 e fi(x) ! . (5) Minimizing L is then equivalent to maximizing the confidence of the classifier in the target class. Notice that in Eq. (5) the scores of all the classes are involved, so that it increases the relative weight of the target class respect to the others, making the targeted attacks more effective. Experiments. We present the results for targeted attacks on ImageNet for Inception-v3 model in Table 9. We calculate the statistics on 1,000 images (the target class is randomly picked for each image) with query limit of 100,000 for l‚àû and on 200 points and with query limit 60,000 for l2, as this one is more expensive computationally because of the lower success rate, with the same norm bounds  used in the untargeted case. We use the Square Attack with p = 0.01 for l‚àû and p = 0.02 for l2. The results for the competing methods for l‚àû are taken from [39], Square Attack: a query-efficient black-box adversarial attack 29 Table 9. Results of targeted attacks on ImageNet for Inception-v3 model using 100k query limit for l‚àû, 60k for l2. The results for the competing methods for l‚àû are taken from [39], except SignHunter [2] which we evaluated using their code Norm Attack Failure rate Avg. queries Median queries l‚àû Bandits [31] 7.5% 25341 18053 SignHunter [2] 1.1% 8814 5481 Parsimonious [49] 0.0% 7184 5116 DFOc ‚Äì Diag. CMA [39] 6.0% 6768 3797 DFOc ‚Äì CMA [39] 0.0% 6662 4692 Square Attack 0.0% 4584 2859 l2 Bandits [31] 24.5% 20489 17122 SimBA-DCT [28] 25.5% 30576 30180 Square Attack 33.5% 19794 15946 except SignHunter [2] which was not evaluated in the targeted setting before, thus we performed the evaluation using their code on 100 test points using the cross-entropy as the loss function. For l2, we use Bandits [31] with the standard parameters used in the untargeted scenario, while we ran a grid search over the step size of SimBA (we set it to 0.03) and keep the other hyperparameters as suggested for the Inception-v3 model. The targeted l‚àû-Square Attack achieves 100% success rate and requires 1.5 times fewer queries on average than the nearest competitor [39], showing that even in the targeted scenario our simple scheme outperforms the state-of-the-art methods. On the other hand, our l2-attack suffers from worse higher failure rate than the competitors, but achieves lower average and median number of queries (although on a different number of successful points). E.2 Success Rate on ImageNet for Different Number of Queries In this section, we provide a more detailed comparison to the competitors from Table 2 under different query budgets and more comments about the low query regime experiment in Fig. 4. We show in Fig. 6 the behaviour of the success rate for each attack depending on the number queries. The success rates of the attacks from [39] (DFOc‚ÄìCMA‚Äì50 and DFOd‚ÄìDiag. CMA‚Äì30) and [50] (BayesAttack) for different number of queries were obtained via personal communication directly from the authors, and were calculated on 500 and 10,000 randomly sampled points, respectively. For the other attacks, as mentioned above, the success rate is calculated on 1,000 randomly sampled points. l‚àû-results. First, we observe that the Square Attack outperforms all other methods in the standard regime with 10,000 queries. The gap in the success rate gets larger in the range of 100-1000 queries for the more challenging Inception-v3 model, where we observe over 10% improvement in the success rate over all other methods including SignHunter. Our method also outperforms the BayesAttack in the low query regime, i.e. less than 200 queries, by approximately 20% on every model. We note that DFOd‚ÄìDiag. CMA‚Äì30 method is also quite effective 30 M. Andriushchenko et al. Inception v3 ResNet-50 VGG-16-BN l‚àû-attacks l‚àû-attacks low query regime l2-attacks l2-attacks low query regime Fig. 6. Success rate vs number of queries for different attacks on ImageNet on three standardly trained models. The low query regime corresponds to up to 200 queries, while the standard regime corresponds to 10,000 queries. ‚àó denotes the results obtained via personal communication with the authors and evaluated on 500 and 10,000 randomly sampled points for DFO [39] and BayesAttack [50] methods, respectively in the low query regime showing results close to BayesAttack. However, it is also outperformed by our Square Attack. l2-results. First, since the l2-version of SignHunter [2] is not competitive to Bandits on ImageNet (see Fig. 2 in [2]), we do not compare to them here. The l2-Square Attack outperforms both Bandits and SimBA, and the gap is particularly large in the low query regime. We note that the success rate of SimBA plateaus after some iteration. This happens due to the fact that their algorithm only adds orthogonal updates to the perturbation, and does not have any way to correct the greedy decisions made earlier. Thus, there is no progress anymore after the norm of the perturbation reaches the  = 5 (note that we used for SimBA the same parameters of the comparison between SimBA and Bandits in [28]). Contrary to this, both Bandits and our attack constantly keep improving the success rate, although with a different speed. Square Attack: a query-efficient black-box adversarial attack 31 Table 10. Results of untargeted l‚àû-perturbations produced by the Square Attack on architectures with dilated convolutions ‚àû Model Failure rate Avg. queries Median queries 0.05 DRN-A-50 0.0% 86 12 DRN-C-42 0.0% 57 7 DRN-D-38 0.0% 48 6 E.3 Performance on Architectures with Dilated Convolutions In Sec. 4.2, we provided justifications for square-shaped updates for convolutional networks. Thus, a reasonable question is whether the Square Attack still works equally well on less standard convolutional networks such as, for example, networks with dilated convolutions. For this purpose, we evaluate three different architectures introduced in [59] that involve dilated convolutions: DRN-A-50, DRN-C-42 and DRN-D-38. We use ‚àû = 0.05 as in the main ImageNet experiments from Table 2. We present the results in Table 10 and observe that for all the three model the Square Attack achieves 100% success rate and both average and median number of queries stay comparable to that of VGG or ResNet-50 from Table 2. Thus, this experiment suggests that our attack can be applied not only to standard convolutional networks, but also to more recent neural network architectures. E.4 Imperceptible Adversarial Examples with the Square Attack Adversarial examples in general need not be imperceptible, for example adversarial patches [10,34] are clearly visible, and yet can be used to attack machine learning systems deployed in-the-wild. However, if imperceptibility is the goal, it can be easily ensured by adjusting the size of the allowed perturbations. In the main ImageNet experiments in Table 2 we used ‚àû = 0.05 = 12.75/255 since this is standard in the literature [2,31,39,49], and for which all the considered attacks produce visible perturbations. Below we additionally provide results on the more than 3√ó smaller threshold ‚àû = 4/255 which leads to imperceptible perturbations (see Fig. 7). Our attack still achieves almost perfect success rate requiring only a limited number of queries as shown in Table 11. Thus, one can also generate imperceptibile adversarial examples with the Square Attack simply by adjusting the perturbation size. Table 11. Results of untargeted imperceptible l‚àû-perturbations produced by the Square Attack on standard architectures ‚àû Model Failure rate Avg. queries Median queries 4/255 VGG 0.5% 424 115 ResNet-50 0.3% 652 213 Inception v3 5.4% 1013 391 32 M. Andriushchenko et al. Original image Perturbation Adversarial image Fig. 7. Visualization of the imperceptible adversarial examples found by the l‚àû Square Attack on ImageNet using ResNet-50 for ‚àû = 4/255. All the original images were correctly classified while the adversarial images are misclassified by the model. The perturbations are amplified for the visualization purpose E.5 Analysis of Adversarial Examples that Require More Queries Here we provide more visualizations of adversarial perturbations generated by the untargeted Square Attack for  = 0.05 on ImageNet. We analyze here the inputs that require more queries to be misclassified. We present the results in Fig. 8 where we plot adversarial examples after 10, 100 and 500 iterations of our attack. First, we note that a misclassification is achieved when the margin loss becomes negative. We can observe that the loss decreases gradually over iterations, and a single update only rarely leads to a significant decrease of the loss. As the attack progresses over iterations, the size of the squares is reduced according to our piecewise-constant schedule leading to more refined perturbations since the algorithm accumulates a larger number of square-shaped updates. Square Attack: a query-efficient black-box adversarial attack 33 Margin loss 10 iterations 100 iterations 500 iterations Class: limpkin Class: limpkin Class: dowitcher 0 100 200 300 400 500 Iteration 0 2 4 6 8 Margin loss Class: tennis ball Class: tennis ball Class: fig 0 100 200 300 400 500 Iteration 0 2 4 6 Margin loss Class: jinrikisha Class: jinrikisha Class: tricycle 0 100 200 300 400 500 Iteration 0 2 4 6 Margin loss Fig. 8. Visualization of adversarial examples for which the untargeted l‚àû Square Attack requires more queries. We visualize adversarial examples and perturbations after 10, 100 and 500 iterations of the attack. The experiment is done on ImageNet using ResNet-50 for ‚àû = 0.05. Note that a misclassification is achieved when the margin loss becomes negative 34 M. Andriushchenko et al. E.6 Breaking the Post-averaging Defense We investigate whether the l‚àû-robustness claims of [36] hold (as reported in https://www.robust-ml.org/preprints/). Their defense method is a randomized averaging method similar in spirit to [18]. The difference is that [36] sample from the surfaces of several d-dimensional spheres instead of the Gaussian distribution, and they do not derive any robustness certificates, but rather measure robustness by the PGD attack. We use the hyperparameters specified in their code (K=15, R=6 on CIFAR-10 and K=15, R=30 on ImageNet). We show in Table 12 that the proposed defense can be broken by the l‚àû-Square Attack, which is able to reduce the robust accuracy suggested by PGD from 88.4% to 15.8% on CIFAR-10 and from 76.1% to 0.4% on ImageNet (we set p = 0.3 for our attack). This again highlights that straightforward application of gradient-based white-box attacks may lead to inaccurate robustness estimation, and usage of the Square Attack can prevent false robustness claims. Table 12. l‚àû-robustness of the post-averaging randomized defense [36]. The Square Attack shows that these models are not robust ‚àû Dataset Robust accuracy Clean PGD Square 8/255 CIFAR-10 92.6% 88.4% 15.8% ImageNet 77.3% 76.1% 0.4%