

# Automated Feature Selection: A Reinforcement Learning Perspective

Kunpeng Liu, Yanjie Fu, Le Wu, Xiaolin Li, Charu Aggarwal *Fellow, IEEE,* and Hui Xiong *Fellow, IEEE*



This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2021.3115477, IEEE Transactions on Knowledge and Data Engineering

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING



**Abstract**—



Feature selection is a critical step in machine learning that selects the most important features for a subsequent prediction task. Effective feature selection can help reduce dimensionality, improve prediction accuracy, and increase result comprehensibility. 



It is traditionally challenging to find the optimal feature subset from the feature subset space as the space could be very large. While much effort has been made on feature selection, reinforcement learning can provide a new perspective towards a more globally-optimal searching strategy. 



In our previous study [1], we propose a multi-agent reinforcement learning framework for the feature selection problem. Specifically, we first reformulate feature selection with a reinforcement learning framework by regarding each feature as an agent. Besides, we obtain the state of the environment in three ways, i.e., statistic description, autoencoder, and graph convolutional network (GCN), in order to derive a fixed-length state representation as the input of reinforcement learning. 



In addition, we study how the coordination among feature agents can be improved by a more effective reward scheme. Also, we provide a GMM-based generative rectified sampling strategy to accelerate the convergence of multi-agent reinforcement learning. 



Our method searches the feature subset space globally and can be easily adapted to real-time scenarios due to the nature of reinforcement learning. In the extended version, we further accelerate the framework from two aspects. 



Specifically, from the sampling aspect, we show the indirect acceleration by proposing a rank-based softmax sampling strategy, and from the exploration aspect, we show the direct acceleration by proposing an interactive reinforcement learning (IRL)-based exploration strategy. Extensive experimental results show the significant improvement of the proposed method over conventional approaches.



**Index Terms**—Feature Selection, Multi-Agent Reinforcement Learning, Interactive Reinforcement Learning. 





## 1 INTRODUCTION



Feature selection aims to select an optimal subset of relevant features for a downstream predictive task [2], [3]. Effective feature selection can help to reduce dimension- ality, shorten training times, enhance generalization, avoid overfitting, improve predictive accuracy, and provide better interpretation and explanation. In this paper, we study the problem of automated feature subspace exploration forimproving subsequent predictive tasks.



Prior studies in feature selection can be grouped into three categories: (i) filter methods (e.g., univariate feature selection [4], [5], correlation based feature selection [3], [6]), in which features are ranked by a specific score; (ii) wrapper methods (e.g., evolutionary algorithms [7], [8], branch and bound algorithms [9], [10]), in which optimal feature subset is identified by a search strategy that collab- orates with predictive tasks; (iii) embedded methods (e.g., LASSO [11], decision tree [12]), in which feature selection is part of the optimization objective of predictive tasks. However, these studies have shown not just strengths but also some limitations. For example, filter methods ignore the feature dependencies and interactions between feature selection and predictors. 



- *Yanjie Fu and Kunpeng Liu are with the University of Central Florida. Email: yanjie.fu@ucf.edu, kunpengliu@knights.ucf.edu.*

- *Le Wu is with the Hefei University of Technology. Email: lewu@hfut.edu.cn.*

- *Xiaolin Li is with the Nanjing University. Email: lixl@nju.edu.cn*

- *Charu Aggarwal is with the IBM T. J. Watson Research Center.*

  *Email: charu@us.ibm.com.*

- *Hui Xiong is with the Rutgers University. Email: hxiong@rutgers.edu.*



1041-4347 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Authorized licensed use limited to: Beijing Jiaotong University. Downloaded on February 21,2022 at 09:02:20 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2021.3115477, IEEE Transactions on Knowledge and Data Engineering.



Wrapper methods have to search a very large feature space of 2N feature subspace candidates, where N is the number of features. Embedded methods are subject to the strong structured assumptions of predictive models, i.e., in LASSO, the non-zero weighted features are considered to be important. As can be seen, feature selection is a complicated process that requires (i) strategic design of feature significance measurement, (ii) accelerated search of optimal feature subset, and (iii) meaningful integration of predictive models.

Reinforcement learning can interact with environments, learn from action rewards, balance exploitation and explo- ration, and search for long-term optimal decisions [13], [14]. These traits provide great potential to automate feature subspace exploration. Existing studies of automated feature selection in [15], [16] create a single agent to make decisions. In these models, the single agent has to determine the selection or deselection of all N features. In other words, the action space of this agent is 2N . Such formulation is similar to the evolutionary algorithms [7], [8], [17], which tend to obtain local optima.



___注意__

In this paper, we intend to propose a solution for automated feature selection using reinforcement learning. However, several challenges arise toward this goal. First, how can we reformulate the problem so that the action space in reinforcement learning could be limited? Second, how can we accurately describe the state representation in reinforcement learning? Third, how can we efficiently accelerating the exploration of optimal features?



To address the aforementioned challenges, we propose to reformulate the feature selection problem with multi- agent reinforcement learning framework [1]. Specifically, we first assign one agent to each feature, the actions of these feature agents are to select or deselect their corresponding features, and the state of environment is characteristics of the selected feature subspace. We then propose to integrate feature-feature redundancy and feature-label relevance with predictive accuracy as the reward scheme. In this way, we guide the cooperation and competition between agents for effective feature exploration. Moreover, we propose im- proved methods to derive a fixed-length representation vec- tor from the dynamically changing selected feature subset, e.g., in dynamic graph based graph convolutional network (GCN), we construct a feature-feature similarity graph to describe the state. Since nodes are features, the number of nodes changes over time. We exploit GCN to learn state representations from dynamic graphs. Finally, we propose to accelerate the framework by improving the sampling strategy in the experience replay.



Traditionally, we use experience replay [18], [19] to train our multi-agent framework. In the experience replay, an agent takes samples from the agent’s memory that stores different types of training samples to train the model. In automatic control area, reinforcement learning usually considers all of the samples in the memory, because all possible states need to be evaluated. However, in feature selection, noises, outliers, or low-reward data samples can lead to inaccurate understanding of a feature and feature- feature correlations, and, thus, jeopardize the accuracy of feature selection. Can we create a new sampling strategy to select sufficient high-quality samples and avoid low-quality samples? An intuitive method is to oversample high-quality samples by increasing their sampling probabilities. But, this method can not guarantee the independence of samples between different training steps, because the same high- quality samples repeatedly appear in different steps. To address this issue, in our previous paper [1], we develop a Gaussian mixture model (GMM) based generative rectified sampling strategy. Specifically, we first train a GMM with high-quality samples. The trained GMM is then used to generate new independent samples from different mixture distribution components for reinforcement learning.



This paper further extends our previous work [1]. We find that there exist three limitations in the GMM-based sampling strategy, i.e., i) The Gaussian mixture model may not be the perfect model to fit sample’s distribution; ii) The fitting of GMM is costly; iii) Noise may pollute samples. To address these issues, in this extended version, we develop a softmax based sampling strategy. Specifically, we first rank the samples by their reward, and then derive their priority by their inverse rank. The sampling probability is derived by the softmax of priority thereafter. In the exploration strategy aspect, in our previous paper [1], reinforcement learning agent explores the environment and learns from the reward. With more and more experience accumulated, the agent can find a more and more promising exploration direction. 



This exploration strategy is simple and general, which can be easily adapted to almost every reinforcement learning problems. However, when the state space is extremely large, its exploration efficiency would be rather low. To reduce the exploration space, in the extended version, we introduce interactive reinforcement learning (IRL) [20], [21]. In IRL, a pre-trained naive feature selection plays the role of ‘advisor’ to guide the reinforcement learning feature selection algo- rithm to quickly pass its apprenticeship period. Specifically, we firstly derive a feature subset SK via a naive feature Selection algorithm. 



In the apprenticeship steps, we randomly choose half of the features in SK to add them in the selected feature subset. Through this addition, the state representa- tion is changed and thus guides the reinforcement learning to a better exploration direction. After the apprenticeship period, the multi-agent reinforcement learning leaves S and do feature selection independently.



In summary, in this paper, we develop an enhanced multi-agent reinforcement learning framework for feature subspace exploration. Specifically, our contributions are as follows: (1) We reformulate feature selection problem with a multi-agent reinforcement learning framework and de- sign a new reward scheme to guide the cooperation and competition between agents. (2) We develop three differ- ent methods: meta descriptive statistics, autoencoder based deep representation, and dynamic graph based graph con- volutional network (GCN), to derive accurate state represen- tation. (3) We develop three different strategies: GMM-based generative rectified sampling strategy, rank-based softmax sampling strategy and IRL-based exploration strategy to im- prove the training and exploration. (4) We conduct extensive experiments to demonstrate the enhanced performances of our methods.



## 2 RELATED WORK



------ 简单分类（3类。filter methods , wapper methods , embedded methods）------

**Feature Selection.** 

Feature selection can be categorized into three types, based on how the feature selection algorithm combines with the machine learning tasks, i.e., filter meth- ods, wrapper methods and embedded methods [2], [22]. 

Filter methods rank the features merely by relevance scores and only top-ranking features are selected. The representa- tive filter methods are univariate feature selection [4], [5] and correlation based feature selection [3], [6]. With very simply computation complexity, filter methods are very fast and thus they’re efficient on high-dimensional datasets. However, they ignore the feature dependencies, as well as interactions between feature selection and the subsequent predictors. 



Unlike filter methods, wrapper methods take advantage of the predictors and consider the prediction performance as the objective function [23]. The represen- tative wrapper methods are branch and bound algorithms [9], [10]. Wrapper methods are supposed to achieve better performance than filter methods since they search on the whole feature subset space. However, the feature subset space exponentially increases with the number of features, making traversing the feature subset space a NP-hard prob- lem. Evolutionary algorithms [7], [8], [17] low down the computational cost but could only promise locally optimal results. 



Embedded methods combine feature selection with predictors more closely than wrapper methods, and actually they incorporate feature selection as part of predictors. The most widely used embedded methods are LASSO [11], deci- sion tree [12] and SVM-RFE [24]. Embedded methods could have supreme performance on the incorporated predictors, but normally not very compatible with other predictors. 





---------两张图----------

(a) General Process of Feature Selection

(b) Multi-Agent Reinforcement Learning on Feature Selection

Fig. 1: From traditional feature selection to multi-agent reinforcement learning based feature subspace exploration. In the reinforcement learning based feature selection, the interpreter represents the selected feature subset into a state vector by representation learning methods and obtains the overall reward by evaluate the feature subset via downstream machine learning tasks.



**Multi-Agent Reinforcement Learning.**



 Our work is re- lated to multi-agent reinforcement learning, where multi- ple agents share a complex environment and interact with each other [25]. *Stankovic et al.* proposed new algorithms for multi-agent distributed iterative value function approx- imation where the agents are allowed to have different behavior policies while evaluating the response to a sin- gle target policy [26]. *Liao et al.* proposed Multi-objective Optimization by Reinforcement Learning (MORL) to solve the optimal power system dispatch and voltage stability problem, which is undertaken on individual dimension in a high-dimensional space via a path selected by an estimated path value which represents the potential of finding a bet- ter solution [27]. *Yang et al.* developed deep reinforcement learning algorithms which could handle large scale agents with effective communication protocol [28], [29]. *Lin et al.* proposed to tackle the large-scale fleet management prob- lem using reinforcement learning, and proposed a contex- tual multi-agent reinforcement learning framework which successfully tackled the taxi fleet management problem [14]. 



However, these methods define their states by handcraft rules instead of by representation learning, which may leave out important information provided by the environment. And also, as we know the training speed of multi-agent reinforcement learning is low due to the large action space, but these methods rarely study how to improve the training efficiency. Existing studies [15], [16] create a single agent to make decisions. However, this agent has to determine the selection or deselection of all N features. In other words, the action space of this agent is 2N . Such formulation is similar to the evolutionary algorithms [7], [8], [17], which are NP- hard problems and can only obtain local optima.





## 3 PROBLEM FORMULATION



--------研究的特征子空间探索问题--------



Fig. 2: The demonstration of reward assignment process. The feature agent 1 and 2 issue an action to select the feature 1 and feature 2. The feature agent 3 issues an action to disselect the feature 3.



We study the problem of feature subspace exploration, which is formulated as a multi-agent reinforcement learn- ing task. Figure 1 shows an overview of our proposed multi-agent reinforcement learning based feature explo- ration framework. Given a set of features to be explored, we first create a feature agent for each feature. This fea- ture agent is to decide whether its associated feature is selected or not. The selected feature subset is regarded as the environment, in which feature agents interact with each other. The correlations between features are schemed by re- ward assignment. Specifically, the components in our multi- agent reinforcement learning framework includes agents,state, environment, reward, reward assignment strategy, and agent actions.

**Agent.** Assuming there are N features, we define N agents for the N features. For one agent, it is designed to make the selection decision for the corresponding feature.

**Actions.** For the i-th feature agent, the feature action ai = 1 indicates the i-th feature is selected, and ai = 0 indicates the i-th feature is deselected.
 **Environment.** In our design, the environment is the feature subspace, representing a selected feature subset. Whenever a feature agent issue an action to select or deselect a feature, the state of feature subspace (environment) changes.

**State.** The state s is to describe the selected feature subset. To extract the representation of s, we explore three differ- ent strategies, i.e., meta descriptive statistics, autoencoder based deep representation and dynamic graph based graph convolutional network (GCN). We will elaborate these three state representation techniques in Section 4.3.

**Reward.** We design a measurement to quantify the overall reward R generated by the selected feature subset, which is defined the weighted sum of (i) predictive accuracy of the selected feature subset Acc, (ii) redundancy of the selected feature subset Rv, and (iii) relevance of the selected feature subset Rd.

**Reward Assignment Strategy.** We develop a strategy to allocate the overall reward to each feature agent. The assign- ment of the overall reward to each agent, indeed, shows the coordination and competition relationship among agents.



--------图2展示了一个奖励分配的例子--------



In principle, we should recognize and reward all of the participated feature agents. Figure 2 shows an example of reward assignment. There are four features with four corresponding feature agents. In the previous iteration, the feature 1, 2, 3 are selected, and the feature 4 is unselected. In the current iteration, feature agent 1 and feature agent 2 issue actions to select feature 1 and feature 2; feature agent 3 issues an action to deselect feature 3; feature agent 4 does not participate and issue any action to change the status of feature 4. In summary, there are only three feature agents (FA1, FA2, FA3) that participate and issue actions. Therefore, the current reward R is equally shared by these three agents.





## 4 MULTI AGENT   REINFORCEMENT FEATURE SELECTION LEARNING

In this section, we propose a multi-agent reinforcement learning framework for automated feature subspace explo- ration. Later, we discuss how to measure the reward, how to improve the state representation and how to accelerate the proposed framework.

### 4.1 Framework Overview

Figure 3 shows our proposed framework consists of many feature subspace exploration steps. Each exploration step includes two stages, i.e., control stage and training stage.

In the control stage, each feature agent takes actions based on their policy networks, which take current state as input and output recommended actions and next state. The select/deselect actions of each feature agent will change the size and contents of the selected feature subset, and thus, lead to a new selected feature subspace. We regard the selected feature subset as environment. The state represents the statistical characteristics of the selected feature subspace. We derive a comprehensive representations of the state through three different methods, i.e., descriptive statistics, autoencoder and GCN (refer to Section 4.3). Meanwhile, the actions taken by feature agents generate an overall reward. This reward will then be assigned to each of the participating agents.



In the training stage, agents train their policy via experience replay independently. For agent i, at time t, a

newly-created tuple  {st, at, rt, st+1},  including the state  (sti), the action (ati), the reward (rit) and the next state (si t+1), is stored into each agent’s memory. The agent i uses its corresponding mini-batch samples to train its Deep Q- Network (DQN), in order to obtain the maximum long-term reward based on the Bellman Equation [30]:

Q(st, at|θ ) = rt + γ max Q(st+1, at+1|θ ) (1) i i t i i i t+1

where θ is the parameter set of Q network, and γ is the discount factor.

-------需要定义停止的准则-------

The exploration of feature subspace continues until con- vergence or meeting several predefined criteria.



**4.2 Measuring Reward**

We propose to combine the predictive accuracy Acc, the feature subspace relevance Rv, and the feature subspace redundancy Rd as the reward R of actions.
 **Predictive Accuracy.** Our goal is to explore and identify a satisfactory feature subset, which will be used to train a pre- dictive model in a downstream task, such as classification and outlier detection. We propose to use the accuracy Acc of the predictive model to quantify the reward. Specifically, if the predictive accuracy is high, the actions that produce the selected feature subset should receive a high reward; if the predictive accuracy is low, the actions that produce the selected feature subset should receive low rewards.

**Feature Subspace Characteristics.** Aside from exploiting the predictive accuracy as reward, we propose to take into account the characteristics of the selected feature subset. Specifically, a qualified feature subset is usually of low information redundancy and of high information relevance to the predictive labels (responses). Both the information relevance and redundancy can be quantified by the mutual information, denoted by I. Formally, I by:





---------讲Rd和Rv的定义-------

I(x;y) = 􏰂p(xi,yj)log( p(xi,yj) ) (2) i,j p(xi )p(yj ) 公式（2）

where xi, yi is the i-th and j-th feature, p(x,y) is the joint distribution of x and y, while p(x) and p(y) are marginal distribution of x and y.

The *information redundancy* of a feature subset, de- noted by Rd, can be quantified by the sum of pair- wise mutual information among features. Formally, Rd is given by:

Rd =  公式（3）



where S is the feature subset, xi is the i-th feature, The *information relevance* of a feature subset, denoted by Rv, can be quantified by the mutual information between features and labels. Formally, Rv is given by:

Rv = 公式（4）

where c is the label vector.



### 4.3 Improving State Representation

Assuming there is a M ∗ N dataset D, which includes M data samples and N features. Let nj be the number of selected features at the j-th exploration step. Then, M ∗ nj is the dimension of the selected data matrix S, which varies over exploration steps. However, the policy network and target network in DQN require the state representation vector s to be a fixed-length vector all the time. We thus, need to derive a fixed-length state vector s from the selected data matrix S, whose dimensions change over time.

To derive accurate state representation with fixed length, we develop three different methods, including (i) meta descriptive statistics of feature subspace; (ii) static subspace graphs based autoencoder; (iii) dynamic feature-feature sim- ilarity graphs based graph convolutional network (GCN). The commonness between these three methods is that they all first learn representations for each feature, and then aggregate them to get a state representation. The differences between them lie on the representation learning algorithms and aggregation strategies.



Fig. 3: Framework.The framework consists of two stages. In the control stage, feature agents select or drop their corresponding features based on policies. In the training stage, the policies are trained via samples from memories.



Fig. 4: Meta descriptive statistics. We extract descriptive statistics twice from the feature subspace to obtain a fixed-length state vector.





**Method 1: Meta Descriptive Statistics of Feature Subspace.**

Figure 4 shows how we extract the meta data of descriptive statistics from the selected data matrix through a two-step procedure.

*Step 1:* We extract descriptive statistics of the selected data matrix S, including the standard deviation, minimum, maximum and Q1 (the first quartile), Q2 (the second quar- tile), and Q3 (the third quartile). Specifically, we extract the seven descriptive statistics of each feature (**column**) in S, and thus, obtain a *descriptive statistics matrix* D with size of 7∗nj.

*Step 2:* We extract the seven descriptive statistics of each row in the descriptive statistics matrix D, and obtain a meta *descriptive statistics matrix* D′ with a size of 7 ∗ 7.

*Finally*, we link each column D together into the state vector s with a fixed length of 49.



Fig. 5: Autoencoder based deep representations. We use two auto-encoders to map the feature subspace into a fixed-length state vector.

**Method 2: Autoencoder Based Deep Representation of Feature Subspace.** 

Autoencoder has been widely used for representation learning by minimizing the reconstruction loss between an original input and a reconstructed output [31]. An autoencoder contains an encoder that maps the input into a latent representation, and an decoder that recon- structs the original input based on the latent representation. Figure 5 shows how we extract the state vector from the selected data matrix through a two-step algorithm.

*Step 1:* Assuming at the j-th exploration step, S is the se- lected data matrix, and nj is the number of selected features. For each feature (**column**) in S, we apply an autoencoder to convert each feature column into a k-length latent vector, and thus, obtain a *latent matrix* L with a dimension of k ∗ nj .



**Method 3: Dynamic-Graph Based Graph Convolutional Network (GCN).** 

Method 1 and method 2 extract explicit and latent representations of each feature. In this method, we consider not just a feature’s individual representations, but also the correlations among features. Figure 6 shows how the GCN works. To better capture the relationship among features, we first convert the selected data matrix S into a dynamic complete graph G, where a node is a feature column in S. With the *feature correlation graph* G, any graph node embedding techniques could be used for node latent representation by exploiting the correlation among features. As the focus of this paper is not to design more sophisticated node embedding models, we choose GCN as it is a state- of-the-art graph embedding models and shows competing effectiveness in many graph based tasks.

Let S be the selected data matrix with a dimension of M ∗ N , Z be the representation matrix of nodes (features) with a dimension of k ∗ N, k is the length of updated representation. The neural network layer in GCN is given by:

H(l+1) = f(H(l),A) 公式（5）

where H(0) = S, H(L) = Z, L is the layer number, A is the adjacency matrix of graph G. The regular GCN can be reduced into a simplified version by considering the node’s own representation (rather than merely the neighbor structures) and performing symmetric normalization [32]:

f(H(l),A) = σ(Dˆ−21 AˆDˆ−12 H(l)W(l)), (6)

where Aˆ = A + I with I being an identity matrix, Dˆ is the diagonal node degree matrix of Aˆ.

By solving GCN, we obtain the latent representations Z of each feature. We average the representation of each feature into the k-legnth state representation vector.



Fig. 6: Dynamic-graph based GCN. We denote the feature subspace by a dynamic graph and use GCN to update representations of each node.



### 4.4 Improving Experience Replay via Generative Recti- fied Sampling

———————改进DQN的经验回放（通过生成矫正采样）———————



Experience replay is widely used to improve training ef- ficiency of neural networks in reinforcement learning [18], [19]. After taking each action, the latest sample, in the form of a tuple that consists of the′ action (a), the reward (r), the state (s) and the next state (s ), are stored into the memory to replace the oldest sample. In training step, a mini-batch of samples are picked to update the policy network. The vanilla experience replay treats the samples equal, thus it uniformly samples the data from its memory.



In the task of feature subspace exploration, we are partic- ularly interested in exploiting high-quality samples to accel- erate the exploration speed. Prior studies tackle this problem by increasing the sampling probabilities of high-quality samples [33], [34]. However, such strategy creates a new problem: the sampler repeatedly selects a limited number of high-quality samples. Consequently, prior studies can not guarantee the independence of selected samples between different training steps, and can not cover a comprehensive space in the unknown high-quality sample population.



To deal with this problem, we propose a Gaussian mixture model (GMM) based generative rectified sampling algorithm. For each agent, as algorithm 1 shows, we take a set of memory samples T = {< a,r,s,s′ >} as inputs. We firstly cluster the memory samples into two groups: T0 and T1. Samples with the selected action (a = 0) are assigned to group T0, while samples with the deselected action (a = 1) are assigned to group T1. Later, we rank the memory samples in T in terms of reward (r) and select the top p proportion of high-reward samples in each group as high-quality samples. 

The selected high-quality samples are then used to train two GMM based generative models for their corresponding groups via an Expectation Maximization (EM) algorithm [35]. After that, for each group, we use its corresponding well-trained GMM model to generate simulated samples to replace the 1−p proportion of low-reward samples in the corresponding group. In this way, we create two high-reward, large-size, yet independent memory sample sets for the select-action and deselect-action groups. We combine the two simulated memory sample sets into a new high-quality dataset. The agent will take a mini-batch of samples from the new high-quality dataset for accelerating training.



**Algorithm 1:** The GMM-Based Generative Rectified Sampling Algorithm

**Input :** Memory dataset T .

**Output:** A mini-batch of samples B.
 **1** p ← high-quality sample proportion of T .
 **2** Stratify T into two groups. Samples with a = 0 are assigned to group T0 and samples with a = 1 are assigned to group T1.
 **3** **for** i=0 to 1**do**

 **4** Ni ← sample number of Ti .
 **5** Ki ← component number of GMM model G i .
 **6** Rank samples in Ti by their reward r, then select top Ni ∗ p samples from Ti to form th ehigh-quality dataset Hi.
 **7** Use Hi to train the GMM Gi = 􏰁Ki φiN (μi, Σi )via EM algorithm.
 **8** Generate Ni ∗ (1 − p) samples from Gi to for mthe generated dataset Gi.

**9** Join Hi and Gi to create high-quality dataset of action i, Ti . 

**10** **end**

**11** Join T0′ and T1′ to get high-quality dataset T ′ . 

**12** Sample a mini-batch of samples B from T ′ .



## 5 EXTENSION: ACCELERATING  FEATURE SUBSPACE EXPLORATION

--------扩展优化部分，加速特征子空间的探索-------

In the previous version, we initially explores how to acceler- ate the feature subspace exploration process via improving sampling strategy, which accelerates the exploration indi- rectly by improving the training of exploration strategy. (Section 4.4). In the extended version, we further dive into improving the sampling strategy, and also, we study how to accelerate the exploration process via improving exploration strategy directly.

### 5.1 Accelerating Feature Subspace Exploration via Rank-Based Softmax Sampling



-------基于排序的Softmax 采样加速特征子空间探索-------

The GMM-based generative rectified sampling strategy can make full use of the samples in experience replay, thus accelerate the training process of reinforcement learning policy. However, it naturally has three potential drawbacks: 1) It is based on an assumption that the sample are gen- erated by a Gaussian Mixture Model, while their actual distribution could be different; 2) The fitting of GMM model is computationally expensive. To make things worse, it needs to fit the GMM model every time it samples; 3) There could exist noise in the samples, which affects the fitting accuracy of GMM model. Here we have one question: can we propose a more simple sampling strategy yet effective than the sampling strategy in 4.4, which could have lower computational burden and higher robustness?

To tackle these problems, we introduce a rank-based softmax sampling strategy. In this sampling strategy, we measure the importance of each samples by their ranks in the experience replay memory. The sampling probability for each sample is then designed based on their ranks:



(7) 

where NE is the size of experience replay memory, pi is the priority of i-th sample, and we make  （pi公式）where rank(i) is the rank of sample i based on its reward.

The softmax operation promises the sum of all probabilities equals 1. Since rank(i) is a relative value, it has high tolerance of noise and could be very robust. There is no assumption of GMM distribution, thus there is no need of fitting, making the computational burden very low. Specif- ically, as algorithm 2 shows, we firstly derive the rank for each sample, and then obtain their sampling probabilities by Equation 7. The agent will take a mini-batch of samples based on the rank-based sampling probabilities. Compared with the GMM-based generative rectified sampling strategy, it is efficient due to the low computational burden, and effective due to the robustness on noise.



**Algorithm 2:** The Softmax Sampling Algorithm

**Input :** Memory dataset T .

**Output:** A mini-batch of samples B.

1. **1**  NE ← size of experience replay memory T .

2. **2**  Rank samples in T by their reward, and let

   p = 1 be their priority. i rank(i)

3. **3**  Derive the sampling probability for each data sample by Equation 7.

4. **4**  Sample a mini-batch of samples B from T .





-------加速特征子空间的探索-------

### 5.2 Accelerating Feature Subspace Exploration via Interactive Reinforcement Learning

In the classic reinforcement learning framework, the agent repeatedly explores the state space and gets reward, after which it gets more and more experience and behaves better and better. This exploration strategy is general and uni- versal, meaning that it can be applied to any formulated reinforcement learning problems. However, in our case, the state space is extremely large, and if we simply adapt the conventional exploration strategy, the exploration efficiency would be rather low. Here we have one question: Can we propose a more advanced exploration strategy, which could explore along a more promising direction, so that the feature space exploration process would be accelerated?

The proposed multi-agent reinforcement learning frame- work improves itself step by step, and it has a appren- ticeship period in the beginning when its performance is very bad. To reduce its exploration space, we introduce interactive reinforcement learning (IRL) [20], [21]. In IRL, a naive feature selection method, i.e., K-Best Selection [4], works as the ‘advisor’ to guide reinforcement learning to explore along a relatively good direction. After pre-defined steps, the reinforcement learning abandons the advisor and explore the state space independently.

Specifically, as algorithm 3 shows, we firstly derive a feature subset SK via K-Best Selection. In the apprenticeship steps, we randomly choose half of the features in SK to add them in the selected feature subset. Through this addition, the state representation is changed and thus guides the reinforcement learning to a better exploration direction. The reason we don’t use all of the features in SK every step is to avoid over-fitting, and to keep the feature selection process different from the K-Best Selection. After the apprenticeship period, the multi-agent reinforcement learning would do feature selection independently.



**Algorithm 3:** The Interactive Reinforcement Learn- ing Enhanced Exploration Strategy

**Input :** Feature number K, apprenticeship step NA, overall step NO, feature set S.

**Output:** Optimal feature subset S ′ .
 Derive a feature subset SK via K-Best Selection. 

Randomly initialize selected feature subset S0′ . 

**for**i=1toNA **do**

Derive a selected feature subset Si′ by multi-agent reinforcement learning feature selection proposed in Section 4. (*Note: This step relies on the selected feature subset* Si′−1 *from the last step, as* Si′−1 *decides the state representation.*)

Randomly choose K/2 features from SK, denoted as SK/2.

iK/2 Let Si′ =Si′ + Si .

**end
 for**i=NA +1toNO **do**

Derive a selected feature subset Si′ by multi-agent reinforcement learning feature selection proposed in Section 4.

**end**



## 6 EXPERIMENTAL RESULTS

We evaluate the proposed methods in feature selection with real-world datasets. Compared with the previous version [1], we add one more dataset and conduct all the experi- ments on it to validate the data robustness of our methods. Besides, we design more experiments to study the newly proposed softmax sampling strategy and IRL-based explo- ration strategy.

### 6.1 Data Description

The experiments are conducted on two publicly available datasets as follows:

1. https://www.kaggle.com/c/forest-cover-type-prediction/data

**Dataset 1.** This dataset is a cartographic dataset from Kaggle 1. There are 15120 samples with 54 features that describe the characteristics of different wilderness areas. The class labels are categorical values that range from 1 to 7, and represent seven forest cover types (the predominant kind of tree cover) in the areas. Among the 54 features, 10 of them are continuous, and the remaining 44 are categorical. The downstream task is to predict the cover type of wilderness area.

**Dataset 2.** This dataset is a place localization dataset from UCI 2 [36]. There are 34465 samples with 119 features that describe the characteristics of different places. The class label are categorical values that range from 1 to 2, and repre- sent two geographical spots. Among the 119 features, 89 of them are continuous, and the remaining 30 are categorical. The downstream task is to predict the geographical spot with spot descriptions.

**6.2 Evaluation Metrics**

To show the effectiveness of the proposed method, we use the following metrics for evaluation.

**Overall Accuracy** is the ratio of number of correct predictions to number of all predictions. Formally, the overall accuracy is given by (TP+TN)/(TP+TN+FP+FN) , where TP, TN, FP, FN are true positive, true negative, false positive and false negative for all classes. We use this metric to measure the accuracy of a classifier on test dataset. The latter three metrics measure classification performance of each label from different aspects.

**Precision** is given by (T Pk)/(TPk+FPk)  which represents the ratio of  true positive to true positive plus false positive with respect to the k-th (k ∈ [1,7] for Dataset 1 and k ∈ [1,2] for Dataset 2) label. 

 **Recall** is given by (TPk )/( (TPk +FNk)) which represents the ratio of true positive to true positive plus false negative with respect to the k-th (k ∈ [1,7] for Dataset 1 and k ∈ [1,2] for Dataset 2) label.

**F-measure** considers both precision and recall in a single metric by taking their harmonic mean. Formally, F-measure is given by 2∗P ∗R/(P +R), where P and R are precision and recall respectively.



### 6.3 Baseline Algorithms



------------一共有6种Baseline -----------

2. https://archive.ics.uci.edu/ml/datasets/Nomao



We compare performance of our proposed Multi-Agent Re- inforcement Learning Feature Selection (MARLFS) against the following six baseline algorithms, where K-Best Se- lection and mRMR belong to filter methods; LASSO and Recursive Feature Elimination (RFE) belong to embedded methods; Genetic Feature Selection (GFS) and Single-Agent Reinforcement Learning Feature Selection (SARLFS) belong to wrapper methods.

**(1) K-Best Selection.** The K-Best Selection [4] firstly ranks features by their χ2 scores with the target vector (label vector), and then selects the K highest scoring features. In the experiments, we make K equal to the number of selected features in MARLFS.

**(2) mRMR.** The mRMR [37] firstly ranks features by min- imizing feature’s redundancy, while maximizing their rele- vance with the target vector (label vector), and then selects the K highest ranking features. In the experiments, we make K equal to the number of selected features in MARLFS.

**(3) LASSO.** LASSO [11] conducts feature selection and fea- ture space shrinkage via l1 penalty, which drops the feature variables whose coefficients are 0. The hyper parameter in LASSO is its regularization weight λ, which is set to 1.0 in the experiments.
 **(4) Recursive Feature Elimination (RFE).** RFE [38] selects features by recursively selecting smaller and smaller feature subsets. Firstly, the predictor is trained by all features and the importance of each feature are scored by the predictor. After that, the least important features are deselected. This procedure process recursively until the desired number of features are selected. In the experiments, we set the selected feature number half of the feature space.

**(5) Genetic Feature Selection (GFS).** Genetic Feature Se- lection [39] selects features by firstly calculating the fitness level for each feature and then generates better feature subsets via crossover and mutation. In the experiments, we set crossover probability to 0.5, mutation probability to 0.2, crossover independent probability to 0.5 and mutation independent probability to 0.05.

**(6) Single-Agent Reinforcement Learning Feature Selec- tion (SARLFS).** In SARLFS [16], the agent learns a KWIK (Knows What It Knows) model, which is represented by a dynamic Bayesian network, deduces a minimal feature set from this network, and computes a policy on this feature subset using dynamic programming methods. In the exper- iments, the two accuracy thresholds in the KWIK are set to ε = 0.15, δ = 0.10.

### 6.4 Overall Performances

We compare our method MARLFS with baseline methods in terms of overall accuracy as well as precision, recall and F-measure of the seven classes on the real-world data. In the experiments, for all deep networks, we set mini-batch size to 32 and use AdamOptimizer with a learning rate of 0.01. For all experience replays, we set memory size to 200. We set the Q network in our methods as a two-layer ReLU with 64 and 8 nodes in the first and second layer. The high-quality proportion in GMM sampling is 0.20 for Dataset 1 and 0.30 for Dataset 2. Unless specified, we use GCN method as the representation learning algorithm in the experiments, whose network is a two-layer ReLU with 128 and 32 nodes in the first and second layer. The predictor we use is a random forest with 100 decision trees. Figure 7 and 8 show that our method exceeds all of the baseline methods in the task of exploring a qualified feature subset.

### 6.5 Robustness Check



Fig. 7: Performance comparison of different feature selection algorithms on dataset 1

Fig. 8: Performance comparison of different feature selection algorithms on dataset 2



TABLE 1: Overall accuracy of feature selection algorithms on Dataset 1.

TABLE 2: Overall accuracy of feature selection algorithms on Dataset 2.

TABLE 3: Significance of predictive accuracy.



The predictive accuracy relies on not just feature selection, but also predictors. We apply our method to different predictors in order to investigate whether our explored feature subset are consistently stable and can consistently outperform other baseline methods on various predictors. In this way, we can examine the robustness of our methods. Aside from the random forest (RF) predictor, we use (i) LASSO; (ii) Decision Tree (DT); (iii) SVM with a rbf kernel, and (iv) XGBoost as predictors for this experiment. Table 1 and 2 show that our MARLFS outperforms the baselines methods over almost all of the predictors. However, when we use LASSO to perform both feature selection and target prediction, the accuracy of our method is slightly lower than LASSO. This might be explained by the reason that both feature section and prediction optimization are integrated and unified in a single model framework. However, when we use LASSO to perform feature selection, and use other classification models for prediction, our method outperform such type of baselines.

To verify the significance of the proposed method, we design new statistical significant tests on the overall perfor- mance. To obtain sufficient samples, we randomly pick up N/2 (N is the feature number) features without replacement to generate new datasets, and implement baseline methods and the proposed method to carry out significant tests. Specifically, we obtain 30 new dataset for both Dataset 1 and Dataset 2. From Table 3, we can see that all p-value are smaller than the alpha value 0.05, which indicates that our proposed method is of great significance.



### 6.6 Study of Reward Function

We study the design of the reward function in our frame- work. Specifically, we consider four cases: (i) **Acc** that only considers accuracy in the reward function; (ii) **Rv** that only considers relevance in the reward function; (iii) **Rd** that only considers redundancy in the reward function; (iv) **Acc+Rv+Rd** that considers accuracy, relevance and redun- dancy in the reward function.

Figure 9 and 10 show that Acc is the second best reward function, since it leads the exploration to the direction of improving accuracy. Rv and Rd are less satisfactory. This is because both are unsupervised indicators of rewards and are not directly relevant to prediction accuracy. Acc+Rv+Rd achieve the best performances since it considers both su- pervised indicator and unsupervised indicator into account. Specifically, Figure 9(a) shows the comparisons of overall accuracy over exploration steps. Figure 9(b), 9(c) and 9(d) show the comparisons of precision, recall and F-measure over different classes with 3000 exploration steps.

### 6.7 Study of State Representation Learning

We compare the performances different representation learning methods. We consider five cases, i.e., (i) **MDS**: meta descriptive statistics, which uses the meta data of descrip- tive statistics of feature subspace to represent the state; (ii) **AE**: auto-encoder based deep representation, which uses deep auto-encoder to encode feature subspace twice to obtain state representation; (iii) **GCN**: uses Dynamic-Graph Based GCN; (iv) **MDS+AE**: combines the variables of (i) and (ii) to represent the state; (v) **MDS+AE+GCN**: combines the variables of (i), (ii), and (iii) to represent the state.



Fig. 9: Performance comparison of different reward functions on Dataset 1

Fig. 10: Performance comparison of different reward functions on Dataset 2.

TABLE 4: Overall accuracy comparison of sampling strategies on Dataset 1.

TABLE 5: Overall accuracy comparison of sampling strategies on Dataset 2.



Figure 11 and 12 show GCN outperforms MDS and AE. This is because GCN could better capture the relationship between features in the feature subspace. After taking the two combined methods into account, MDS+AE achieves the best performance, since it considers both explicit and implicit information from the selected features. An interesting observation is that MDS+AE+GCN doesn’t have better performance than MDS+AE+GCN. This might be explained by the fact that there is potential training loss in the training phrase of AE and GCN. In other words, integrating AE and GCN might possibly introduce more model biases. Specifically, Figure 11(a) and 12(a) show the comparisons of overall accuracy over exploration steps. Figure 11(b) and 12(b), 11(c) and 12(c), 11(d) and 12(d) show the comparisons of precision, recall and F-measure over different classes with 3000 exploration steps.

### 6.8 Study of GMM-based Generative Rectified Sam- pling

We study the impacts of GMM-based generative recti- fied sampling, where the high-quality proportion p ∈ [0.1, 0.2, 0.3, 0.4, 1] respectively. Here, when p = 1, our GMM-based method reduces to the traditional sampling strategy, where samples are considered as high-quality. We call the method with p = 1 as the non-GMM method.

Figure 13 and 14 show all GMM-based sampling meth- ods (p < 1.0) outperform the non-GMM method (p = 1.0). For Dataset 1, p = 0.2 shows the best performances and can quickly explore a quality feature space, while For Dataset 2, p = 0.3 shows the best performances and can quickly explore a quality feature space. Specifically, 13(a) and 14(a) show the comparisons of overall accuracy over exploration steps. Figure 13(b) and 14(b), 13(c) and 14(c), 13(d) and 14(d) show the comparisons of precision, recall and F-measure over different classes with 3000 exploration steps.

### 6.9 Study of Softmax Sampling

We study the impacts of softmax sampling and compare it with vanilla experience replay (uniformly sampling) and GMM-based sampling. We use meta descriptive statistics as the state representation method, and we use MARLFS as feature selection method. We run each setting 5 times to compare their averaged execution time per step. The explo- ration step is set to 3000. Our experiments were conducted on a machine with the following specification:

• Processor: 64-bit Intel I9-9920X @ 4.40GHz with 12 core(s)

• Memory: 128GiB DIMM DDR4 2666MHz • SSD: 2TB PCIe NVMe - M.2

Table 4 and 5 show that the GMM-based sampling method achieves the best accuracy, and the softmax sampling method has a parellel accuracy with GMM-based sampling, which are both higher than the uniform sampling. Table 6 and 7 show that the softmax sampling method costs a bit more time than uniform sampling, and both of them require less execution time than the GMM-based sampling method.

### 6.10 Study of Interactive Reinforcement Learning



TABLE 6: Execution time comparison of sampling strategies on Dataset 1.

Fig. 11: Performance comparison of different representation learning methods on dateset 1

Fig. 11: Performance comparison of different representation learning methods on dateset 2

(a) Overall Accuracy (b) Precision (c) Recall

Fig. 13: Performance comparison of different GMM sampling strategies sampling strategies on dateset 1 Fig. 14: Performance comparison of different  GMM sampling strategies sampling strategies on dateset 2

TABLE 7: Execution time comparison of sampling strategies on Dataset 2.



We study the impacts of IRL, where its apprenticeship steps NA ∈ [0, 0.1, 0.2, 0.3, 0.4, 1] ∗ 3000 respectively. We set K = 38 for K-Best Selection on Dataset1 and K = 82 for K-Best Selection on Dataset2. Here, when NA = 0, the IRL strategy will be reduced into the traditional exploration strategy, and when NA = 3000, the IRL spends all its exploration steps being advised by K-Best Selection method. Figure 15 shows that all IRL sampling methods (p = 0.0) outperform the non-GMM method (p > 0.0). For both Dataset 1 and Dataset 2, p = 0.2 shows the best performances and can quickly explore a quality feature space.



(a) Overall Accuracy on Dataset 1

(b) Overall Accuracy on Dataset 2 .

 Fig. 15: Performance comparison of different IRL sampling strategies.





### 6.11 **Study of Execution Time**

Reinforcement learning usually requires much time to learn from trials. Thus, it is necessary to study the time consuming of our method to verify its applicability. We use meta descriptive statistics as the state representation method. We run each setting 5 times to compare their averaged execution time. The exploration step is set to 3000. The predictor in wrapper methods and our method MARLFS is fixed the random forest method. From Table 8, we can see that the K-Best, mRMR, LASSO and RFE methods are fast and efficient. Our method MARLFS requires more time than these traditional methods. However, the excution time of MARLFS is parallel with the remain traditional featrue selection methods, i.e., GRS and SARLFS.



(a) Predictive Accuracy on Dataset 1 (b) Predictive Accuracy on Dataset 2

Fig. 16: Performance comparison of different reward assignment strategies.

TABLE 8: Execution time comparison of feature selection algorithms.





### 6.12 **Study of Reward Assignment Strategy**

The reward assignment strategy only measures whether an agent is a participated agent or not, but it does not measure how much reward each participated agent should be as- signed, and simply equally assign the overall reward to each participated agent. To address this concern, we design a new reward assignment strategy, i.e., fine-grained reward assign- ment (FGRA). Compared with the original coarse-grained reward assignment (CGRA), the overall reward is no longer assigned equally, but according to their contribution to the accuracy improvement. We evaluate the feature importance in random forest based on mean decrease in impurity (implemented by 3), and use the importance as weight to assign the overall weight to each agent. The performance comparison between FGRA and CGRA is shown in Figure 16. We can see that the FGRA, which measures how much reward each participated agent should be assigned, reveals better performance.



## **7 CONCLUSION** REMARKS

In this paper, we study the problem of automated feature subspace exploration. Through this method, we can reduce dimensionality, shorten training times, enhance generaliza- tion, avoid overfitting, and improve predictive accuracy in order to support downstream predictive tasks. We formulate the problem of automated feature subspace exploration as a multi-agent reinforcement learning framework, in which each feature is associated to a feature agent, a feature agent can decide to select or drop a feature, the reward function is a combination of accuracy, redundancy, and relevance, and the environment is the characteristics of the selected feature subspace. To better represent the environment, we propose three different representation learning methods. To accelerating feature exploration, we develop a GMM-based generative rectified sampling strategy a softmax sampling strategy and an IRL-based exploration strategy. Finally, we present extensive experiments on two real world datasets to demonstrate the effectiveness of the proposed method. The propose feature selection method can be widely used in the data mining and machine learning areas, and it is more helpful when users are not experts in feature selection but need this technology in their work and research.



3.https://scikit-learn.org/stable/autoexamples/ensemble/plotforestimpoprpt.a2n9c3e–s.3h2t1m,l1992.





**R****EFERENCES**

[1] K. Liu, Y. Fu, P. Wang, L. Wu, R. Bo, and X. Li, “Automating feature subspace exploration via multi-agent reinforcement learning,” in *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2019, pp. 207–215.

[2] G. Chandrashekar and F. Sahin, “A survey on feature selection methods,” *Computers & Electrical Engineering*, vol. 40, no. 1, pp. 16–28, 2014.

[3] L. Yu and H. Liu, “Feature selection for high-dimensional data: A fast correlation-based filter solution,” in *Proceedings of the 20th international conference on machine learning (ICML-03)*, 2003, pp. 856–863.

[4] Y. Yang and J. O. Pedersen, “A comparative study on feature selection in text categorization,” in *Icml*, vol. 97, 1997, pp. 412–420. [5] G. Forman, “An extensive empirical study of feature selection metrics for text classification,” *Journal of machine learning research*,

vol. 3, no. Mar, pp. 1289–1305, 2003.
 [6] M. A. Hall, “Feature selection for discrete and numeric class

machine learning,” 1999.
 [7] J. Yang and V. Honavar, “Feature subset selection using a ge-

netic algorithm,” in *Feature extraction, construction and selection*.

Springer, 1998, pp. 117–136.
 [8] Y. Kim, W. N. Street, and F. Menczer, “Feature selection in unsu-

pervised learning via evolutionary search,” in *Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining*. ACM, 2000, pp. 365–369.

[9] P. M. Narendra and K. Fukunaga, “A branch and bound algorithm for feature subset selection,” *IEEE Transactions on computers*, no. 9, pp. 917–922, 1977.

[10] R.KohaviandG.H.John,“Wrappersforfeaturesubsetselection,” *Artificial intelligence*, vol. 97, no. 1-2, pp. 273–324, 1997.

[11] R. Tibshirani, “Regression shrinkage and selection via the lasso,” *Journal of the Royal Statistical Society. Series B (Methodological)*, pp. 267–288, 1996.

[12] V. Sugumaran, V. Muralidharan, and K. Ramachandran, “Feature selection using decision tree and classification through proximal support vector machine for fault diagnostics of roller bearing,” *Mechanical systems and signal processing*, vol. 21, no. 2, pp. 930–942, 2007.

[13] X. Zhao, L. Xia, L. Zhang, Z. Ding, D. Yin, and J. Tang, “Deep reinforcement learning for page-wise recommendations,” *arXiv preprint arXiv:1805.02343*, 2018.

[14] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efficient large-scale fleet management via multi-agent deep reinforcement learning,” *arXiv preprint arXiv:1802.06444*, 2018.

[15] S. M. H. Fard, A. Hamzeh, and S. Hashemi, “Using reinforcement learning to find an optimal set of features,” *Computers & Mathe- matics with Applications*, vol. 66, no. 10, pp. 1892–1904, 2013.

[16] M. Kroon and S. Whiteson, “Automatic feature selection for model-based reinforcement learning in factored mdps,” in *Machine Learning and Applications, 2009. ICMLA’09. International Conference on*. IEEE, 2009, pp. 324–330.

[17] F.-A. Fortin, F.-M. D. Rainville, M.-A. Gardner, M. Parizeau, and C. Gagne ́, “Deap: Evolutionary algorithms made easy,” *Journal of Machine Learning Research*, vol. 13, no. Jul, pp. 2171–2175, 2012.

[18] L.-J. Lin, “Self-improving reactive agents based on reinforcement learning, planning and teaching,” *Machine learning*, vol. 8, no. 3-4,



1. [19]  V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p. 529, 2015.
2. [20]  F.Cruz,S.Magg,Y.Nagai,andS.Wermter,“Improvinginteractive reinforcement learning: What makes a good teacher?” *Connection Science*, vol. 30, no. 3, pp. 306–325, 2018.
3. [21]  F.Cruz,J.Twiefel,S.Magg,C.Weber,andS.Wermter,“Interactive reinforcement learning through speech guidance in a domestic scenario,” in *2015 International Joint Conference on Neural Networks (IJCNN)*. IEEE, 2015, pp. 1–8.
4. [22]  Y. Saeys, I. Inza, and P. Larran ̃aga, “A review of feature selection techniques in bioinformatics,” *bioinformatics*, vol. 23, no. 19, pp. 2507–2517, 2007.
5. [23]  D. Guo, H. Xiong, V. Atluri, and N. Adam, “Semantic feature selection for object discovery in high-resolution remote sensing imagery,” in *Pacific-Asia Conference on Knowledge Discovery and Data*

*Mining*. Springer, 2007, pp. 71–83.

1. [24]  I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, “Gene selection for cancer classification using support vector machines,” *Machine learning*, vol. 46, no. 1-3, pp. 389–422, 2002.

2. [25]  A.Tampuu,T.Matiisen,D.Kodelja,I.Kuzovkin,K.Korjus,J.Aru, J. Aru, and R. Vicente, “Multiagent cooperation and competition with deep reinforcement learning,” *PloS one*, vol. 12, no. 4, p. e0172395, 2017.

3. [26]  M.Stankovic,“Multi-agentreinforcementlearning,”in*NeuralNet- works and Applications (NEUREL), 2016 13th Symposium on*. IEEE, 2016, pp. 1–1.

4. [27]  H. Liao, Q. Wu, and L. Jiang, “Multi-objective optimization by reinforcement learning for power system dispatch and voltage stability,” in *Innovative Smart Grid Technologies Conference Europe (ISGT Europe), 2010 IEEE PES*. IEEE, 2010, pp. 1–8.

5. [28]  Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean field multi-agent reinforcement learning,” *arXiv preprint arXiv:1802.05438*, 2018.

6. [29]  P. Peng, Y. Wen, Y. Yang, Q. Yuan, Z. Tang, H. Long, and J. Wang, “Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games,” *arXiv preprint arXiv:1703.10069*, 2017.

7. [30]  R.S.SuttonandA.G.Barto,*Reinforcementlearning:Anintroduction*. MIT press, 2018.

8. [31]  Y. Bengio *et al.*, “Learning deep architectures for ai,” *Foundations and trends® in Machine Learning*, vol. 2, no. 1, pp. 1–127, 2009.

9. [32]  T. N. Kipf and M. Welling, “Semi-supervised classification with

   graph convolutional networks,” *arXiv preprint arXiv:1609.02907*,

   2016.

10. [33]  T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized

    experience replay,” *arXiv preprint arXiv:1511.05952*, 2015.

11. [34]  H. Wei, G. Zheng, H. Yao, and Z. Li, “Intellilight: A reinforcement learning approach for intelligent traffic light control,” in *Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge*

    *Discovery & Data Mining*. ACM, 2018, pp. 2496–2505.

12. [35]  A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likeli- hood from incomplete data via the em algorithm,” *Journal of the*

    *royal statistical society. Series B (methodological)*, pp. 1–38, 1977.

13. [36]  L. Candillier and V. Lemaire, “Design and analysis of the nomao challenge active learning in the real-world,” in *Proceedings of the ALRA: Active Learning in Real-world Applications, Workshop ECML-*

*PKDD*. sn, 2012.

1. [37]  H.Peng,F.Long,andC.Ding,“Featureselectionbasedonmutual

   information criteria of max-dependency, max-relevance, and min- redundancy,” *IEEE Transactions on pattern analysis and machine intelligence*, vol. 27, no. 8, pp. 1226–1238, 2005.

2. [38]  P.M.Granitto,C.Furlanello,F.Biasioli,andF.Gasperi,“Recursive feature elimination with random forest for ptr-ms analysis of agroindustrial products,” *Chemometrics and Intelligent Laboratory Systems*, vol. 83, no. 2, pp. 83–90, 2006.

3. [39]  R. Leardi, “Genetic algorithms in feature selection,” in *Genetic algorithms in molecular modeling*. Elsevier, 1996, pp. 67–86.

13

**Kunpeng Liu** is currently a PhD candidate at the University of Central Florida (UCF). His gen- eral interests are data mining and reinforcement learning.

**Yanjie Fu** is currently an assistant professor at the University of Central Florida (UCF). His general interests are data mining and big data analytics.

**Le Wu** is currently an assistant professor at the Hefei University of Technology (HFUT), China. Her general area of research interests is data mining, recommender systems and social net- work analysis.

**Xiaolin Li** is currently an associate professor at the School of Management at Nanjing Univer- sity, China. Her main research interests include business intelligence, data mining, and decision making.

**Charu Aggarwal** has been a research staff member at the IBM T.J. Watson Research Cen- ter since June 1996. He is interested in the use of data mining techniques for Web and e- commerce applications.



**Hui Xiong** is currently a Full Professor and Vice Chair of the Management Science and Informa- tion Systems Department at the Rutgers, the State University of New Jersey. His general area of research is data mining and knowledge engi- neering.









